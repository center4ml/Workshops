{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/center4ml/Workshops/blob/2023_2/Day_2/10_computational_graph_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fa4b9724-7cd4-4055-9eeb-d91511eb0cbd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMFxJp3KKa_w"
      },
      "source": [
        "# First example - repetition\n",
        "\n",
        "So let's see how it works in practice in PyTorch. We don't need a neural network to see it.\n",
        "\n",
        "Let's have $f(x,y)=x^3+y^2$ as an example.\n",
        "Then you can calculate by hand:\n",
        "\n",
        "$\\frac{\\partial f(x,y)}{\\partial x} = 3x^2, \\frac{\\partial f(x,y)}{\\partial y} = 2y$\n",
        "\n",
        "Concretely, as an example with which we will work some more:\n",
        "\n",
        "$\\frac{\\partial f(x,y)}{\\partial x} \\Big|_{x=2} = 3 \\cdot 2^2 = 12$\n",
        "\n",
        "$\\frac{\\partial f(x,y)}{\\partial y} \\Big|_{y=4} = 2 \\cdot 4 = 8$\n",
        "\n",
        "You get all this automatically with PyTorch, which builds a tree of an expression that is constructed as we go. Let's have an example in Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-b9CEqfLTpw",
        "outputId": "ec5ff31f-6d25-40bf-b8d9-d98de186fd39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f: <AddBackward0 object at 0x7f3448dd3ee0>\n",
            "pow3: <PowBackward0 object at 0x7f3500fbd540>\n",
            "pow2: <PowBackward0 object at 0x7f3448dd3ee0>\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[2., 3., 2.], [2., 3., 3.]], requires_grad=True)  # we initiate the points with values\n",
        "y = torch.tensor([[4., 5., 5.], [4., 5., 5.]], requires_grad=True)  # equal to where we want to compute\n",
        "                                                                    # partial derivatives\n",
        "pow3 = x ** 3\n",
        "pow2 = y ** 2\n",
        "f = pow3 + pow2                              # then we build the function\n",
        "\n",
        "print('f:', f.grad_fn)\n",
        "print('pow3:',pow3.grad_fn)\n",
        "print('pow2:',pow2.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkCwCqKuKslY"
      },
      "source": [
        "The expression tree (which is used for computing gradients) gets built as we go, with leaves being the individual tensors we start from.\n",
        "\n",
        "To calculate gradients numerically at the current tensor values you can call `backward()`.\n",
        "\n",
        "Let's see if we get\n",
        "\n",
        "$\\frac{\\partial f(x,y)}{\\partial x} \\Big|_{x=2} = 12$\n",
        "\n",
        "$\\frac{\\partial f(x,y)}{\\partial y} \\Big|_{y=4} = 8$\n",
        "\n",
        "as expected. The gradients of $f(x,y)$ with respect to variables $x$ and $y$ get written to a `grad` field of those variables (`x.grad`, `y.grad`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-gw5kueNWMD",
        "outputId": "8d9efbab-f880-4df5-ad6b-64228fb38e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f == tensor([[24., 52., 33.],\n",
            "        [24., 52., 52.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print('f ==', f)\n",
        "# f.backward() results in an error 'grad can be implicitly created only for scalar outputs`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDA4ToJLNNGC"
      },
      "source": [
        "You cannot call `backward()` on any other tensor than a scalar. And `f` is not a scalar. The reason behind it is that loss is always a scalar. Recall, that in backpropagation you adjust weights of a neural network according to gradients calculated with respect to those weights on a loss function (which results is a scalar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5ec77c-ec04-4bcf-aafa-fe6f959ef21b"
      },
      "source": [
        "Recall, that to calculate gradients you need to call `backward()`, but you may call it on scalar variables only. But `f.sum()` is a scalar!\n",
        "\n",
        "Let's try to call `backward()` on `f.sum()` scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64cd43a2-8cd2-462d-9ed4-30b960d15cc2",
        "outputId": "11087385-2a06-485f-963c-81bcc44352d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[12., 27., 12.],\n",
              "        [12., 27., 27.]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bd52f90-4024-4185-b188-afcbd39cb995",
        "outputId": "4a21a7a8-f2cc-440d-f534-d3be70fe883b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 8., 10., 10.],\n",
              "        [ 8., 10., 10.]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98MYNdwPAOlM"
      },
      "source": [
        "OK, results are as expected (i.e. as calculated by hand earlier) coordinate-wise, i.e. on every coordinate separately.\n",
        "\n",
        "### Explanation of this `sum()` trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9bCA9ImM3Gd"
      },
      "source": [
        "\n",
        "The operations (powers and addition) work coordinate-wise, so in fact, for those tensors of order 2, and dimensions 2 by 3, we have that `f` is a matrix sized 2 by 3 of functions $f_{ij}$, each defined as $f_{ij} = x_{ij}^3 + y_{ij}^2$ and independent of **other** coordinates.\n",
        "\n",
        "Consequently, a partial derivative of $f_{ij}$ with respect to say $x_{ij}$ is equal to a partial derivative of `f.sum()` with respect to $x_{ij}$, mathematically $ \\frac{\\partial f_{ij}}{\\partial x_{ij}} = \\frac{\\partial \\sum_{kl}f_{kl}}{\\partial x_{ij}}$\n",
        "\n",
        "**Note, that it works independently on number of orders of tensors in question nor on the particular order dimensions.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btNiSmioNShH"
      },
      "source": [
        "# Another way to get the gradients\n",
        "\n",
        "is to call the `torch.autograd.grad` method.\n",
        "It ***returns*** the gradients instead of storing them in the graphs leafes (as `torch.autograd.backward` would do)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qqbv82yYM6nY"
      },
      "outputs": [],
      "source": [
        "# let us reinitialize the variables\n",
        "x = torch.tensor([[2., 3., 2.], [2., 3., 3.]], requires_grad=True)\n",
        "y = torch.tensor([[4., 5., 5.], [4., 5., 5.]], requires_grad=True)\n",
        "f = x**3 + y**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D-KlMhYNEm1",
        "outputId": "8a4085c7-a7b8-4432-e9b9-0eb9c7a860a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f_x: (tensor([[12., 27., 12.],\n",
            "        [12., 27., 27.]], grad_fn=<MulBackward0>),)\n"
          ]
        }
      ],
      "source": [
        "# f_x = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(x)) # frees the graph after computations\n",
        "# f_x = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(x), retain_graph=True) # retain_graph: the graph used to compute the grad will be kept. Allows to differentiate f again.\n",
        "\n",
        "f_x = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(x), create_graph=True) # create_graph: graph of the derivative will be constructed, allowing to compute higher order derivative products.\n",
        "\n",
        "print(f\"f_x: {f_x}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG2Ym-HAS-e0"
      },
      "source": [
        "Notice, that the x.grad is empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gDNB7QfJSnFU"
      },
      "outputs": [],
      "source": [
        "# f.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RAx3jEECZL"
      },
      "source": [
        "### Higher order derivatives\n",
        "\n",
        "As one may expected, we have to differentiate  one more time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvQbF7cX83vA",
        "outputId": "7ef06970-23f2-444b-a7d9-38b583f9e7bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[12., 18., 12.],\n",
              "         [12., 18., 18.]]),)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_xx = torch.autograd.grad(f_x, x, grad_outputs=torch.ones_like(x))\n",
        "# f_xx = torch.autograd.grad(f_x, x, grad_outputs=torch.ones_like(x), retain_graph=True)\n",
        "# f_xx = torch.autograd.grad(f_x, x, grad_outputs=torch.ones_like(x), create_graph=True) # allows to differentiate f_xx to compute higher order derivatives\n",
        "\n",
        "f_xx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnNFCUiUOFqL"
      },
      "source": [
        "### Conclusions:\n",
        "\n",
        "We need to create (and keep) the computational graph to find the higher order derivativess.\n",
        "\n",
        "`create_graph (bool, optional)` – If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.\n",
        "\n",
        "\n",
        "`retain_graph (bool, optional)` – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph.\n",
        "\n",
        "#### References:\n",
        "\n",
        "<https://pytorch.org/docs/1.5.0/autograd.html#torch.autograd.grad>\n",
        "\n",
        "https://stackoverflow.com/questions/69148622/difference-between-autograd-grad-and-autograd-backward\n",
        "\n",
        "https://discuss.pytorch.org/t/when-do-i-use-create-graph-in-autograd-grad/32853\n",
        "\n",
        "https://discuss.pytorch.org/t/whats-the-difference-between-torch-autograd-grad-and-backward/94638\n",
        "\n",
        "https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO/ePuAPC2gbZs/UpjbhP3n",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
