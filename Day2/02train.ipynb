{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B_k7PVhY6ZXM",
        "XTNBAV6MHT51",
        "LI_guuZQHG7V",
        "8yX_VacfMIFb",
        "fa4lRGQpP4uy",
        "v85DnyRSmxSl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rudiments of Natural Language Processing: Train"
      ],
      "metadata": {
        "id": "Qb74H7Ot6Q_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the second part of the NLP workshop at the Center for Machine Learning. In this notebook we take the IMDB data preprocessed earlier and train a very simple model to classify the movie reviews as positive or negative. We do the following:\n",
        "\n",
        "*   Gather some prerequisites\n",
        "*   Load the data\n",
        "*   Build a very simple model\n",
        "*   Implement the training loop\n",
        "*   Train and evaluate the model\n",
        "*   Explore the word embeddings"
      ],
      "metadata": {
        "id": "pbgtnZNa1e60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "B_k7PVhY6ZXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you even start go to the menu above and make sure that your execution environment uses the GPU. Otherwise the model will train very slowly. If you change the environment later you will need to rerun the notebook from the begining. Now install the missing dependencies:"
      ],
      "metadata": {
        "id": "-UtX0SdK0mL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMKAtkWD6Hr8"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules:"
      ],
      "metadata": {
        "id": "F-l2eYaO7DwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab as colab\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch\n",
        "import torchdata\n",
        "import torchtext"
      ],
      "metadata": {
        "id": "fR6ojpdO7H41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous notebook we saved some data in your home directory on the google drive. To read these data mount your drive:"
      ],
      "metadata": {
        "id": "E28ElKZo3fRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab.drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Klp46zWz8WD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your home directory is now mounted as `drive/MyDrive`. Check it by listing its contents:"
      ],
      "metadata": {
        "id": "XNzyxreo4Vk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive"
      ],
      "metadata": {
        "id": "3MLEtxfF4a4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the files `vocab.pkl`, `train_data.csv` and `valid_data.csv`. If not run the previous notebook to produce them."
      ],
      "metadata": {
        "id": "0Ah5wnZ848MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "XTNBAV6MHT51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just to remind print the first ten rows of the training CSV file:"
      ],
      "metadata": {
        "id": "KpbJb75-7C7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head drive/MyDrive/train_data.csv"
      ],
      "metadata": {
        "id": "KU_Fwg3_7NQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row represents one review. The first column contains the classification label which is 0 or 1 for negative or positive reviews respectively. The next columns contain indices of subsequent words in a review according to the vocabulary. We will be loading these data using datapipes which will soon become a new standard in pytorch but are now in beta phase so you may encounter some problems. Create a datapipe that reads rows from the training CSV file:"
      ],
      "metadata": {
        "id": "tkDk--hx5bXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()"
      ],
      "metadata": {
        "id": "vjk3Pdpj65-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This datapipe yields the rows as lists of text strings that actually contain numbers:"
      ],
      "metadata": {
        "id": "sjChjhHN8uTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_pipe.header(10):\n",
        "    print(sample)"
      ],
      "metadata": {
        "id": "U6iueB3482mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consume this datapipe so that the label and word indices are unpacked to separate variables:"
      ],
      "metadata": {
        "id": "LDx3uqIn9NcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label0, *index1 in train_pipe.header(10):\n",
        "    print(label0, index1)"
      ],
      "metadata": {
        "id": "DCt_ixkq8Aak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now write a separate `decode` function that does the unpacking:"
      ],
      "metadata": {
        "id": "H0mZPfZy98Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(sample):\n",
        "    label0, *index1 = sample\n",
        "    return label0, index1"
      ],
      "metadata": {
        "id": "XcX4dMj69RgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply this function to each row yielded by the datapipe by mapping it onto the datapipe:"
      ],
      "metadata": {
        "id": "saUWPj4u-RD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)"
      ],
      "metadata": {
        "id": "MzDI_zv8_Duh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This new datapipe directly yields tuples of labels and indices:"
      ],
      "metadata": {
        "id": "qU0anpMK_IL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label0, index1 in train_pipe.header(10):\n",
        "    print(label0, index1)"
      ],
      "metadata": {
        "id": "88vNnvm89ei3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the `decode` function also convert the labels and indices from text strings to float and integer numbers respectively:"
      ],
      "metadata": {
        "id": "sVTJaQDo_eFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(sample):\n",
        "    label0, *index1 = sample\n",
        "    label0 = float(label0)\n",
        "    index1 = [int(index0) for index0 in index1]\n",
        "    return label0, index1"
      ],
      "metadata": {
        "id": "Af3YwPpR-imN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinstantiate the datapipe with the new `decode` function and test it:"
      ],
      "metadata": {
        "id": "zM074_WR_76F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "\n",
        "for label0, index1 in train_pipe.header(10):\n",
        "    print(label0, index1)"
      ],
      "metadata": {
        "id": "gPR2ej2q_mMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let the `decode` function also convert the indices from python lists to pytorch tensors:"
      ],
      "metadata": {
        "id": "kg3Hut3XAQnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(sample):\n",
        "    label0, *index1 = sample\n",
        "    label0 = float(label0)\n",
        "    index1 = torch.tensor([int(index0) for index0 in index1])\n",
        "    return label0, index1"
      ],
      "metadata": {
        "id": "J_Cjy6F4_7_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinstantiate the datapipe and test it:"
      ],
      "metadata": {
        "id": "sPOcs_tNAc7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "\n",
        "for label0, index1 in train_pipe.header(10):\n",
        "    print(label0, index1)"
      ],
      "metadata": {
        "id": "hTcJE9hcAF1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reviews in the CSV files are preliminary shuffled but it is best to consume them in different order during each epoch of training. Shuffle the datapipe and a different random shuffling will be applied in each epoch:"
      ],
      "metadata": {
        "id": "oKirEZ8dCKyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "train_pipe = train_pipe.shuffle(buffer_size = 16384)\n",
        "\n",
        "for label0, index1 in train_pipe.header(10):\n",
        "    print(label0, index1)"
      ],
      "metadata": {
        "id": "4dlwWit9AkOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datapipe yields tuples of labels and indices. To explicitly see these tuples consume the datapipe without unpacking:"
      ],
      "metadata": {
        "id": "XszrYkNuDYM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_pipe.header(10):\n",
        "    print(sample)"
      ],
      "metadata": {
        "id": "hkn6pl-4BOW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is usually done in batches. Let the datapipe yield batches and set the batch size to two just for illustration:"
      ],
      "metadata": {
        "id": "1rObT5XHEdMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "train_pipe = train_pipe.shuffle(buffer_size = 16384)\n",
        "train_pipe = train_pipe.batch(2)\n",
        "\n",
        "for batch in train_pipe.header(10):\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "Uei_YBchB0p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each yielded batch is now a list containing tuples. But training requires that labels and indices are yielded separately. Define a function `collate` that unzips the yielded list of tuples into a tuple containing all labels in the batch and separately all indices in the batch:"
      ],
      "metadata": {
        "id": "UQjQXoXaFlxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    label1, index2 = zip(*batch)\n",
        "    return label1, index2"
      ],
      "metadata": {
        "id": "P6mnrB3ECqwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function could be applied to the datapipe using the ordinary mapping. But there is an equivalent `collate` operation whose name emphasizes that a custom collation of reviews into batches is performed:"
      ],
      "metadata": {
        "id": "iDdHeR6_IGVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "train_pipe = train_pipe.shuffle(buffer_size = 16384)\n",
        "train_pipe = train_pipe.batch(2)\n",
        "train_pipe = train_pipe.collate(collate)\n",
        "\n",
        "for batch in train_pipe.header(10):\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "AK3FftHoCyTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strictly speaking each yielded batch is now a tuple of two tuples. The first inner tuple contains labels of all reviews in the batch and the second inner tuple contains word indices of all reviews in the batch. Consume this datapipe by unpacking the outer tuple into labels and indices:"
      ],
      "metadata": {
        "id": "W8lT0H1YJA_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label1, index2 in train_pipe.header(10):\n",
        "    print(label1)\n",
        "    print(index2)"
      ],
      "metadata": {
        "id": "-8TO5-IBDO1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels are now yielded as an ordinary python tuple. Let the `collate` function convert this tuple to a pytorch tensor:"
      ],
      "metadata": {
        "id": "S3q8o7VjLhmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    label1, index2 = zip(*batch)\n",
        "    label1 = torch.tensor(label1)\n",
        "    return label1, index2"
      ],
      "metadata": {
        "id": "QRfKW0nyEAeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinstantiate the datapipe with the new `collate` function and test it:"
      ],
      "metadata": {
        "id": "t00dipzaMfWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "train_pipe = train_pipe.shuffle(buffer_size = 16384)\n",
        "train_pipe = train_pipe.batch(2)\n",
        "train_pipe = train_pipe.collate(collate)\n",
        "\n",
        "for label1, index2 in train_pipe.header(10):\n",
        "    print(label1)\n",
        "    print(index2)"
      ],
      "metadata": {
        "id": "cHYjICanEKqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word indices of all reviews in the batch are now yielded as a tuple of one-dimensional pytorch tensors. Training requires that it is converted to a two-dimensional tensor whose rows correspond to subsequent reviews in the batch and columns to subsequent words in each review. But the problem here is that different reviews in the same batch have different lenghts. So the shorter reviews will be padded with zeros to reach the length of the longest review in the batch. Recall that zero is the word index of the special word `<pad>`. The padding is done using a dedicated pytorch function:"
      ],
      "metadata": {
        "id": "Z0kupuHaMxG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    label1, index2 = zip(*batch)\n",
        "    label1 = torch.tensor(label1)\n",
        "    index2 = torch.nn.utils.rnn.pad_sequence(index2, padding_value = 0, batch_first = True)\n",
        "    return label1, index2"
      ],
      "metadata": {
        "id": "JAMYP1xFEeM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinstantiate the datapipe and test it:"
      ],
      "metadata": {
        "id": "4_HVVbD2P0c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "train_pipe = train_pipe.shuffle(buffer_size = 16384)\n",
        "train_pipe = train_pipe.batch(2)\n",
        "train_pipe = train_pipe.collate(collate)\n",
        "\n",
        "for label1, index2 in train_pipe.header(10):\n",
        "    print(label1)\n",
        "    print(index2)"
      ],
      "metadata": {
        "id": "Yp5DF-xxEohv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now one row of each two-dimensional index tensor is always padded with zeros at the end. Change the batch size to the value of 64 that we will actually use:"
      ],
      "metadata": {
        "id": "5Z4TJkWlQKdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/train_data.csv'])\n",
        "train_pipe = train_pipe.parse_csv()\n",
        "train_pipe = train_pipe.map(decode)\n",
        "train_pipe = train_pipe.shuffle(buffer_size = 16384)\n",
        "train_pipe = train_pipe.batch(64)\n",
        "train_pipe = train_pipe.collate(collate)"
      ],
      "metadata": {
        "id": "n4KissnOFZh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final version of our training datapipe. Print the shapes of some initial batches and note how different batches have different sizes in the second dimension corresponding to the review length:"
      ],
      "metadata": {
        "id": "vHU9od7pQzsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label1, index2 in train_pipe.header(10):\n",
        "    print(index2.size())"
      ],
      "metadata": {
        "id": "XP8R7WZ8GoBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a corresponding datapipe for validation data. Note that validation data does not require shuffling:"
      ],
      "metadata": {
        "id": "jC3oL6iAGohO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_pipe = torchdata.datapipes.iter.FileOpener(['drive/MyDrive/valid_data.csv'])\n",
        "valid_pipe = valid_pipe.parse_csv()\n",
        "valid_pipe = valid_pipe.map(decode)\n",
        "valid_pipe = valid_pipe.batch(64)\n",
        "valid_pipe = valid_pipe.collate(collate)"
      ],
      "metadata": {
        "id": "QIsfx6H-Ffjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a simple model"
      ],
      "metadata": {
        "id": "LI_guuZQHG7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now build a model to classify the reviews. As input the model takes a padded batch of word indices as yielded by our datapipes. Just to remind how models are constructed in pytorch start with a trivial one that simply returns these indices without any change:"
      ],
      "metadata": {
        "id": "jGcEs1XMZlNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.nn.Module.__init__(self)\n",
        "    def forward(self, index): #(samples, frames)\n",
        "        return index\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "Sb7ugN2NG3qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the comment we denoted the shape of the input index tensor as `(samples, frames)` where `samples` is the number of reviews in the batch and `frames` is the number of words in each review padding included. The output obviously has the same shape. Feed a few batches to this model and print the input and output shapes:"
      ],
      "metadata": {
        "id": "0mNNrXtddIYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, index in valid_pipe.header(10):\n",
        "    print(index.size())\n",
        "    index = model(index)\n",
        "    print(index.size())\n",
        "    print()"
      ],
      "metadata": {
        "id": "LkxGMpjnHdOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will soon need the number of unique words in the vocabulary. Read the vocabulary from the pickle file on your google drive and store its length in a variable called `indices`:"
      ],
      "metadata": {
        "id": "Jvq-01p7HwXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/vocab.pkl', 'rb') as stream:\n",
        "    vocab = pickle.load(stream)\n",
        "\n",
        "indices = len(vocab)"
      ],
      "metadata": {
        "id": "xLoPVqg8GWC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To each unique word in the vocabulary we will assign a real-valued vector that is called a word embedding because it embeds a word in multi-dimensional space. Elements of this vector are called features and we denote their number as `features`. In real-world models the embeddings have hundreds of dimensions but here we limit ourselves to four: "
      ],
      "metadata": {
        "id": "qOSe4BpVe7ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = 4"
      ],
      "metadata": {
        "id": "TRY1PJn4gH4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeddings will be learned from data together with the model. They are in fact part of the model and are stored in a special embedding layer. When constructing this layer we pass the number of unique words in the vocabulary `indices` and the requested embedding size `features`:"
      ],
      "metadata": {
        "id": "CDBEQUytgF5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.encoder = torch.nn.Embedding(indices, features)\n",
        "    def forward(self, index):         #(samples, frames)\n",
        "        feature = self.encoder(index) #(samples, frames, features)\n",
        "        return feature\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "KnDKHe-ZIq7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer is just a lookup table. It takes a word index and returns its corresponding embedding vector. It can also take any multidimensional tensor of word indices and return their embeddings elementwise. The embedding size is added to the input tensor shape as the last dimension. In our model the embedding layer is the first or input one. It takes a batch of word indices and produces a batch of embeddings. Pass a few batches to this model and print the shapes of input indices and output embeddings:"
      ],
      "metadata": {
        "id": "L4PwzyF1jVpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, index in valid_pipe.header(10):\n",
        "    print(index.size())\n",
        "    feature = model(index)\n",
        "    print(feature.size())\n",
        "    print()"
      ],
      "metadata": {
        "id": "tjEd4zjxI_06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding features are trainable parameters of the Embedding layer and thus become trainable parameters of the whole model. They are determined during training via backpropagation together with all the remaining parameters of the model. The output of the embedding layer contains embeddings of all words in a review. But we want to classify the review as a whole. Therefore the embeddings of all the component words must be somehow aggregated into a single set of features describing the whole review. In our simple model we just take the arithmetic mean of all the word embeddings in each review separately:"
      ],
      "metadata": {
        "id": "KnMYta8Tooo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.encoder = torch.nn.Embedding(indices, features)\n",
        "    def forward(self, index):         #(samples, frames)\n",
        "        feature = self.encoder(index) #(samples, frames, features)\n",
        "        feature = feature.mean(1)     #(samples, features)\n",
        "        return feature\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "lE3Yf1CYJpFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the mean does not depend on the order of words in a review so in this way we loose all information about word order. Inspect the input and output shapes for a few batches:"
      ],
      "metadata": {
        "id": "QCyPyJ59Lhai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, index in valid_pipe.header(10):\n",
        "    print(index.size())\n",
        "    feature = model(index)\n",
        "    print(feature.size())\n",
        "    print()"
      ],
      "metadata": {
        "id": "o50OqdkxJxeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the number of average features is fixed and a priori known so we can pass these features to a simple linear layer that will produce logits for later classification:"
      ],
      "metadata": {
        "id": "SIfp5UphMnqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.encoder = torch.nn.Embedding(indices, features)\n",
        "        self.classifier = torch.nn.Linear(features, 1)\n",
        "    def forward(self, index):            #(samples, frames)\n",
        "        feature = self.encoder(index)    #(samples, frames, features)\n",
        "        feature = feature.mean(1)        #(samples, features)\n",
        "        logit = self.classifier(feature) #(samples, 1)\n",
        "        return logit\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "s1vEOKUcJ5RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification layer outputs just one logit because we will use the formalism of binary classification. Print the input and output shapes for a few batches:"
      ],
      "metadata": {
        "id": "KZgsiWvBNRR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, index in valid_pipe.header(10):\n",
        "    print(index.size())\n",
        "    logit = model(index)\n",
        "    print(logit.size())\n",
        "    print()"
      ],
      "metadata": {
        "id": "R2tIwoArKYol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For technical reasons it will be convenient to get rid of the singleton dimension of the logits. Do it by flattening the logits:"
      ],
      "metadata": {
        "id": "QySJQjRHONQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.encoder = torch.nn.Embedding(indices, features)\n",
        "        self.classifier = torch.nn.Linear(features, 1)\n",
        "    def forward(self, index):            #(samples, frames)\n",
        "        feature = self.encoder(index)    #(samples, frames, features)\n",
        "        feature = feature.mean(1)        #(samples, features)\n",
        "        logit = self.classifier(feature) #(samples, 1)\n",
        "        logit = logit.flatten()          #(samples)\n",
        "        return logit\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "76mFTX2yK3Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print some input and output shapes again:"
      ],
      "metadata": {
        "id": "bjNxWM5qOm8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, index in valid_pipe.header(10):\n",
        "    print(index.size())\n",
        "    logit = model(index)\n",
        "    print(logit.size())\n",
        "    print()"
      ],
      "metadata": {
        "id": "LElNAByWK-9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate prediction accuracy"
      ],
      "metadata": {
        "id": "8yX_VacfMIFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has not been trained yet but it is technically usable already. The model yields logits also known as unnormalized probability scores. Print some of them:"
      ],
      "metadata": {
        "id": "gj5B5fc-PeV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for truth_label, index in valid_pipe.header(10):\n",
        "    logit = model(index)\n",
        "    print(logit)"
      ],
      "metadata": {
        "id": "vZR0VVBZMNbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model often gives logits of one sign only but this will change during trainig. A negative or positive logit means that the corresponding review is classified as negative or positive respectively. Construct the predicted labels accordingly:"
      ],
      "metadata": {
        "id": "TYK1nnrSQRU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for truth_label, index in valid_pipe.header(10):\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    print(model_label)"
      ],
      "metadata": {
        "id": "J8I5qNXiMsRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the predicted labels against the ground-true ones to check if the model has hit the correct labels:"
      ],
      "metadata": {
        "id": "0gTE8x8XS1b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for truth_label, index in valid_pipe.header(10):\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    print(hit)"
      ],
      "metadata": {
        "id": "2d8GUJ1jNl9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction accuracy is the number of correcly predicted labels divided by the total number of predictions. Calculate and print the accuracy separately for each batch:"
      ],
      "metadata": {
        "id": "yzQTS4pCTZwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for truth_label, index in valid_pipe.header(10):\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    accuracy = hit.count_nonzero() / hit.numel()\n",
        "    print(accuracy)"
      ],
      "metadata": {
        "id": "81ZilmSfOc_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the accuracy in the whole dataset sum the correct and total predictions over batches and then divide:"
      ],
      "metadata": {
        "id": "JVwCFAv6T3K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_accuracy = torch.tensor(0)\n",
        "samples = torch.tensor(0)\n",
        "for truth_label, index in valid_pipe:\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    valid_accuracy += hit.count_nonzero()\n",
        "    samples += hit.numel()\n",
        "valid_accuracy = valid_accuracy / samples\n",
        "print(valid_accuracy)"
      ],
      "metadata": {
        "id": "mTvs0vCrOy65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This accuracy is close to 0.5 but not because an untrained model gives random predictions. We have already seen that a newly initialized model usually puts most reviews into the same class. So the accuracy is close to 0.5 because half of the reviews are actually negative and half are positive. Instead of printing the ccuracy as a pytorch tensor extract its only value and print it as an ordinary python number:"
      ],
      "metadata": {
        "id": "sL7YGWpRUeGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_accuracy.item())"
      ],
      "metadata": {
        "id": "6B2uiUu4ZeBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format this printout to three digits:"
      ],
      "metadata": {
        "id": "OhIY12MLZq1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('%5.3f' % valid_accuracy)"
      ],
      "metadata": {
        "id": "c2AZ0KHUZwJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate the model"
      ],
      "metadata": {
        "id": "fa4lRGQpP4uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now train the model on the training data and evaluate it on both the training and validation data. Start with this code that iterates in batches over both datasets and calculates the prediction accuracy for each of them:"
      ],
      "metadata": {
        "id": "MONgHHcjYI23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_accuracy = torch.tensor(0)\n",
        "samples = torch.tensor(0)\n",
        "for truth_label, index in valid_pipe:\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    valid_accuracy += hit.count_nonzero()\n",
        "    samples += hit.numel()\n",
        "valid_accuracy = valid_accuracy / samples\n",
        "train_accuracy = torch.tensor(0)\n",
        "samples = torch.tensor(0)\n",
        "for truth_label, index in train_pipe:\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    train_accuracy += hit.count_nonzero()\n",
        "    samples += hit.numel()\n",
        "train_accuracy = train_accuracy / samples\n",
        "print('%5.3f %5.3f' % (train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "id": "5RtGS8cJP-gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each batch of the training data calculate the binary cross entropy as the loss value calculate its gradients by automatic differentiation and perform a minimization step by using an optimizer that we choose to be Adam:"
      ],
      "metadata": {
        "id": "Q_ORxy-uavwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "valid_accuracy = torch.tensor(0)\n",
        "samples = torch.tensor(0)\n",
        "for truth_label, index in valid_pipe:\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    valid_accuracy += hit.count_nonzero()\n",
        "    samples += hit.numel()\n",
        "valid_accuracy = valid_accuracy / samples\n",
        "train_accuracy = torch.tensor(0)\n",
        "samples = torch.tensor(0)\n",
        "for truth_label, index in train_pipe:\n",
        "    logit = model(index)\n",
        "    model_label = logit.gt(0).float()\n",
        "    hit = model_label.eq(truth_label)\n",
        "    train_accuracy += hit.count_nonzero()\n",
        "    samples += hit.numel()\n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, truth_label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "train_accuracy = train_accuracy / samples\n",
        "print('%5.3f %5.3f' % (train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "id": "AnLQPOs4Qrai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training accuracy is a bit higher now because some learning has already been done. One iteration through the entire training set is called an epoch. Perform a reasonable number of epochs. These calculations may be quite slow so stop the execution of this cell at any time:"
      ],
      "metadata": {
        "id": "dsfqyYbqbwfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "for epoch in range(128):\n",
        "    valid_accuracy = torch.tensor(0)\n",
        "    samples = torch.tensor(0)\n",
        "    for truth_label, index in valid_pipe:\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        valid_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "    valid_accuracy = valid_accuracy / samples\n",
        "    train_accuracy = torch.tensor(0)\n",
        "    samples = torch.tensor(0)\n",
        "    for truth_label, index in train_pipe:\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        train_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, truth_label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_accuracy = train_accuracy / samples\n",
        "    print('%5i %5.3f %5.3f' % (epoch, train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "id": "D-fBjD_yTQsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To later plot the training and vaidation accuracy store their values from every epoch in a list:"
      ],
      "metadata": {
        "id": "7uG31Rz_eiWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = list()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "for epoch in range(128):\n",
        "    valid_accuracy = torch.tensor(0)\n",
        "    samples = torch.tensor(0)\n",
        "    for truth_label, index in valid_pipe:\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        valid_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "    valid_accuracy = valid_accuracy / samples\n",
        "    train_accuracy = torch.tensor(0)\n",
        "    samples = torch.tensor(0)\n",
        "    for truth_label, index in train_pipe:\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        train_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, truth_label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_accuracy = train_accuracy / samples\n",
        "    history.append((epoch, train_accuracy.item(), valid_accuracy.item()))\n",
        "    print('%5i %5.3f %5.3f' % (epoch, train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "id": "8gJmTKkSejOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of training is to obtain a model with maximum accuracy on the validation data. Save the model to disk whenever the validation accuracy increases:"
      ],
      "metadata": {
        "id": "yB1XOZ34eIbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = list()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "max_valid_accuracy = torch.tensor(0.)\n",
        "for epoch in range(128):\n",
        "    valid_accuracy = torch.tensor(0)\n",
        "    samples = torch.tensor(0)\n",
        "    for truth_label, index in valid_pipe:\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        valid_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "    valid_accuracy = valid_accuracy / samples\n",
        "    if max_valid_accuracy.lt(valid_accuracy):\n",
        "        max_valid_accuracy = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    train_accuracy = torch.tensor(0)\n",
        "    samples = torch.tensor(0)\n",
        "    for truth_label, index in train_pipe:\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        train_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, truth_label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_accuracy = train_accuracy / samples\n",
        "    history.append((epoch, train_accuracy.item(), valid_accuracy.item()))\n",
        "    print('%5i %5.3f %5.3f' % (epoch, train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "id": "YNM4PwqHfKpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will soon use the saved model to examine its properties. If you want to save the model permanently change the target folder to your google drive. To speed up the calculations perform them on GPU. To do this you need a GPU-enabled execution environment. If you have not set it at the start do it now and rerun the notebook from the very beginning. Create the model once again and move it to GPU:"
      ],
      "metadata": {
        "id": "mw7zAWvGf_LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().cuda()"
      ],
      "metadata": {
        "id": "ICZrJs3PVZ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move all the tensors in the training loop to GPU. Unfortunately the free colab GPUs are quite slow so instead of running the code you can display the hidden output preparred earlier. If you want to train the model yourself execute the cell without interruption:"
      ],
      "metadata": {
        "id": "X8z_jliDiLjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = list()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "max_valid_accuracy = torch.tensor(0.).cuda()\n",
        "for epoch in range(128):\n",
        "    valid_accuracy = torch.tensor(0).cuda()\n",
        "    samples = torch.tensor(0).cuda()\n",
        "    for truth_label, index in valid_pipe:\n",
        "        truth_label, index = truth_label.cuda(), index.cuda()\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        valid_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "    valid_accuracy = valid_accuracy / samples\n",
        "    if max_valid_accuracy.lt(valid_accuracy):\n",
        "        max_valid_accuracy = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    train_accuracy = torch.tensor(0).cuda()\n",
        "    samples = torch.tensor(0).cuda()\n",
        "    for truth_label, index in train_pipe:\n",
        "        truth_label, index = truth_label.cuda(), index.cuda()\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        train_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, truth_label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_accuracy = train_accuracy / samples\n",
        "    history.append((epoch, train_accuracy.item(), valid_accuracy.item()))\n",
        "    print('%5i %5.3f %5.3f' % (epoch, train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "id": "9zrWztAfiM4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "de43d547-8292-4865-b0f7-7fb12130b5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 0.539 0.500\n",
            "    1 0.683 0.650\n",
            "    2 0.735 0.715\n",
            "    3 0.775 0.717\n",
            "    4 0.792 0.769\n",
            "    5 0.811 0.792\n",
            "    6 0.824 0.803\n",
            "    7 0.837 0.814\n",
            "    8 0.849 0.821\n",
            "    9 0.858 0.837\n",
            "   10 0.866 0.847\n",
            "   11 0.873 0.853\n",
            "   12 0.880 0.858\n",
            "   13 0.883 0.863\n",
            "   14 0.888 0.865\n",
            "   15 0.891 0.870\n",
            "   16 0.893 0.871\n",
            "   17 0.896 0.876\n",
            "   18 0.899 0.879\n",
            "   19 0.901 0.880\n",
            "   20 0.903 0.881\n",
            "   21 0.905 0.885\n",
            "   22 0.908 0.887\n",
            "   23 0.910 0.889\n",
            "   24 0.912 0.889\n",
            "   25 0.914 0.891\n",
            "   26 0.915 0.892\n",
            "   27 0.917 0.890\n",
            "   28 0.919 0.892\n",
            "   29 0.920 0.892\n",
            "   30 0.922 0.894\n",
            "   31 0.924 0.895\n",
            "   32 0.925 0.896\n",
            "   33 0.927 0.897\n",
            "   34 0.928 0.897\n",
            "   35 0.930 0.898\n",
            "   36 0.931 0.899\n",
            "   37 0.933 0.900\n",
            "   38 0.934 0.900\n",
            "   39 0.934 0.900\n",
            "   40 0.935 0.899\n",
            "   41 0.937 0.900\n",
            "   42 0.938 0.900\n",
            "   43 0.939 0.901\n",
            "   44 0.940 0.901\n",
            "   45 0.940 0.900\n",
            "   46 0.942 0.902\n",
            "   47 0.943 0.902\n",
            "   48 0.944 0.903\n",
            "   49 0.945 0.903\n",
            "   50 0.946 0.902\n",
            "   51 0.947 0.902\n",
            "   52 0.947 0.903\n",
            "   53 0.949 0.903\n",
            "   54 0.950 0.905\n",
            "   55 0.951 0.904\n",
            "   56 0.952 0.905\n",
            "   57 0.953 0.905\n",
            "   58 0.954 0.905\n",
            "   59 0.955 0.905\n",
            "   60 0.956 0.906\n",
            "   61 0.957 0.906\n",
            "   62 0.957 0.906\n",
            "   63 0.958 0.906\n",
            "   64 0.959 0.906\n",
            "   65 0.960 0.906\n",
            "   66 0.960 0.906\n",
            "   67 0.961 0.907\n",
            "   68 0.962 0.906\n",
            "   69 0.962 0.907\n",
            "   70 0.963 0.907\n",
            "   71 0.964 0.905\n",
            "   72 0.965 0.907\n",
            "   73 0.965 0.905\n",
            "   74 0.966 0.906\n",
            "   75 0.966 0.907\n",
            "   76 0.967 0.907\n",
            "   77 0.968 0.906\n",
            "   78 0.968 0.906\n",
            "   79 0.969 0.906\n",
            "   80 0.969 0.905\n",
            "   81 0.970 0.905\n",
            "   82 0.970 0.906\n",
            "   83 0.971 0.906\n",
            "   84 0.972 0.905\n",
            "   85 0.972 0.907\n",
            "   86 0.972 0.906\n",
            "   87 0.973 0.906\n",
            "   88 0.973 0.905\n",
            "   89 0.974 0.905\n",
            "   90 0.974 0.906\n",
            "   91 0.975 0.905\n",
            "   92 0.976 0.905\n",
            "   93 0.975 0.905\n",
            "   94 0.976 0.905\n",
            "   95 0.976 0.905\n",
            "   96 0.977 0.905\n",
            "   97 0.977 0.904\n",
            "   98 0.978 0.905\n",
            "   99 0.978 0.904\n",
            "  100 0.978 0.905\n",
            "  101 0.979 0.904\n",
            "  102 0.979 0.903\n",
            "  103 0.980 0.903\n",
            "  104 0.980 0.904\n",
            "  105 0.981 0.903\n",
            "  106 0.981 0.905\n",
            "  107 0.981 0.904\n",
            "  108 0.981 0.903\n",
            "  109 0.982 0.901\n",
            "  110 0.982 0.903\n",
            "  111 0.982 0.903\n",
            "  112 0.983 0.902\n",
            "  113 0.983 0.903\n",
            "  114 0.983 0.902\n",
            "  115 0.983 0.903\n",
            "  116 0.983 0.903\n",
            "  117 0.984 0.901\n",
            "  118 0.984 0.902\n",
            "  119 0.984 0.902\n",
            "  120 0.985 0.902\n",
            "  121 0.985 0.901\n",
            "  122 0.985 0.900\n",
            "  123 0.986 0.900\n",
            "  124 0.986 0.901\n",
            "  125 0.986 0.901\n",
            "  126 0.987 0.901\n",
            "  127 0.987 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation accuracy should reach 90% This is very good given that we use only four embedding features and a trivial model that completely neglects the order of words. To obtain higher accuracy it is necessary to use a more complex model and pretrain it on a much larger corpus of text. Models trained only on the IMDB dataset usually achieve validation accuracy lower than 90% so worse than this trivial one. To plot the accuracies unzip the training history into three lists:"
      ],
      "metadata": {
        "id": "k5u80nA9j4Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch, train_accuracy, valid_accuracy = zip(*history)"
      ],
      "metadata": {
        "id": "8zg6yvVSbAps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now make the plot:"
      ],
      "metadata": {
        "id": "Pzp03M6jlxbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "plt.plot(epoch, train_accuracy)\n",
        "plt.plot(epoch, valid_accuracy)"
      ],
      "metadata": {
        "id": "B2QQf0wSbZKU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "7d4ad960-aeae-4fd4-dd12-57b9acf28471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0edd29de50>]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training accuracy only grows and would eventually reach 100% The validation accuracy attains a maximum of about 90% and then drops. This is a typical behavior due to overfitting."
      ],
      "metadata": {
        "id": "ZqPD9VEbmCL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the embeddings"
      ],
      "metadata": {
        "id": "v85DnyRSmxSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now see what happens if we reduce the number of embedding features to one:"
      ],
      "metadata": {
        "id": "v08yxdTa1Um7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = 1"
      ],
      "metadata": {
        "id": "mo-vsb65kIbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct and instantiate the model:"
      ],
      "metadata": {
        "id": "7cmsyeyA1huX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        self.encoder = torch.nn.Embedding(indices, features)\n",
        "        self.classifier = torch.nn.Linear(features, 1)\n",
        "    def forward(self, index):            #(samples, frames)\n",
        "        feature = self.encoder(index)    #(samples, frames, features)\n",
        "        feature = feature.mean(1)        #(samples, features)\n",
        "        logit = self.classifier(feature) #(samples, 1)\n",
        "        logit = logit.flatten()          #(samples)\n",
        "        return logit\n",
        "\n",
        "model = Model().cuda()"
      ],
      "metadata": {
        "id": "58e0WMMdkls7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train it:"
      ],
      "metadata": {
        "id": "uoKYkJeo1pU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = list()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "max_valid_accuracy = torch.tensor(0.).cuda()\n",
        "for epoch in range(128):\n",
        "    valid_accuracy = torch.tensor(0).cuda()\n",
        "    samples = torch.tensor(0).cuda()\n",
        "    for truth_label, index in valid_pipe:\n",
        "        truth_label, index = truth_label.cuda(), index.cuda()\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        valid_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "    valid_accuracy = valid_accuracy / samples\n",
        "    if max_valid_accuracy.lt(valid_accuracy):\n",
        "        max_valid_accuracy = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    train_accuracy = torch.tensor(0).cuda()\n",
        "    samples = torch.tensor(0).cuda()\n",
        "    for truth_label, index in train_pipe:\n",
        "        truth_label, index = truth_label.cuda(), index.cuda()\n",
        "        logit = model(index)\n",
        "        model_label = logit.gt(0).float()\n",
        "        hit = model_label.eq(truth_label)\n",
        "        train_accuracy += hit.count_nonzero()\n",
        "        samples += hit.numel()\n",
        "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, truth_label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_accuracy = train_accuracy / samples\n",
        "    history.append((epoch, train_accuracy.item(), valid_accuracy.item()))\n",
        "    print('%5i %5.3f %5.3f' % (epoch, train_accuracy, valid_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Khm6XALzkOz0",
        "outputId": "985db0e7-e793-4b9a-dce4-e3b72eaf56ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 0.509 0.500\n",
            "    1 0.520 0.501\n",
            "    2 0.540 0.511\n",
            "    3 0.558 0.528\n",
            "    4 0.587 0.547\n",
            "    5 0.634 0.589\n",
            "    6 0.672 0.618\n",
            "    7 0.731 0.707\n",
            "    8 0.754 0.730\n",
            "    9 0.768 0.745\n",
            "   10 0.783 0.764\n",
            "   11 0.787 0.771\n",
            "   12 0.795 0.779\n",
            "   13 0.803 0.782\n",
            "   14 0.809 0.791\n",
            "   15 0.817 0.797\n",
            "   16 0.823 0.802\n",
            "   17 0.831 0.808\n",
            "   18 0.836 0.810\n",
            "   19 0.843 0.818\n",
            "   20 0.849 0.825\n",
            "   21 0.854 0.831\n",
            "   22 0.859 0.838\n",
            "   23 0.864 0.842\n",
            "   24 0.868 0.846\n",
            "   25 0.872 0.849\n",
            "   26 0.876 0.854\n",
            "   27 0.879 0.855\n",
            "   28 0.881 0.858\n",
            "   29 0.884 0.861\n",
            "   30 0.886 0.863\n",
            "   31 0.888 0.866\n",
            "   32 0.890 0.866\n",
            "   33 0.892 0.869\n",
            "   34 0.894 0.870\n",
            "   35 0.895 0.871\n",
            "   36 0.896 0.873\n",
            "   37 0.898 0.875\n",
            "   38 0.900 0.877\n",
            "   39 0.901 0.878\n",
            "   40 0.902 0.879\n",
            "   41 0.903 0.881\n",
            "   42 0.904 0.882\n",
            "   43 0.905 0.881\n",
            "   44 0.908 0.883\n",
            "   45 0.909 0.883\n",
            "   46 0.910 0.885\n",
            "   47 0.911 0.885\n",
            "   48 0.912 0.886\n",
            "   49 0.913 0.887\n",
            "   50 0.913 0.888\n",
            "   51 0.915 0.888\n",
            "   52 0.916 0.888\n",
            "   53 0.917 0.890\n",
            "   54 0.918 0.890\n",
            "   55 0.919 0.890\n",
            "   56 0.919 0.892\n",
            "   57 0.920 0.892\n",
            "   58 0.922 0.891\n",
            "   59 0.922 0.893\n",
            "   60 0.923 0.894\n",
            "   61 0.925 0.893\n",
            "   62 0.925 0.894\n",
            "   63 0.926 0.895\n",
            "   64 0.927 0.895\n",
            "   65 0.927 0.896\n",
            "   66 0.929 0.896\n",
            "   67 0.929 0.896\n",
            "   68 0.930 0.897\n",
            "   69 0.931 0.895\n",
            "   70 0.931 0.898\n",
            "   71 0.932 0.898\n",
            "   72 0.933 0.898\n",
            "   73 0.933 0.899\n",
            "   74 0.934 0.898\n",
            "   75 0.935 0.899\n",
            "   76 0.935 0.899\n",
            "   77 0.936 0.899\n",
            "   78 0.936 0.899\n",
            "   79 0.937 0.899\n",
            "   80 0.939 0.900\n",
            "   81 0.939 0.900\n",
            "   82 0.940 0.900\n",
            "   83 0.940 0.900\n",
            "   84 0.941 0.901\n",
            "   85 0.942 0.901\n",
            "   86 0.942 0.901\n",
            "   87 0.943 0.902\n",
            "   88 0.944 0.902\n",
            "   89 0.944 0.902\n",
            "   90 0.945 0.902\n",
            "   91 0.946 0.902\n",
            "   92 0.947 0.902\n",
            "   93 0.947 0.902\n",
            "   94 0.948 0.902\n",
            "   95 0.948 0.902\n",
            "   96 0.949 0.902\n",
            "   97 0.950 0.902\n",
            "   98 0.950 0.903\n",
            "   99 0.951 0.902\n",
            "  100 0.951 0.902\n",
            "  101 0.952 0.902\n",
            "  102 0.952 0.902\n",
            "  103 0.953 0.902\n",
            "  104 0.954 0.903\n",
            "  105 0.955 0.902\n",
            "  106 0.955 0.902\n",
            "  107 0.955 0.902\n",
            "  108 0.956 0.902\n",
            "  109 0.956 0.902\n",
            "  110 0.957 0.902\n",
            "  111 0.957 0.901\n",
            "  112 0.958 0.902\n",
            "  113 0.958 0.902\n",
            "  114 0.958 0.902\n",
            "  115 0.959 0.901\n",
            "  116 0.959 0.902\n",
            "  117 0.960 0.902\n",
            "  118 0.961 0.902\n",
            "  119 0.961 0.901\n",
            "  120 0.961 0.901\n",
            "  121 0.962 0.902\n",
            "  122 0.962 0.902\n",
            "  123 0.963 0.902\n",
            "  124 0.963 0.902\n",
            "  125 0.964 0.902\n",
            "  126 0.964 0.901\n",
            "  127 0.965 0.901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the accuracies:"
      ],
      "metadata": {
        "id": "1nV4QgbKrmuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch, train_accuracy, valid_accuracy = zip(*history)\n",
        "\n",
        "plt.grid()\n",
        "plt.plot(epoch, train_accuracy)\n",
        "plt.plot(epoch, valid_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "yKkUX0IVrsWq",
        "outputId": "3143cae5-bdd0-422e-c6ba-95dc84b3a7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0edd446910>]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the two plots. Now the model has less parameters and learns slower on the training data. Yet for the same reason there is less overfitting so the validation accuracy is roughly the same and about 90% Since we achieve the same 90% accuracy with only one embedding feature it should be already clear what the model actually does... It assigns positive embeddings to positive words and negative embeddings to negative words or vice versa. Then it calculates a mean off all the embeddings in a review. To check it move the modem to CPU and load the recordered parameters:"
      ],
      "metadata": {
        "id": "swu50AE8r5y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cpu()\n",
        "model.load_state_dict(torch.load('model.pt'))"
      ],
      "metadata": {
        "id": "Jwh4Z04EthC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this way we load the parameters from the epoch when the model performed best on the validation data. Get the list of unique words in the vocabulary:"
      ],
      "metadata": {
        "id": "vT40opNk28Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = vocab.get_itos()"
      ],
      "metadata": {
        "id": "u3vQdz393EQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now iterate through some initial word indices. For each index get the corresponding word from the list. Also pass the index to the embedding layer and get the corresponding feature. Print them:"
      ],
      "metadata": {
        "id": "fIWrvX3s3iyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(1024):\n",
        "    print('%5i %16s %7.3f' % (index, itos[index], model.encoder(torch.tensor([[index]]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyKvGMW8uZR-",
        "outputId": "bbb1cc7c-d74c-4152-bdce-18112a2ff763",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0            <pad>  -0.037\n",
            "    1            <unk>   1.018\n",
            "    2              the  -0.232\n",
            "    3              and  -0.859\n",
            "    4                a   0.112\n",
            "    5               of  -0.007\n",
            "    6               to   0.725\n",
            "    7               is  -1.023\n",
            "    8               in  -0.258\n",
            "    9               it  -2.203\n",
            "   10                i  -0.450\n",
            "   11             this   1.371\n",
            "   12             that   0.092\n",
            "   13              was   1.804\n",
            "   14               as  -0.123\n",
            "   15             with  -0.434\n",
            "   16              for   0.151\n",
            "   17            movie  -0.608\n",
            "   18              but   0.494\n",
            "   19             film  -0.831\n",
            "   20               on   0.674\n",
            "   21              not   3.025\n",
            "   22              you  -4.050\n",
            "   23              are   0.729\n",
            "   24              his  -0.828\n",
            "   25             have   1.727\n",
            "   26               be   0.661\n",
            "   27              one  -0.460\n",
            "   28               he  -1.998\n",
            "   29              its  -2.738\n",
            "   30              all  -0.205\n",
            "   31               at   0.225\n",
            "   32               by   0.264\n",
            "   33               an  -1.021\n",
            "   34             they   0.374\n",
            "   35               so  -1.028\n",
            "   36              who  -1.381\n",
            "   37             from  -0.749\n",
            "   38             like   0.108\n",
            "   39               or   0.781\n",
            "   40             just   4.236\n",
            "   41              her  -0.570\n",
            "   42              out  -1.719\n",
            "   43               if   1.281\n",
            "   44            about   0.348\n",
            "   45              has  -0.549\n",
            "   46            there   1.616\n",
            "   47             some   0.879\n",
            "   48             what  -1.697\n",
            "   49             good  -4.715\n",
            "   50             when  -0.952\n",
            "   51             more  -3.144\n",
            "   52             very  -2.798\n",
            "   53               up  -0.616\n",
            "   54               no   3.858\n",
            "   55             time  -1.683\n",
            "   56             even   5.465\n",
            "   57               my  -3.517\n",
            "   58            would   3.858\n",
            "   59              she  -0.733\n",
            "   60            which   0.200\n",
            "   61             only   5.210\n",
            "   62           really  -0.472\n",
            "   63              see  -4.830\n",
            "   64            story  -0.372\n",
            "   65            their  -1.788\n",
            "   66             were   0.974\n",
            "   67              had   0.708\n",
            "   68              can  -2.627\n",
            "   69             well  -7.519\n",
            "   70               me  -0.955\n",
            "   71             than   0.443\n",
            "   72             much   2.064\n",
            "   73               we  -1.112\n",
            "   74              bad  14.868\n",
            "   75              get  -0.199\n",
            "   76            great -16.402\n",
            "   77               do   3.177\n",
            "   78             been   2.883\n",
            "   79            other  -2.819\n",
            "   80             will  -2.297\n",
            "   81             into   1.149\n",
            "   82           people  -1.541\n",
            "   83             also  -3.726\n",
            "   84          because   0.893\n",
            "   85            first  -2.265\n",
            "   86              him  -2.242\n",
            "   87              how   1.017\n",
            "   88             most  -4.172\n",
            "   89             dont   2.687\n",
            "   90           movies  -0.708\n",
            "   91             made   0.880\n",
            "   92             make   3.648\n",
            "   93             then   3.452\n",
            "   94             them   0.362\n",
            "   95            films   0.003\n",
            "   96              way  -2.770\n",
            "   97              too   2.360\n",
            "   98            could   2.853\n",
            "   99              any   4.877\n",
            "  100       characters   0.691\n",
            "  101            after  -0.120\n",
            "  102            think  -4.653\n",
            "  103            watch   0.020\n",
            "  104              two   0.006\n",
            "  105             seen  -4.765\n",
            "  106             many  -0.843\n",
            "  107        character   0.473\n",
            "  108            being   1.796\n",
            "  109             love  -5.309\n",
            "  110           acting   5.976\n",
            "  111             plot   5.337\n",
            "  112            never   0.369\n",
            "  113             life  -1.905\n",
            "  114              did   1.040\n",
            "  115            where   0.526\n",
            "  116             best -11.851\n",
            "  117             know  -1.793\n",
            "  118           little  -2.535\n",
            "  119             show  -1.101\n",
            "  120             over   1.146\n",
            "  121             ever  -0.808\n",
            "  122              off   3.545\n",
            "  123             does   0.945\n",
            "  124             your   1.628\n",
            "  125           better   3.923\n",
            "  126              man  -0.705\n",
            "  127              end  -2.045\n",
            "  128            scene   0.393\n",
            "  129            still  -7.023\n",
            "  130              say  -0.597\n",
            "  131            these   1.408\n",
            "  132                s   0.802\n",
            "  133             here   2.076\n",
            "  134           scenes   0.716\n",
            "  135              why   5.440\n",
            "  136            while  -1.324\n",
            "  137        something   1.797\n",
            "  138             such   2.786\n",
            "  139               go  -0.722\n",
            "  140          through   2.827\n",
            "  141             back  -1.600\n",
            "  142           should   4.019\n",
            "  143               im   2.371\n",
            "  144             real  -2.599\n",
            "  145            those  -3.700\n",
            "  146         watching   4.896\n",
            "  147           actors   3.698\n",
            "  148            thing   3.151\n",
            "  149              now  -4.076\n",
            "  150            didnt   3.413\n",
            "  151           doesnt   4.777\n",
            "  152            years  -2.835\n",
            "  153           though  -2.345\n",
            "  154            funny  -0.408\n",
            "  155              old   3.139\n",
            "  156           before  -2.312\n",
            "  157             work   0.837\n",
            "  158          another   2.255\n",
            "  159         actually   1.560\n",
            "  160          nothing  11.946\n",
            "  161            makes  -6.831\n",
            "  162             look   1.174\n",
            "  163         director   5.996\n",
            "  164            going  -1.086\n",
            "  165             find  -3.837\n",
            "  166             same   1.329\n",
            "  167              lot  -4.347\n",
            "  168            every   1.394\n",
            "  169              new  -0.240\n",
            "  170              few   1.132\n",
            "  171            again  -3.131\n",
            "  172             part   0.314\n",
            "  173             cant   4.030\n",
            "  174             down   2.270\n",
            "  175            thats   1.596\n",
            "  176           things  -3.452\n",
            "  177             cast  -1.174\n",
            "  178               us  -2.738\n",
            "  179           horror  -0.889\n",
            "  180            quite  -5.329\n",
            "  181           pretty   0.546\n",
            "  182            world  -4.460\n",
            "  183             want   1.523\n",
            "  184            seems   5.688\n",
            "  185           around   1.249\n",
            "  186              big   0.488\n",
            "  187            young  -0.239\n",
            "  188          however   1.540\n",
            "  189             fact   0.068\n",
            "  190              got  -2.825\n",
            "  191             take  -2.154\n",
            "  192             long   1.931\n",
            "  193          thought  -3.692\n",
            "  194           enough   3.268\n",
            "  195             both  -4.722\n",
            "  196              ive  -0.698\n",
            "  197             give   1.712\n",
            "  198              own  -6.172\n",
            "  199            right  -3.380\n",
            "  200              may  -5.311\n",
            "  201           action  -2.791\n",
            "  202           series  -4.944\n",
            "  203           comedy  -1.038\n",
            "  204          between   0.062\n",
            "  205            music  -3.153\n",
            "  206             must  -3.888\n",
            "  207             role  -1.066\n",
            "  208          without  -2.419\n",
            "  209             isnt   3.419\n",
            "  210              saw  -3.311\n",
            "  211         original   5.575\n",
            "  212           always  -7.491\n",
            "  213            times  -4.606\n",
            "  214              guy   1.700\n",
            "  215             gets  -0.274\n",
            "  216           almost   0.384\n",
            "  217             come  -0.469\n",
            "  218      interesting   6.546\n",
            "  219            whole   2.650\n",
            "  220            point   4.924\n",
            "  221           theres   1.259\n",
            "  222            least   6.437\n",
            "  223             done  -1.456\n",
            "  224              far   4.531\n",
            "  225              bit  -8.345\n",
            "  226          minutes  12.153\n",
            "  227             feel  -5.202\n",
            "  228           script  10.439\n",
            "  229              hes  -0.636\n",
            "  230               am   4.998\n",
            "  231           making   3.341\n",
            "  232         anything   6.258\n",
            "  233            might   4.441\n",
            "  234           family  -3.597\n",
            "  235            since  -4.178\n",
            "  236             last  -3.166\n",
            "  237         probably   0.228\n",
            "  238               tv  -0.363\n",
            "  239      performance  -3.770\n",
            "  240              yet  -4.195\n",
            "  241             kind  -0.225\n",
            "  242             away   2.787\n",
            "  243              fun -12.762\n",
            "  244            worst  30.600\n",
            "  245             sure  -4.292\n",
            "  246             girl   0.174\n",
            "  247           anyone  -2.702\n",
            "  248             each  -4.791\n",
            "  249           played  -3.471\n",
            "  250             hard   2.548\n",
            "  251           rather   2.660\n",
            "  252            found   1.559\n",
            "  253              day  -3.002\n",
            "  254            woman   1.928\n",
            "  255           screen   0.273\n",
            "  256       especially -10.213\n",
            "  257         although  -4.561\n",
            "  258          looking   5.148\n",
            "  259          believe   1.896\n",
            "  260              our  -3.882\n",
            "  261           trying   6.275\n",
            "  262            shows  -4.856\n",
            "  263           having  -2.268\n",
            "  264              set   2.499\n",
            "  265              dvd  -9.984\n",
            "  266           course  -2.732\n",
            "  267       everything  -1.309\n",
            "  268             goes   3.132\n",
            "  269           ending  -1.081\n",
            "  270              put   0.382\n",
            "  271        different  -5.695\n",
            "  272            comes   2.300\n",
            "  273            maybe   4.813\n",
            "  274            place  -0.637\n",
            "  275            worth  -6.092\n",
            "  276             book   4.862\n",
            "  277            three   2.238\n",
            "  278             main   2.422\n",
            "  279             once  -3.514\n",
            "  280         american  -0.374\n",
            "  281            sense   1.171\n",
            "  282           reason   7.180\n",
            "  283            wasnt   3.118\n",
            "  284          watched   0.219\n",
            "  285             play  -0.734\n",
            "  286          effects   1.285\n",
            "  287             true  -7.379\n",
            "  288            looks   8.799\n",
            "  289            money  10.612\n",
            "  290            actor   5.489\n",
            "  291              job  -9.366\n",
            "  292            plays  -2.660\n",
            "  293          someone   4.386\n",
            "  294         together  -5.289\n",
            "  295              war  -2.460\n",
            "  296             high   0.488\n",
            "  297             half   7.169\n",
            "  298             year   3.401\n",
            "  299           during   0.532\n",
            "  300          instead  11.032\n",
            "  301         everyone  -5.921\n",
            "  302            takes  -3.638\n",
            "  303         audience   1.416\n",
            "  304             said   2.794\n",
            "  305             seem  -0.096\n",
            "  306            black   1.211\n",
            "  307            later  -4.812\n",
            "  308          special  -1.116\n",
            "  309        beautiful  -8.077\n",
            "  310          himself  -0.974\n",
            "  311             left   5.660\n",
            "  312          version   0.890\n",
            "  313           seeing  -0.365\n",
            "  314            night  -1.291\n",
            "  315             john  -1.619\n",
            "  316             shot  -0.057\n",
            "  317        excellent -20.834\n",
            "  318            house  -1.161\n",
            "  319             idea   8.087\n",
            "  320             star   0.207\n",
            "  321            death  -4.088\n",
            "  322             mind  -3.633\n",
            "  323             nice  -4.646\n",
            "  324              fan  -1.541\n",
            "  325             wife   1.258\n",
            "  326            short  -2.254\n",
            "  327           budget  -2.970\n",
            "  328             used   1.109\n",
            "  329       completely   7.484\n",
            "  330            youre   1.728\n",
            "  331             else   3.067\n",
            "  332           simply   3.491\n",
            "  333          friends   0.402\n",
            "  334           second   0.151\n",
            "  335             kids   0.600\n",
            "  336             poor  18.893\n",
            "  337              top  -6.671\n",
            "  338             less   3.822\n",
            "  339             read   2.105\n",
            "  340             home  -1.169\n",
            "  341              men  -0.797\n",
            "  342            along  -1.910\n",
            "  343             dead  -0.605\n",
            "  344             line   3.252\n",
            "  345           either   5.486\n",
            "  346             help  -1.931\n",
            "  347           camera   6.569\n",
            "  348            wrong   6.709\n",
            "  349           boring  23.478\n",
            "  350          classic  -9.434\n",
            "  351              try   0.094\n",
            "  352            given   3.410\n",
            "  353              use   0.988\n",
            "  354            enjoy  -8.004\n",
            "  355              low   3.633\n",
            "  356             next   1.193\n",
            "  357             full  -3.020\n",
            "  358     performances  -9.068\n",
            "  359       production   1.570\n",
            "  360           stupid  13.802\n",
            "  361             need   0.218\n",
            "  362        hollywood  -2.201\n",
            "  363            truly  -6.079\n",
            "  364            until  -5.102\n",
            "  365           father   2.990\n",
            "  366             rest   5.872\n",
            "  367              sex   3.951\n",
            "  368            awful  26.632\n",
            "  369           school  -1.513\n",
            "  370           couple   3.920\n",
            "  371            start   1.381\n",
            "  372            women   4.559\n",
            "  373            video   1.373\n",
            "  374             tell   0.031\n",
            "  375         terrible  21.343\n",
            "  376        recommend  -7.414\n",
            "  377         remember  -3.420\n",
            "  378           others  -2.209\n",
            "  379              let   2.440\n",
            "  380       understand  -1.849\n",
            "  381             mean   5.158\n",
            "  382             came  -1.660\n",
            "  383          perhaps   0.502\n",
            "  384          getting   0.167\n",
            "  385             name   3.574\n",
            "  386           itself  -0.935\n",
            "  387          moments  -0.381\n",
            "  388          episode  -4.414\n",
            "  389            style  -0.577\n",
            "  390             face   0.470\n",
            "  391          playing   1.681\n",
            "  392            human  -6.227\n",
            "  393            stars  -0.051\n",
            "  394             keep  -4.751\n",
            "  395          perfect -18.932\n",
            "  396        wonderful -15.471\n",
            "  397            small  -3.141\n",
            "  398          written  -0.495\n",
            "  399           person   1.694\n",
            "  400            often  -4.376\n",
            "  401            piece   2.738\n",
            "  402             guys  -3.283\n",
            "  403            early   1.900\n",
            "  404            doing   4.576\n",
            "  405            lines   2.172\n",
            "  406       definitely -13.993\n",
            "  407          couldnt   8.749\n",
            "  408             head   0.556\n",
            "  409         dialogue   4.824\n",
            "  410              boy  -0.031\n",
            "  411            gives  -5.321\n",
            "  412        certainly  -3.703\n",
            "  413       absolutely   1.330\n",
            "  414            worse  16.513\n",
            "  415            laugh   1.021\n",
            "  416            liked -10.049\n",
            "  417             case   0.490\n",
            "  418               oh  10.058\n",
            "  419           become  -2.513\n",
            "  420            title   2.686\n",
            "  421             live  -2.650\n",
            "  422           mother   2.065\n",
            "  423             went   3.509\n",
            "  424     entertaining -12.194\n",
            "  425              yes  -1.746\n",
            "  426            loved -14.387\n",
            "  427           called   4.795\n",
            "  428             lost   3.841\n",
            "  429          picture  -0.794\n",
            "  430           entire   5.570\n",
            "  431             sort   4.674\n",
            "  432          finally  -2.458\n",
            "  433             hope  -2.467\n",
            "  434          overall  -4.828\n",
            "  435               mr   1.851\n",
            "  436          problem   9.242\n",
            "  437            based  -1.003\n",
            "  438             felt   3.581\n",
            "  439         supposed  13.397\n",
            "  440           cinema   0.790\n",
            "  441           friend   1.550\n",
            "  442            drama  -1.841\n",
            "  443            sound  -0.126\n",
            "  444          several  -0.799\n",
            "  445        beginning  -1.304\n",
            "  446            white  -0.738\n",
            "  447          against  -0.676\n",
            "  448             game  -2.662\n",
            "  449             dark  -2.568\n",
            "  450        direction   6.542\n",
            "  451             shes  -0.583\n",
            "  452             fans  -1.869\n",
            "  453            waste  30.212\n",
            "  454            humor  -4.045\n",
            "  455          despite   0.824\n",
            "  456               id  -1.958\n",
            "  457            lives  -1.311\n",
            "  458          totally   1.538\n",
            "  459            under  -0.166\n",
            "  460           seemed   6.429\n",
            "  461          already   4.116\n",
            "  462             care   3.791\n",
            "  463            guess   7.222\n",
            "  464           wanted   2.858\n",
            "  465       throughout  -4.443\n",
            "  466          example   6.782\n",
            "  467          becomes   2.938\n",
            "  468             days  -1.633\n",
            "  469            final   0.742\n",
            "  470             wont  -1.383\n",
            "  471            youll  -3.013\n",
            "  472             lead   2.448\n",
            "  473             evil   0.823\n",
            "  474          quality   2.905\n",
            "  475         children  -0.553\n",
            "  476             turn  -1.609\n",
            "  477          amazing -17.965\n",
            "  478    unfortunately  13.508\n",
            "  479             able  -4.125\n",
            "  480             side  -2.370\n",
            "  481          history  -1.124\n",
            "  482                   2.045\n",
            "  483            girls   0.581\n",
            "  484                b   2.046\n",
            "  485         horrible  18.006\n",
            "  486             fine  -6.380\n",
            "  487           killer   3.029\n",
            "  488          writing   4.316\n",
            "  489            flick   0.512\n",
            "  490            heart  -6.040\n",
            "  491              art   0.234\n",
            "  492             town   1.942\n",
            "  493            wants  -2.247\n",
            "  494            close   0.839\n",
            "  495              son   1.253\n",
            "  496             kill   0.563\n",
            "  497          michael  -0.226\n",
            "  498           matter  -3.994\n",
            "  499            works  -4.350\n",
            "  500           theyre   0.811\n",
            "  501              run   0.577\n",
            "  502              act   3.959\n",
            "  503              etc   1.636\n",
            "  504            stuff  -3.266\n",
            "  505              ill   3.612\n",
            "  506           behind   0.169\n",
            "  507            parts  -2.646\n",
            "  508             gave   3.416\n",
            "  509        brilliant -16.213\n",
            "  510             past  -1.053\n",
            "  511          enjoyed -13.926\n",
            "  512         directed   0.638\n",
            "  513             late  -7.100\n",
            "  514            tries   9.687\n",
            "  515             hand   2.572\n",
            "  516         favorite -16.620\n",
            "  517            turns  -3.922\n",
            "  518            genre  -3.635\n",
            "  519             eyes  -4.457\n",
            "  520              car   1.438\n",
            "  521             hour   5.696\n",
            "  522             city  -2.274\n",
            "  523             soon  -2.176\n",
            "  524           expect  -5.236\n",
            "  525        sometimes  -4.657\n",
            "  526          actress   2.773\n",
            "  527             stop  -0.982\n",
            "  528             ones  -1.336\n",
            "  529         thinking   3.945\n",
            "  530       themselves  -2.137\n",
            "  531        obviously   1.908\n",
            "  532           starts   3.438\n",
            "  533            child   3.271\n",
            "  534           decent   6.645\n",
            "  535           killed   2.345\n",
            "  536           myself   2.155\n",
            "  537              kid   0.646\n",
            "  538            blood   3.911\n",
            "  539           viewer  -1.873\n",
            "  540             took   1.542\n",
            "  541              god   4.210\n",
            "  542             hell   2.396\n",
            "  543            heard  -2.271\n",
            "  544           anyway  -3.471\n",
            "  545           highly -14.627\n",
            "  546             self   3.557\n",
            "  547             says  -1.897\n",
            "  548             type  -2.283\n",
            "  549            fight   0.552\n",
            "  550             slow   5.217\n",
            "  551           except   4.603\n",
            "  552          stories   1.630\n",
            "  553            voice   1.825\n",
            "  554            group   2.104\n",
            "  555            known  -2.897\n",
            "  556          happens   2.436\n",
            "  557         daughter   3.988\n",
            "  558          feeling   0.261\n",
            "  559           coming  -5.477\n",
            "  560           moment  -0.885\n",
            "  561       experience  -4.121\n",
            "  562             told  -2.405\n",
            "  563           writer   2.318\n",
            "  564        hilarious -16.710\n",
            "  565        extremely   1.497\n",
            "  566            leave  -0.214\n",
            "  567         violence   1.500\n",
            "  568           cannot   3.873\n",
            "  569          obvious   6.876\n",
            "  570           strong  -8.152\n",
            "  571     particularly   2.065\n",
            "  572           police  -0.988\n",
            "  573            roles  -1.923\n",
            "  574               ok   5.719\n",
            "  575           chance  -8.398\n",
            "  576           murder  -3.379\n",
            "  577           looked   6.734\n",
            "  578              hit  -1.756\n",
            "  579             lack   7.376\n",
            "  580          serious   1.530\n",
            "  581             crap  12.301\n",
            "  582            score  -4.645\n",
            "  583        including  -1.948\n",
            "  584           wonder   7.347\n",
            "  585            james   0.052\n",
            "  586           living  -0.805\n",
            "  587           simple -11.279\n",
            "  588         happened   1.054\n",
            "  589             gore   1.634\n",
            "  590         involved   3.356\n",
            "  591             cool  -4.561\n",
            "  592           please   3.683\n",
            "  593              cut   0.580\n",
            "  594          wouldnt   5.710\n",
            "  595            shown  -2.691\n",
            "  596             song  -1.377\n",
            "  597           happen  -3.507\n",
            "  598          attempt  10.007\n",
            "  599         complete   5.693\n",
            "  600              age  -1.288\n",
            "  601             save  16.020\n",
            "  602              ago  -0.154\n",
            "  603             hero   3.431\n",
            "  604        seriously   1.689\n",
            "  605          reality  -4.106\n",
            "  606         interest   7.199\n",
            "  607             lets   2.676\n",
            "  608         released  -1.441\n",
            "  609           robert  -0.104\n",
            "  610            taken  -1.916\n",
            "  611           career   4.166\n",
            "  612            today -15.022\n",
            "  613          opening  -1.062\n",
            "  614           talent   4.170\n",
            "  615          english  -2.692\n",
            "  616           saying   3.448\n",
            "  617             none  10.973\n",
            "  618          brother   0.017\n",
            "  619           across   2.153\n",
            "  620          exactly  -0.505\n",
            "  621            jokes   1.509\n",
            "  622            alone   0.053\n",
            "  623      documentary  -3.893\n",
            "  624   cinematography  -0.682\n",
            "  625            david  -1.872\n",
            "  626              sad   2.667\n",
            "  627         possible   2.862\n",
            "  628          running   2.043\n",
            "  629           number  -0.007\n",
            "  630            hours   3.960\n",
            "  631         yourself   1.372\n",
            "  632            light  -4.270\n",
            "  633            order   2.852\n",
            "  634          usually   0.928\n",
            "  635         annoying  15.943\n",
            "  636            whose   0.936\n",
            "  637             body   0.940\n",
            "  638             view   0.610\n",
            "  639            shots   3.325\n",
            "  640           taking  -2.651\n",
            "  641             wish  -4.015\n",
            "  642     relationship  -4.727\n",
            "  643            level  -1.087\n",
            "  644         somewhat  -0.726\n",
            "  645             call   3.167\n",
            "  646           change  -3.261\n",
            "  647           beyond   6.784\n",
            "  648             ends   1.506\n",
            "  649           mostly   0.848\n",
            "  650            power  -0.565\n",
            "  651          started   3.025\n",
            "  652             huge   0.440\n",
            "  653          opinion   0.387\n",
            "  654             jack  -3.659\n",
            "  655            happy  -2.806\n",
            "  656            major   1.340\n",
            "  657            knows  -2.181\n",
            "  658       ridiculous  16.392\n",
            "  659            scary   2.307\n",
            "  660        directors   3.194\n",
            "  661            silly   9.035\n",
            "  662           female   1.581\n",
            "  663             four   2.623\n",
            "  664            words  -0.481\n",
            "  665           turned   2.754\n",
            "  666            novel   1.930\n",
            "  667        important  -2.619\n",
            "  668             knew  -2.684\n",
            "  669             word   2.226\n",
            "  670           rating   2.222\n",
            "  671          talking   2.592\n",
            "  672            usual   0.846\n",
            "  673           middle   3.266\n",
            "  674          country   0.562\n",
            "  675        attention  -4.046\n",
            "  676          husband   0.579\n",
            "  677             upon   3.258\n",
            "  678            cheap  11.744\n",
            "  679              non   0.737\n",
            "  680             room   1.948\n",
            "  681        basically   8.147\n",
            "  682     disappointed   9.905\n",
            "  683              due  -0.785\n",
            "  684       apparently   8.278\n",
            "  685           modern  -1.105\n",
            "  686          strange  -1.386\n",
            "  687         episodes  -4.891\n",
            "  688             miss  -3.541\n",
            "  689           single   4.000\n",
            "  690       television   0.133\n",
            "  691            local   1.456\n",
            "  692          musical  -0.653\n",
            "  693         sequence  -2.756\n",
            "  694         problems  -2.212\n",
            "  695            arent   2.966\n",
            "  696            finds  -0.531\n",
            "  697           events  -1.910\n",
            "  698          clearly   3.962\n",
            "  699          whether  -0.251\n",
            "  700          british   1.079\n",
            "  701            whats   4.884\n",
            "  702             talk   3.303\n",
            "  703            class   0.869\n",
            "  704             five   4.382\n",
            "  705            songs   1.615\n",
            "  706         thriller   0.439\n",
            "  707            earth   1.421\n",
            "  708    entertainment  -2.503\n",
            "  709             fast   0.112\n",
            "  710           review   1.714\n",
            "  711              ten  -0.025\n",
            "  712           french  -2.907\n",
            "  713           moving  -8.400\n",
            "  714             lots  -2.273\n",
            "  715             team  -2.529\n",
            "  716            tells  -4.328\n",
            "  717             hate   0.017\n",
            "  718           havent  -4.110\n",
            "  719              add   3.485\n",
            "  720              die   1.565\n",
            "  721         straight   2.619\n",
            "  722            comic  -2.968\n",
            "  723           easily  -5.528\n",
            "  724             sets   0.054\n",
            "  725            above  -0.181\n",
            "  726           sequel   4.415\n",
            "  727            space   0.419\n",
            "  728            falls   7.807\n",
            "  729       soundtrack  -5.513\n",
            "  730           dialog   4.709\n",
            "  731             dull  19.371\n",
            "  732            bring  -0.053\n",
            "  733           giving   3.690\n",
            "  734           future  -2.285\n",
            "  735          release  -8.458\n",
            "  736      predictable  15.220\n",
            "  737           filmed   3.121\n",
            "  738           george   3.628\n",
            "  739       supporting  -2.709\n",
            "  740        enjoyable -13.975\n",
            "  741            tried   7.151\n",
            "  742          viewers   0.451\n",
            "  743         romantic  -2.882\n",
            "  744          parents  -1.942\n",
            "  745          mention   2.489\n",
            "  746          message  -3.260\n",
            "  747          similar  -1.990\n",
            "  748          appears   7.512\n",
            "  749             near   1.295\n",
            "  750          showing   2.172\n",
            "  751            sorry  12.925\n",
            "  752        surprised -13.078\n",
            "  753              eye  -3.427\n",
            "  754            needs  -2.196\n",
            "  755          certain  -5.825\n",
            "  756         suspense  -2.618\n",
            "  757          feature   0.223\n",
            "  758             rock  -1.521\n",
            "  759           within  -0.387\n",
            "  760            theme  -1.085\n",
            "  761             easy  -8.310\n",
            "  762            clear   1.115\n",
            "  763           minute   0.958\n",
            "  764            among  -3.593\n",
            "  765         comments  -2.152\n",
            "  766            bunch   6.441\n",
            "  767          mystery   0.169\n",
            "  768            stand  -1.870\n",
            "  769          working   0.089\n",
            "  770                t   3.629\n",
            "  771             ways  -5.291\n",
            "  772             fall   0.473\n",
            "  773        storyline   5.008\n",
            "  774             king   0.212\n",
            "  775           sister  -0.560\n",
            "  776         elements  -1.311\n",
            "  777            oscar  -4.894\n",
            "  778        fantastic -13.388\n",
            "  779           effort   9.935\n",
            "  780             stay   6.305\n",
            "  781            doubt   1.547\n",
            "  782          theater   2.072\n",
            "  783            named   2.289\n",
            "  784            peter  -2.348\n",
            "  785            avoid  19.050\n",
            "  786          typical  -2.908\n",
            "  787             gone   2.409\n",
            "  788            check  -3.237\n",
            "  789               th  -1.099\n",
            "  790              buy  -3.095\n",
            "  791             boys  -3.931\n",
            "  792          editing   5.358\n",
            "  793         greatest  -8.687\n",
            "  794             tale  -1.791\n",
            "  795           follow  -2.054\n",
            "  796          richard  -0.746\n",
            "  797          viewing  -5.384\n",
            "  798             okay   9.413\n",
            "  799               dr   1.620\n",
            "  800            crime  -1.533\n",
            "  801            feels   3.066\n",
            "  802          subject   1.976\n",
            "  803             deal  -3.973\n",
            "  804             kept  -1.255\n",
            "  805            using   1.968\n",
            "  806          general   0.585\n",
            "  807          imagine   2.199\n",
            "  808           nearly   1.538\n",
            "  809          monster   2.138\n",
            "  810           famous   1.368\n",
            "  811        realistic -11.492\n",
            "  812            means  -2.712\n",
            "  813           points   2.141\n",
            "  814              dog   1.847\n",
            "  815             move   2.570\n",
            "  816             rent   6.275\n",
            "  817               re   2.654\n",
            "  818           actual   0.717\n",
            "  819         surprise  -1.960\n",
            "  820          herself  -3.484\n",
            "  821            youve   1.356\n",
            "  822             lady  -0.939\n",
            "  823           period   0.640\n",
            "  824            leads  -1.525\n",
            "  825          somehow   3.725\n",
            "  826          brought  -5.413\n",
            "  827        animation  -1.872\n",
            "  828       believable  -9.204\n",
            "  829         material   7.729\n",
            "  830              red   0.661\n",
            "  831             form   0.012\n",
            "  832              sit   3.601\n",
            "  833           forget  -3.269\n",
            "  834        sequences  -1.987\n",
            "  835            dance  -0.958\n",
            "  836           begins   0.201\n",
            "  837             paul  -1.313\n",
            "  838           indeed  -1.371\n",
            "  839          killing   2.026\n",
            "  840         whatever   1.487\n",
            "  841             open  -1.719\n",
            "  842              tom   1.172\n",
            "  843           figure  -2.153\n",
            "  844       eventually   0.613\n",
            "  845             weak  13.955\n",
            "  846             deep  -3.406\n",
            "  847             wait   1.807\n",
            "  848          average   2.100\n",
            "  849             whos  -2.337\n",
            "  850           season  -5.472\n",
            "  851        difficult  -0.550\n",
            "  852         expected  -1.052\n",
            "  853             imdb   0.287\n",
            "  854            third   2.172\n",
            "  855          reviews   1.397\n",
            "  856       particular  -1.072\n",
            "  857            shame   5.710\n",
            "  858             hear  -0.820\n",
            "  859             free   3.679\n",
            "  860        situation  -2.334\n",
            "  861              hot  -3.595\n",
            "  862            learn  -2.009\n",
            "  863             lame  17.199\n",
            "  864           needed   0.946\n",
            "  865             note  -1.682\n",
            "  866          footage   2.393\n",
            "  867       atmosphere  -9.227\n",
            "  868           poorly  21.230\n",
            "  869            truth  -0.854\n",
            "  870          premise   6.370\n",
            "  871             york   0.848\n",
            "  872         possibly   3.658\n",
            "  873            acted  -2.037\n",
            "  874        emotional  -3.199\n",
            "  875              box   2.076\n",
            "  876          decided   4.098\n",
            "  877          credits   2.274\n",
            "  878           forced   4.179\n",
            "  879               fi   0.142\n",
            "  880        memorable  -6.610\n",
            "  881              sci  -0.282\n",
            "  882           sexual   2.025\n",
            "  883           became   1.339\n",
            "  884          society  -3.698\n",
            "  885            begin   1.044\n",
            "  886              air   1.802\n",
            "  887         features  -0.463\n",
            "  888        otherwise   8.514\n",
            "  889           unless  15.584\n",
            "  890          reading  -0.117\n",
            "  891          writers   7.822\n",
            "  892            write   5.879\n",
            "  893             male   3.738\n",
            "  894                d   3.879\n",
            "  895               de  -1.016\n",
            "  896           superb -18.815\n",
            "  897           island   0.221\n",
            "  898            badly  17.612\n",
            "  899           nature  -3.886\n",
            "  900          western   0.697\n",
            "  901           leaves   1.731\n",
            "  902           street  -0.373\n",
            "  903            hands  -1.099\n",
            "  904             plus   0.813\n",
            "  905         question  -1.056\n",
            "  906            meets  -3.740\n",
            "  907         previous  -1.940\n",
            "  908          forward   6.269\n",
            "  909           doctor  -0.053\n",
            "  910           laughs   2.588\n",
            "  911           beauty  -6.412\n",
            "  912          romance  -1.355\n",
            "  913             crew  -3.454\n",
            "  914           cheesy   4.022\n",
            "  915            crazy  -3.297\n",
            "  916             whom   0.927\n",
            "  917       interested  -1.690\n",
            "  918          quickly   3.832\n",
            "  919           effect  -1.417\n",
            "  920      masterpiece -12.199\n",
            "  921             meet  -0.760\n",
            "  922          comment  -1.949\n",
            "  923              nor   4.358\n",
            "  924         personal  -2.201\n",
            "  925            keeps  -5.650\n",
            "  926        perfectly -12.075\n",
            "  927              gay   2.216\n",
            "  928           inside  -5.850\n",
            "  929          towards  -1.299\n",
            "  930        following  -4.085\n",
            "  931       screenplay   5.878\n",
            "  932             dumb   8.934\n",
            "  933                e   1.008\n",
            "  934             joke   8.262\n",
            "  935              lee   7.079\n",
            "  936         business  -0.558\n",
            "  937             fire  -0.394\n",
            "  938            weird  -1.128\n",
            "  939       incredibly   2.417\n",
            "  940          earlier  -1.908\n",
            "  941            stage   1.125\n",
            "  942              cop   0.200\n",
            "  943          setting  -0.900\n",
            "  944           creepy  -4.578\n",
            "  945         dramatic   1.217\n",
            "  946           appear   3.285\n",
            "  947        directing   1.284\n",
            "  948           result   8.719\n",
            "  949       background  -2.274\n",
            "  950          realize  -1.056\n",
            "  951          america  -0.410\n",
            "  952            total   6.075\n",
            "  953             baby   0.184\n",
            "  954             mess  15.143\n",
            "  955           sounds   2.221\n",
            "  956         japanese  -1.114\n",
            "  957       girlfriend   2.214\n",
            "  958             copy  -1.792\n",
            "  959           worked  -0.767\n",
            "  960           unique  -8.884\n",
            "  961            front   0.333\n",
            "  962          outside  -0.216\n",
            "  963         powerful  -9.533\n",
            "  964            water   1.631\n",
            "  965          reasons   3.098\n",
            "  966              ask  -0.567\n",
            "  967             bill  -1.547\n",
            "  968           battle  -2.929\n",
            "  969            dream  -2.914\n",
            "  970           brings  -6.477\n",
            "  971      development   1.952\n",
            "  972            apart   1.974\n",
            "  973        political  -1.984\n",
            "  974             mark   1.584\n",
            "  975              pay   3.145\n",
            "  976             rate  -1.743\n",
            "  977          telling   0.993\n",
            "  978             rich  -1.387\n",
            "  979         deserves  -8.816\n",
            "  980           plenty  -8.317\n",
            "  981          various  -0.369\n",
            "  982            spent   3.017\n",
            "  983            admit  -1.932\n",
            "  984           fairly  -0.345\n",
            "  985          present  -3.298\n",
            "  986           wasted  17.540\n",
            "  987              joe  -0.300\n",
            "  988            fails  19.257\n",
            "  989        portrayed   0.095\n",
            "  990            cover   8.542\n",
            "  991             cute  -0.087\n",
            "  992         fighting   0.162\n",
            "  993             list   0.848\n",
            "  994            older  -2.391\n",
            "  995              gun   2.091\n",
            "  996       filmmakers   5.738\n",
            "  997          missing   4.467\n",
            "  998          success  -1.312\n",
            "  999           zombie   2.226\n",
            " 1000            break  -0.887\n",
            " 1001          leading   4.257\n",
            " 1002           secret   2.373\n",
            " 1003           disney   1.886\n",
            " 1004            meant   3.835\n",
            " 1005              odd   0.409\n",
            " 1006            agree   0.650\n",
            " 1007            ideas   5.948\n",
            " 1008        expecting  -4.955\n",
            " 1009              era  -2.361\n",
            " 1010           caught  -7.669\n",
            " 1011            ended   1.956\n",
            " 1012            twist  -1.687\n",
            " 1013         spoilers  -2.092\n",
            " 1014          william   1.934\n",
            " 1015            large  -2.647\n",
            " 1016           create   0.726\n",
            " 1017          fantasy  -0.977\n",
            " 1018         talented   2.835\n",
            " 1019           return  -5.339\n",
            " 1020           hardly   6.679\n",
            " 1021            plain   4.048\n",
            " 1022           remake   5.664\n",
            " 1023          italian  -0.278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that emotionally negative words like *bad* have large embeddings of one sign while positive words like *great* have large embeddings of the opposite sign which is expected. However some neutral words like *minutes* also have large embeddings. This occurs when such a neutral word happens to appear only in negative or only in positive training reviews. Then the model thinks that it is negative or positive respectively. This is one cause of overfitting. Now proceed to the next notebook and introduce the attention mechanism to our model."
      ],
      "metadata": {
        "id": "-fvGHab64cl-"
      }
    }
  ]
}