{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jEURcvJry906",
        "YraDFvxMznvs",
        "H6awpcdi5K5k",
        "A-Z7PltMwym-",
        "Kbae6G_p-mhZ",
        "1cif8nwmpzT6",
        "h2uiDhRu8qtt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rudiments of Natural Language Processing: Data"
      ],
      "metadata": {
        "id": "BnQkalS5w42P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first part of the NLP workshop at the Center for Machine Learning. In this notebook we preprocess the IMDB movie reviews to later classify them as positive or negative. We do the following:\n",
        "\n",
        "*   Gather some prerequisites\n",
        "*   Retrieve the IMDB dataset of movie reviews\n",
        "*   Clean and tokenize the texts\n",
        "*   Split the data into training and validation sets\n",
        "*   Create a vocabulary and save it to disk\n",
        "*   Convert words to indices and save them to disk\n",
        "\n"
      ],
      "metadata": {
        "id": "_aIs_PV93qyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "jEURcvJry906"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing dependencies:"
      ],
      "metadata": {
        "id": "e1Cy_93RuBLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "id": "kCXbrTMmX--6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules:"
      ],
      "metadata": {
        "id": "PFPQfbq6uq1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcK1C7GkXdpx"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import google.colab as colab\n",
        "import itertools\n",
        "import pickle\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create some files and store them on the google drive so that they are available for the next notebooks. Mount your drive:"
      ],
      "metadata": {
        "id": "VzT7pyIRuzEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab.drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y8ZTsmbRsuVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your home directory on the google drive is now mounted as `drive/MyDrive`. Check it by listing its contents:"
      ],
      "metadata": {
        "id": "ee3VTIM6u3mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/"
      ],
      "metadata": {
        "id": "VE6V0hvwtPK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity we will store all the necessary files in the home directory but you can create a dedicated subfolder if you wish."
      ],
      "metadata": {
        "id": "nFQCxc3SwasA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data"
      ],
      "metadata": {
        "id": "YraDFvxMznvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDB dataset can be retrieved in various ways. In order to avoid some technical problems with the existing distributions we prepared a single file `data.zip`. Download it to your home directory on the google drive:"
      ],
      "metadata": {
        "id": "NI1vTIARi95K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget www.fuw.edu.pl/~polbrat/data.zip --directory-prefix=drive/MyDrive/"
      ],
      "metadata": {
        "id": "D_fn-Hm8N9d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip it to the current folder of the execution environment:"
      ],
      "metadata": {
        "id": "Mi_oAT0UN-Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/data.zip"
      ],
      "metadata": {
        "id": "cdYYNsleeNSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this directory will be lost when the environment gets disconnected. List its contents:"
      ],
      "metadata": {
        "id": "S3rYMQlIzwP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "y2LxtTRvZPCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first ten reviews from the CSV file:"
      ],
      "metadata": {
        "id": "xuDx7Hp51vtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head data.csv"
      ],
      "metadata": {
        "id": "NcUfT9fc1iBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels in the first column equal 0 for negative and 1 for positive reviews."
      ],
      "metadata": {
        "id": "3w_wnmj121yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the texts"
      ],
      "metadata": {
        "id": "H6awpcdi5K5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further processing will be easier if the texts are first cleaned a bit. In a simple approach we will remove HTML tags, digits, punctuation, and make everything lowercase. Read the CSV file and store the cleaned texts together with their labels in a list of tuples:"
      ],
      "metadata": {
        "id": "Mp0fZKJX62ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = list()\n",
        "with open('data.csv', 'rt', encoding = 'utf-8') as stream:\n",
        "    reader = csv.reader(stream)\n",
        "    for label0, word1 in reader:\n",
        "        word1 = word1.replace('<hr>', ' ').replace('<br />', ' ')\n",
        "        word1 = word1.replace('!', ' ').replace('\"', ' ').replace('#', ' ').replace('$', ' ').replace('%', ' ')\n",
        "        word1 = word1.replace('&', ' ').replace(\"'\", '').replace('(', ' ').replace(')', ' ').replace('*', ' ')\n",
        "        word1 = word1.replace('+', ' ').replace(',', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' ')\n",
        "        word1 = word1.replace('0', ' ').replace('1', ' ').replace('2', ' ').replace('3', ' ').replace('4', ' ')\n",
        "        word1 = word1.replace('5', ' ').replace('6', ' ').replace('7', ' ').replace('8', ' ').replace('9', ' ')\n",
        "        word1 = word1.replace(':', ' ').replace(';', ' ').replace('<', ' ').replace('=', ' ').replace('>', ' ')\n",
        "        word1 = word1.replace('?', ' ').replace('@', ' ').replace('[', ' ').replace('\\\\', ' ').replace(']', ' ')\n",
        "        word1 = word1.replace('^', ' ').replace('_', ' ').replace('`', ' ').replace('{', ' ').replace('|', ' ')\n",
        "        word1 = word1.replace('}', ' ').replace('~', ' ')\n",
        "        word1 = word1.lower()\n",
        "        word1 = word1.split()\n",
        "        word1 = ' '.join(word1)\n",
        "        data.append((label0, word1))"
      ],
      "metadata": {
        "id": "0VyrwYx-5P5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the cleaned data in the same CSV file:"
      ],
      "metadata": {
        "id": "7hRAboNN54Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.csv', 'wt', encoding = 'utf-8') as stream:\n",
        "    writer = csv.writer(stream)\n",
        "    for label0, word1 in data:\n",
        "        writer.writerow([label0, word1])"
      ],
      "metadata": {
        "id": "gIfYIjA4uZD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first ten cleaned reviews:"
      ],
      "metadata": {
        "id": "PrGJ1AvSwC6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head data.csv"
      ],
      "metadata": {
        "id": "2yqjGFtcwIzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By cleaning in this way we certainly lost some information. But doing significantly better would be cumbersome without specialized tools while this approach is sufficient for our purposes."
      ],
      "metadata": {
        "id": "ziAduaEd3UP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the texts"
      ],
      "metadata": {
        "id": "A-Z7PltMwym-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For further processing we want to represent each review as a list of tokens that will be just words in our simple approach. Just split the texts on spaces and store the labels and sequences of words in a list of tuples:"
      ],
      "metadata": {
        "id": "I5VesykF-jCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = list()\n",
        "with open('data.csv', 'rt', encoding = 'utf-8') as stream:\n",
        "    reader = csv.reader(stream)\n",
        "    for label0, word1 in reader:\n",
        "        word1 = word1.split()\n",
        "        data.append((label0, word1))"
      ],
      "metadata": {
        "id": "_YgjHlZHw5NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the tokenized data in the same CSV file so that the labels are in the first column and the words in the following columns:"
      ],
      "metadata": {
        "id": "NOAzTMiK_U5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.csv', 'wt', encoding = 'utf-8') as stream:\n",
        "    writer = csv.writer(stream)\n",
        "    for label0, word1 in data:\n",
        "        writer.writerow([label0] + word1)"
      ],
      "metadata": {
        "id": "trbrs5ftxwiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first ten tokenized reviews:"
      ],
      "metadata": {
        "id": "HOCAr8IxBRQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head data.csv"
      ],
      "metadata": {
        "id": "aXiG7jzXyI1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data into training and validation sets"
      ],
      "metadata": {
        "id": "Kbae6G_p-mhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the tokenized CSV file back just to see how it is done. By the way convert labels from text format to integer numbers:"
      ],
      "metadata": {
        "id": "3KL_x_iBbH3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = list()\n",
        "with open('data.csv', 'rt', encoding = 'utf-8') as stream:\n",
        "    reader = csv.reader(stream)\n",
        "    for label0, *word1 in reader:\n",
        "        label0 = int(label0)\n",
        "        data.append((label0, word1))"
      ],
      "metadata": {
        "id": "MJzdNcs2bS33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first ten reviews just to check:"
      ],
      "metadata": {
        "id": "hJxxfDatblks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label0, word1 in data[:10]:\n",
        "    print(label0, word1)"
      ],
      "metadata": {
        "id": "N_QfxTrHbqID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the total number of reviews as well as the numbers of negative and positive ones:"
      ],
      "metadata": {
        "id": "3yDRmTdo7W61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data),\n",
        "      sum(label0 == 0 for label0, word1 in data),\n",
        "      sum(label0 == 1 for label0, word1 in data))"
      ],
      "metadata": {
        "id": "dUzihV_ncEp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are as many negative reviews as there are positive ones so that the dataset is exactly balanced. Split the data into training and validation sets of 40000 and 10000 reviews respectively:"
      ],
      "metadata": {
        "id": "pUYpgLVeUdlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data[:40000]\n",
        "valid_data = data[40000:]"
      ],
      "metadata": {
        "id": "n4tW_EuDoS1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the total number of reviews in each set as well as the numbers of negative and positive reviews:"
      ],
      "metadata": {
        "id": "OvsI3FLKob8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data),\n",
        "      sum(label0 == 0 for label0, word1 in train_data),\n",
        "      sum(label0 == 1 for label0, word1 in train_data))\n",
        "\n",
        "print(len(valid_data),\n",
        "      sum(label0 == 0 for label0, word1 in valid_data),\n",
        "      sum(label0 == 1 for label0, word1 in valid_data))"
      ],
      "metadata": {
        "id": "2ymsw16lpCWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The downloaded file was prepared so that both sets are exactly balanced. If this was not the case you would need a better splitting technique than just slicing the data list. "
      ],
      "metadata": {
        "id": "w9jFMKZHF_0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a vocabulary"
      ],
      "metadata": {
        "id": "1cif8nwmpzT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to feed the reviews to any machine-learning model we must somehow convert the texts to numbers. We do it by replacing each word with its index on a list of all considered words that constitute the vocabulary of the problem. The vocabulary contains only words from the training set because anyway we cannot predict what other words will appear in other reviews. We will now create the vocabulary. Drop the labels from the training set and consider only the lists of words in the subsequent reviews:"
      ],
      "metadata": {
        "id": "O9XQ6A_WJfmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2 = [word1 for label0, word1 in train_data]"
      ],
      "metadata": {
        "id": "9ZAIPhYR1DFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the lists of words in the first ten reviews just to see if they are correct:"
      ],
      "metadata": {
        "id": "3BUEMqNmJ1W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word1 in word2[:10]:\n",
        "    print(word1)"
      ],
      "metadata": {
        "id": "1GAa9ZvH1StJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a vocabulary object from all the words in the training set using a builtin function:"
      ],
      "metadata": {
        "id": "sDc7TH7UK0kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = torchtext.vocab.build_vocab_from_iterator(word2)"
      ],
      "metadata": {
        "id": "Cq373y24izo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary object assigns a unique index to each unique word in the provided corpus and is then able to convert words to their indices. For the first ten reviews print their words pass them through the vocabulary and print the resulting word indices:"
      ],
      "metadata": {
        "id": "pKjPuUQFMlKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word1 in word2[:10]:\n",
        "    print(word1)\n",
        "    index1 = vocab(word1)\n",
        "    print(index1)\n",
        "    print()"
      ],
      "metadata": {
        "id": "ZlU2Ogn4i3I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary can also transform a single word into its index. Print the index of a common word `message`:"
      ],
      "metadata": {
        "id": "34Y3up5iPMoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['message']\n",
        "print(index0)"
      ],
      "metadata": {
        "id": "TB0ILWt8jho3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the index of an odd word `jlo` which may be a typo or a proper name appearing by chance in this particular training set:"
      ],
      "metadata": {
        "id": "Xaoo902SQUCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['jlo']\n",
        "print(index0)"
      ],
      "metadata": {
        "id": "L3SexstSjvOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the vocabulary object retrieve a python dictionary that maps words to their indices. Print the first ten keys of this dictionary togehter with their corresponding values:"
      ],
      "metadata": {
        "id": "mSebyMVXSM-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = vocab.get_stoi()\n",
        "for word0 in itertools.islice(stoi.keys(), 10):\n",
        "    index0 = stoi[word0]\n",
        "    print(word0, index0)"
      ],
      "metadata": {
        "id": "ig__gdSkkQhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that despite our cleaning some non-ASCII UTF8 characters remained. You may get rid of them by better cleaning. From the vocabulary object retrieve a list of words arranged according to their indices. Print the first ten indices and words:"
      ],
      "metadata": {
        "id": "3sSc0K07S6Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = vocab.get_itos()\n",
        "for index0 in range(10):\n",
        "    word0 = itos[index0]\n",
        "    print(index0, word0)"
      ],
      "metadata": {
        "id": "26VGV7EQmhOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are actually the most frequent words in our text corpus. The vocabulary can be manually extended with arbitrary words that usually play some special role. Add a word `<unk>`:\n"
      ],
      "metadata": {
        "id": "F_LyAkhnT1m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "specials = ['<unk>']\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(word2, specials = specials)"
      ],
      "metadata": {
        "id": "bbt6pUsTVH-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first ten words again:"
      ],
      "metadata": {
        "id": "S6eY-bGvVOHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = vocab.get_itos()\n",
        "for index0 in range(10):\n",
        "    word0 = itos[index0]\n",
        "    print(index0, word0)"
      ],
      "metadata": {
        "id": "hHozt0yQnFz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The special words are placed at the beginning of the list. They are treated as all other words and can be mapped to their indices as well:"
      ],
      "metadata": {
        "id": "bwPiqhS0Vcxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['<unk>']\n",
        "print(index0)"
      ],
      "metadata": {
        "id": "3vqCotsUnoO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New reviews may contain words absent in the training set and so in the vocabulary. An attempt to map an unknown word causes an error:"
      ],
      "metadata": {
        "id": "dxZKK7iXWWjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['ferdydurke']\n",
        "print(index0)"
      ],
      "metadata": {
        "id": "GJIIRmA0nwVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the vocabulary may be told to map unknown words to a default index which is usually set as the index of the special word `<unk>`:"
      ],
      "metadata": {
        "id": "MntNL8PyZjIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['<unk>']\n",
        "vocab.set_default_index(index0)"
      ],
      "metadata": {
        "id": "PyrrdGzhaCGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check it by mapping any unknown word:"
      ],
      "metadata": {
        "id": "GPvKHIawaF9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['ferdydurke']\n",
        "print(index0)"
      ],
      "metadata": {
        "id": "arHK1y52oKm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of unique words in the vocabulary can be obtained as its length:"
      ],
      "metadata": {
        "id": "cbjUu4Pfi82s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = len(vocab)\n",
        "print(indices)"
      ],
      "metadata": {
        "id": "cOlUf87Hjfeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary contains quite many words but some of them appear in the training set very rarely. If a rare word appears by chance only in positive reviews the model may erroneously think that any review containing this word is positive. It is therefore better to treat such words as unknown and exclude them from the vocabulary. Limit the vocabulary to words present in at least five reviews from the training set:"
      ],
      "metadata": {
        "id": "m322ILL5frHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = torchtext.vocab.build_vocab_from_iterator(word2, 5, specials = specials)\n",
        "index0 = vocab['<unk>']\n",
        "vocab.set_default_index(index0)"
      ],
      "metadata": {
        "id": "WhpDZhqskV5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this dramatically reduces the vocabulary size:"
      ],
      "metadata": {
        "id": "QBG1IT0WkafC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = len(vocab)\n",
        "print(indices)"
      ],
      "metadata": {
        "id": "3hXPi9hOoh8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reduction makes the calculations lighter and helps prevent overfitting. Now the odd word `jlo` is absent from the vocabulary:"
      ],
      "metadata": {
        "id": "AJZwzxJBk2ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index0 = vocab['jlo']\n",
        "print(index0)"
      ],
      "metadata": {
        "id": "Xbfxhq_EpLsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the most freqeunt words like `the`, `and`, `a` etc. carry little information on whether a review is negative or positive. So it would be beneficial to exclude them from the vocabulary as well. We will not do so because there is no ready mechanism the library. Instead our models will learn that these words are not important. During training reviews of different lengths will be grouped into batches where they must have equal lengths. So shorter ones will be padded with an index that does not correspond to any real word but to a special one usually called `<pad>`. Create a vocabulary with two special words `<pad>` and `<unk>`:"
      ],
      "metadata": {
        "id": "yUXkeabEmQe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "specials = ['<pad>', '<unk>']\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(word2, 5, specials = specials)\n",
        "index0 = vocab['<unk>']\n",
        "vocab.set_default_index(index0)"
      ],
      "metadata": {
        "id": "JVR08RTFn8Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that they correspond to indices 0 and 1:"
      ],
      "metadata": {
        "id": "hMKwhlz7oAqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index1 = vocab(['<pad>', '<unk>'])\n",
        "print(index1)"
      ],
      "metadata": {
        "id": "EL37gq0spRO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final form of our vocabulary. Save is to google drive in the pickle format:"
      ],
      "metadata": {
        "id": "cS_8bR1w85gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/vocab.pkl', 'wb') as stream:\n",
        "    pickle.dump(vocab, stream, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "vKj7ANJRpjEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert words to indices"
      ],
      "metadata": {
        "id": "h2uiDhRu8qtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use the created vocabulary to convert the training and validation reviews to word indices. Convert the training data:"
      ],
      "metadata": {
        "id": "wqIhVqvvnRkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [(label0, vocab(word1)) for label0, word1 in train_data]"
      ],
      "metadata": {
        "id": "SlkNOPFc3cTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save it to google drive:"
      ],
      "metadata": {
        "id": "jwiFYu-Qnksy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/train_data.csv', 'wt', encoding = 'utf-8') as stream:\n",
        "    writer = csv.writer(stream)\n",
        "    for label0, index1 in train_data:\n",
        "        writer.writerow([label0] + index1)"
      ],
      "metadata": {
        "id": "ViIsUFW-4PV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the first ten converted reviews:"
      ],
      "metadata": {
        "id": "tBSUSUpSnrzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head drive/MyDrive/train_data.csv"
      ],
      "metadata": {
        "id": "G_aXDk054zPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first column contains the label and the next columns contain subsequent word indices. Do the same with the validation data:"
      ],
      "metadata": {
        "id": "EiJlt5DOoJhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data = [(label0, vocab(word1)) for label0, word1 in valid_data]\n",
        "\n",
        "with open('drive/MyDrive/valid_data.csv', 'wt', encoding = 'utf-8') as stream:\n",
        "    writer = csv.writer(stream)\n",
        "    for label0, index1 in valid_data:\n",
        "        writer.writerow([label0] + index1)\n",
        "\n",
        "!head drive/MyDrive/valid_data.csv"
      ],
      "metadata": {
        "id": "J8k05OjV5Xrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the vocabulary and the data are saved to google drive switch to the next notebook and train a very simple model that will classify the reviews as negative or positive."
      ],
      "metadata": {
        "id": "Vpa0PFURorQC"
      }
    }
  ]
}