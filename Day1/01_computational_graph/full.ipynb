{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4b9724-7cd4-4055-9eeb-d91511eb0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class PDF(object):   #thanks to https://stackoverflow.com/questions/19470099/view-pdf-image-in-an-ipython-notebook\n",
    "  def __init__(self, pdf, size=(200,200)):\n",
    "    self.pdf = pdf\n",
    "    self.size = size\n",
    "\n",
    "  def _repr_html_(self):\n",
    "    return '<iframe src={0} width={1[0]} height={1[1]}></iframe>'.format(self.pdf, self.size)\n",
    "\n",
    "  def _repr_latex_(self):\n",
    "    return r'\\includegraphics[width=1.0\\textwidth]{{{0}}}'.format(self.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33996a9c-2f7a-4256-89bc-499a855aa784",
   "metadata": {},
   "source": [
    "## Automatic gradient functionality\n",
    "\n",
    "When you design a neural network, we humans think in a forward manner: we want data to get into the network and pass through layers and we usually can visualise what transformations of the data can or should be applied so we achieve our goal. Thus, the design and then the implementation of the so called forward pass through a network is something we humans can naturally grasp in our minds. \n",
    "\n",
    "But the way that neural networks are trained requires that all transformations of the forward pass, no matter how complex, are differentiated with respect to the weights of the neural network and all those gradients play then a role in weight update. The more complex were the transformations of the forward pass, the more complex will be the calculation of corresponding gradient functions. It is very inflexible, error prone and in some cases simply imposible or very hard to execute. It is a task for a machine.\n",
    "\n",
    "The appearance of the dynamic expression tree building with automatic gradient calculation ability let us, humans, design only the forward pass (i.e. how the data is transformed in the neural network), while the engine of the framework takes care of correctly executing the backward pass. It gave the design of neural networks unseen flexibility and gave the whole Machine Learning field the boost we are experiencing nowadays.\n",
    "\n",
    "One of the examples of frameworks that support dynamic expression tree building with automatic gradient calculation ability is PyTorch.\n",
    "\n",
    "### First example\n",
    "\n",
    "So let's see how it works in practice in Pytorch. We don't need a neural network to see it.\n",
    "\n",
    "Let's have $f(x,y)=x^3+y^2$ as an example. \n",
    "Then you can calculate by hand: \n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} = 3x^2, \\frac{\\delta f(x,y)}{\\delta y} = 2y$\n",
    "\n",
    "Concretely, as an example with which we will work some more:\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} \\Big|_{x=2} = 3 \\cdot 2^2 = 12$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} \\Big|_{x=3} = 3 \\cdot 3^2 = 27$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta y} \\Big|_{y=4} = 2 \\cdot 4 = 8$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta y} \\Big|_{y=5} = 2 \\cdot 5 = 10$\n",
    "\n",
    "You get all this automatically with PyTorch, which builds a tree of an expression that is constructed as we go. Let's have an example in Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798a24e2-370b-4a02-b770-0c580b1e9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([4.], requires_grad=True)\n",
    "f = x**3 + y**2\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70eb4a9-169c-4b16-8ea5-b63d66566e93",
   "metadata": {},
   "source": [
    "To calculate gradients you need to call `backward()`. Let's see if we get\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} \\Big|_{x=2} = 12$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta y} \\Big|_{y=4} = 8$\n",
    "\n",
    "as expected. The gradients of $f(x,y)$ with respect to variables $x$ and $y$ are held in the `grad` field in those variables (`x.grad`, `y.grad`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cd43a2-8cd2-462d-9ed4-30b960d15cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd52f90-4024-4185-b188-afcbd39cb995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfe398-3271-4a6f-ae21-ba84bdd49e55",
   "metadata": {},
   "source": [
    "OK, so far so good. But the weights in a neural network are not scalars, they are multidimensional entities, most commonly they are two dimensional matrices. In PyTorch, two dimensional matrices are called tensors of order two. What if $x$ and $y$ were tensors of order two? Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a3db72-56ef-4abe-87d9-e1143ea92488",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2., 3., 2.], [2., 3., 3.]], requires_grad=True)\n",
    "y = torch.tensor([[4., 5., 5.], [4., 5., 5.]], requires_grad=True)\n",
    "f = x**3 + y**2\n",
    "f.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ec77c-ec04-4bcf-aafa-fe6f959ef21b",
   "metadata": {},
   "source": [
    "Notice the last line, `f.sum().backward()`. Recall, that to calculate gradients you need to call `backward()`, but you may call it on scalar variables only, because loss that we calculate for a neural network is a scalar. Hence, we use this to trick PyTorch to calculate our gradients for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606424a1-df7e-49bb-8d7d-b6911f8f5fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 27., 12.],\n",
       "        [12., 27., 27.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010a799c-9fe4-43c2-bba8-a03eb06e1b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8., 10., 10.],\n",
       "        [ 8., 10., 10.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd09e7-a78f-406f-bb78-8f67c6744ea6",
   "metadata": {},
   "source": [
    "OK, results are as expected (i.e. as calculated by hand earlier)\n",
    "\n",
    "### Another example\n",
    "\n",
    "Now, let us consider another function. \n",
    "\n",
    "$f(x,y)=xy$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} = y$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta y} = x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b542296-48a4-44fb-b06d-d7a7c5ef2338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  6.,  9.],\n",
       "        [-1.,  1.,  2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3.], [2., 4., 6.]], requires_grad=True)\n",
    "y = torch.tensor([[3., 6., 9.], [-1., 1., 2.]], requires_grad=True)\n",
    "f = x*y\n",
    "f.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a37af2-c206-4d25-a596-3304c75d8b06",
   "metadata": {},
   "source": [
    "Well, $\\frac{\\delta f}{\\delta x} = y$ doesn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eaf4d-99e2-4f20-8779-6f3e6bd23e45",
   "metadata": {},
   "source": [
    "## Your tasks\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Your task is to calculate values of\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} \\Big|_{x=0.25, y=3.0}$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta y} \\Big|_{x=0.25, y=3.0}$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta x} \\Big|_{x=1.0, y=3.0}$\n",
    "\n",
    "$\\frac{\\delta f(x,y)}{\\delta y} \\Big|_{x=1.0, y=3.0}$\n",
    "\n",
    "for\n",
    "\n",
    "$f(x,y)=\\frac{sin(xy)}{sinx}$\n",
    "\n",
    "**TIP:** in PyTorch, if `x` is a tensor, then `torch.sin(x)` is its sinus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2a071-9c89-4e49-a658-bc7f22b0c27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4063cb4b-f357-4899-9c05-84f3178cd370",
   "metadata": {},
   "source": [
    "### Solution below (do not peek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7683df9b-d200-4606-8482-9104d65878b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.9177)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.25, 1.0], requires_grad=True)\n",
    "y = torch.tensor([3.0,3.0], requires_grad=True)\n",
    "f = torch.sin(x*y)/torch.sin(x)\n",
    "f.sum().backward()\n",
    "x.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b38d064-b411-4e9e-855a-9d21c4c5bc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7394)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cd4fae-786d-4516-b118-6b1d26b00528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.6372)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f69b5f-7c24-4450-9f72-e6c8dba021a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1765)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575efd7-2bf6-4b5f-93e7-c5013379d62f",
   "metadata": {},
   "source": [
    "Note what you have achieved. You calculated in a 4-liner code a gradient of a complicated function - with a plot (thanks to Wolfram Alpha) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f39fcada-5d2a-44a1-9425-a807017ae2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=sin_xy-div-sin_x.pdf width=800 height=800></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{sin_xy-div-sin_x.pdf}"
      ],
      "text/plain": [
       "<__main__.PDF at 0x7f2cf8409ac0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF('sin_xy-div-sin_x.pdf',size=(800,800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d4077-95c6-4876-b563-287cbaccad9b",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Try to find on the above plot of the $f(x,y)$ function the points corresponding to the selected $x$ and $y$ coordinates and make sure that\n",
    "\n",
    "- for $(x,y)=(0.25, 3.0)$ the rate of change in $x$ direction is negative while in $y$ direction it is positive, \n",
    "- but if you move a little to the right to $(x,y)=(1.0, 3.0)$ the rate of change in both directions $x, y$ becomes negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
