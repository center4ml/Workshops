{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1389d586-2bf4-4817-924d-b5082a7d4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bee46-53a6-455b-85e5-258f6810a41a",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "In neural network supervised training (and to lesser extent in evaluation) one of the key concepts is the loss function. The loss function measures what is the distance between the known target values and neural network predictions. A good loss function is differentiable and has non-zero gradients. \n",
    "\n",
    "As an example, **Accuracy**, a measure often used in neural network evaluation in classification tasks, has gradients almost everywhere equal to zero. This is not good to train the neural network, which needs to update the weights in a process called backpropagation, and for that it needs non-zero gradients. Thus, Accuracy isn't suitable as a loss function.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) [Some extra reading material on that topic](https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a).\n",
    "\n",
    "Examples of loss functions:\n",
    "\n",
    "- Mean Square Error - MSE - used for regression,\n",
    "- Binary Cross Entropy - a measure used for classification with two classes, it approximates Accuracy but has non-zero gradients,\n",
    "- Cross Entropy - a measure used for general classification. \n",
    "\n",
    "Let us provide an exact definition of MSE.\n",
    "\n",
    "If a target vector is\n",
    "\n",
    "$T=(T_i), i=1, \\ldots, N$\n",
    "\n",
    "and a prediction vector of a regressor is \n",
    "\n",
    "$P=(P_i), i=1, \\ldots, N$\n",
    "\n",
    "Then $MSE(P, T) = \\frac{\\sum_{i=1}^N (T_i-P_i)^2}{N}$\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #1\n",
    "\n",
    "Calculate, using Python, MSE loss of the prediction $P=(1.1, 4.12, 8.9, 14.85)$ versus the target values $T=(1,4,9,16)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d557ccbd-da3d-4184-a3dd-d2f45acedb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3392250000000002\n"
     ]
    }
   ],
   "source": [
    "target = [1, 4, 9, 16]\n",
    "prediction = [1.1, 4.12, 8.9, 14.85]\n",
    "sum = 0.0\n",
    "for i in range(4):\n",
    "    sum += (target[i]-prediction[i])**2\n",
    "mse = sum/len(target)\n",
    "print(mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d06270-eb68-428e-9661-202cafbe6fc0",
   "metadata": {},
   "source": [
    "# Let's talk about Tensors\n",
    "\n",
    "A PyTorch or TensorFlow Tensor is just a **multidimensional array**. But it is much more transformation-centered, and when I say *transformation* I mean mathematical transformation.\n",
    "\n",
    "Many times in the past it proved for me hard to speak clearly of Tensor transformations and dimensions. Below is a screenshot from a random tutorial that I found this morning, which shows a clear confusion when the author talks about Tensor dimensions.\n",
    "\n",
    "![Tensor dimensions confusion - screenshot from https://beerensahu.wordpress.com/2018/03/21/pytorch-tutorial-lesson-1-tensor/](https://i.imgur.com/jikCq6K.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e5cef8-40d6-4d5b-bb97-e4df26a43afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1022e+37,  4.5573e-41, -1.1022e+37],\n",
       "        [ 4.5573e-41,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c9412-13c2-485b-a1ec-bbc65b4036bc",
   "metadata": {},
   "source": [
    "Considering that a Tensor is much more mathematically inclined object than a multidimensional array, let me introduce a naming convention which will be compatible with mathematical objects a Tensor represents. For instance, a vector from $\\mathbb{R}^n$ is considered $n$-dimensional in mathematics but it can be stored in a one-dimensional array, a matrix in $\\mathbb{R}^{r \\times c}$ is considered to be $r \\times c$-dimensional  object, but can be stored in a two-dimensional array. This duality creates a confusion. In my experience the following naming convention is coherent considering that Tensors **are** mathematical objects.\n",
    "\n",
    "Tensor Terms  | Meaning | Multidimensional Array Terms\n",
    "---|---|---\n",
    "**order** | number of indices (levels) in a Tensor | dimension\n",
    "**dimension** | number of components a Tensor can store in a given order | size\n",
    "Tensor of order zero | a constant | - |\n",
    "Tensor of order one | a vector | one-dimensional array |\n",
    "Tensor of order two | a matrix | two-dimensional array |\n",
    "`torch.tensor([1.1, 4.12, 8.9, 14.85])` | an order-one 4-dimensional tensor representing a vector in $\\mathbb{R}^4$ | `[1.1, 4.12, 8.9, 14.85]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13e007-c5e4-43c3-9138-f5ef678d3653",
   "metadata": {},
   "source": [
    "# Predefined Loss Functions in PyTorch\n",
    "\n",
    "In PyTorch there are predefined methods for some of the loss functions. Of course, one is free to define his own loss functions, too, but a predefined loss functions have some advantages\n",
    "- the implementation is numerically stable. As an example, Cross Entropy has a logarithm following the exponent. If you do that correctly, the result is identity. But the exponent of even moderately large values is infinite in numerical calculations.\n",
    "- they usually have more efficient implementations\n",
    "- they have built-in reduction methods\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) The predefined loss functions in PyTorch are all part of `torch.nn.functional` and are [documented here](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "\n",
    "Let us have a look at a predefined version of MSE loss that we've juest calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e85b939-3b1a-4029-acf9-3da47d473f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3392)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1564a-61cd-4429-b9dd-c36ca674db1c",
   "metadata": {},
   "source": [
    "### Reductions\n",
    "\n",
    "Reductions are ways to consolidate the result into one value. Depending on the exact use-case, \n",
    "a user may want to calculate mean loos (as in the definition of MSE) or the sum or even the individual loss values (before any consolidation). This can be controlled with the \n",
    "additional parameter `reduction`\n",
    "\n",
    "- `reduction=\"mean\"`  (this is the default)\n",
    "- `reduction=\"sum\"`\n",
    "- `reduction=\"none\"` to get an array of individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7844f0-1cbd-45ac-87b0-c757d9ff0959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3569)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]), reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c5c84d-1ec7-4964-b41b-4d29b7aadeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0100, 0.0144, 0.0100, 1.3225])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42802f49-30ae-4532-ab0b-edf1b9cbd73d",
   "metadata": {},
   "source": [
    "We can see that the fourth element of a prediction error had a largest contribution in a MSE by using `reduction = \"none\"`. If you want to sum the values, rather than average them, use `reduction = \"sum\"`. \n",
    "\n",
    "## Classification loss\n",
    "\n",
    "### Two classes - Binary Cross Entropy\n",
    "\n",
    "OK, now let's examine other loss functions, the ones that are used for classification. If the target is only classes 0 or 1, and predictions are floats between 0 and 1, then it is binary classification and the appropriate loss is Binary Cross Entropy loss with the following usage example in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4926174c-31c5-4cfc-93e0-3af3b31e2d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2614, 0.1165, 0.0101])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy(torch.tensor([0.0, 0.77, 0.11, 0.99]), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c9724-b096-45ee-b3f8-19cbaa69226f",
   "metadata": {},
   "source": [
    "If the target is only classes 0 or 1, and predictions are arbitrary floats, then it is still binary classification but before using Binary Cross Entropy loss the values should be transformed first into $<0,1>$ interval with the use of a Sigmoid: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d14579ae-7ea3-493c-b694-36fcb483d47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2693e-01, 4.2789e-02, 6.9315e-01, 1.0000e+02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy(torch.sigmoid(torch.tensor([-2.0, 3.13, 0.0, -120.0])), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf37b51-fc2e-4b6b-9325-95a4b827cea0",
   "metadata": {},
   "source": [
    "Or, you can apply a sigmoid automatically (which is recommended because of numerical stability) with using `torch.nn.functional.binary_cross_entropy_with_logits()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c366a7-793e-4168-8fa2-2854b2a53184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2693e-01, 4.2789e-02, 6.9315e-01, 1.2000e+02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-2.0, 3.13, 0.0, -120.0]), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43243162-d179-4917-ae44-39dac88763dd",
   "metadata": {},
   "source": [
    "### Arbitrary number of classes - Cross Entropy\n",
    "\n",
    "It used for multiclass classification, but - in principle - there is nothing stopping us from using it for two classes, too. Please observe, that instead of a vector of logits, you must provide the loss function with raw predictions for all classes (thus, as predictions, we provide a tensor with one more level, i.e. with an increased order). A softmax is executed internally. Observe, that classes are provided as `torch.long` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "003f5c2f-68b6-42e0-8f66-1ab226473f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.6931,   0.6931,   0.6931, 120.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(torch.tensor([[-2.0, -2.0], [3.14, 3.14], [0.0, 0.0], [120.0, 0.0]]), torch.tensor([0,1,0,1]).type(torch.long), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f266b91-b360-4d9b-8b1a-6638d0498687",
   "metadata": {},
   "source": [
    "### Summary of classification loss options\n",
    "\n",
    "![summary](https://imgur.com/COfTYRh.png) \n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #2\n",
    "\n",
    "You do a two class classification task. Your classifier has a few layers, but it does NOT have a sigmoid as the final nonlinarity. Rather, it may output arbitrary values between minus and plus infinity.\n",
    "\n",
    "For the input data examples with known classes $(0,1,0,1,1,1,0)$ the classifier outputs $(-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9)$.\n",
    "\n",
    "What is the mean Binary Cross Entropy Loss?\n",
    "What is the Binary Cross Entropy Loss for the individual data points?\n",
    "Which data point has maximal loss, and which has the minimal loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091b76c5-e1de-487d-bae2-e941ea66febf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1366)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"mean\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8510b7fe-7f6f-4385-bb96-726b05948990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7418e-06, 4.8587e-02, 8.5436e-01, 5.3563e-02, 0.0000e+00, 7.1526e-07,\n",
       "        6.7949e-06])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfc0f5e-31a9-4aa9-baf3-9436b7e33c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df20d37-d191-4bf4-b28a-84d95dc0d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df8167-e708-4a02-b5c8-d22e70732644",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #3\n",
    "\n",
    "You do a three class classification task. Your classifier has a few layers and it outputs arbitrary values between minus and plus infinity.\n",
    "\n",
    "For the four input data examples with known classes $(0, 2, 1, 0)$ the classifier outputs \n",
    "- $(2.8, -2.0, 0.1)$ for the first data point,\n",
    "- $(2.1, 0.3, 1.8)$ for the second data point,\n",
    "- $(0.2, 0.2, 0.3)$ for the third data point,\n",
    "- $(0.1, -0.2, 0.0)$ for the last data point.\n",
    "\n",
    "What is the mean Cross Entropy Loss?\n",
    "What is the Cross Entropy Loss for the individual data points?\n",
    "Which data point has maximal loss, and which has the minimal loss?\n",
    "\n",
    "**Remember!**\n",
    "- increase the order of prediction tensor by one level.\n",
    "- target classes should be provided as `torch.long` type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fdc8c00-a576-4ebf-8d8f-d7f09af8128f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7809)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"mean\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0444e08c-c486-4103-83ce-ef382dadf6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0727, 0.9451, 1.1331, 0.9729])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"none\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4644a11-7f5f-462a-893e-e1290e62b239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb0b2e7-8ba9-4635-acf9-8205ce2c6eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e445a29-ba2d-4a6d-bd99-0a6c7e1d239e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
