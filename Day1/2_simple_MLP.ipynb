{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers separated by nonlinear components (also called *activation functions*). A nonlinear component in-between the linear layers is essential: without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. Also, please note that it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. You can read more about this dataset here: https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snowakowski/venv/jupyter/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "### Your task #1\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "### Your task #2\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1306604762738429, 0.30810780385646264)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([4, 7, 5,  ..., 3, 9, 0])\n",
      "1 -th batch labels : tensor([6, 0, 3,  ..., 4, 8, 9])\n",
      "2 -th batch labels : tensor([6, 6, 1,  ..., 8, 6, 1])\n",
      "3 -th batch labels : tensor([5, 0, 5,  ..., 8, 1, 1])\n",
      "4 -th batch labels : tensor([8, 2, 3,  ..., 6, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "### Your taks #3\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i==0:\n",
    "            print(i, \"-th batch inputs :\", batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
    "                                          #that output from a preceding layer serves as input to the next layer \n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3466291427612305\n",
      "epoch: 0 batch: 1 current batch loss: 2.3982832431793213\n",
      "epoch: 0 batch: 2 current batch loss: 2.3480312824249268\n",
      "epoch: 0 batch: 3 current batch loss: 2.321091651916504\n",
      "epoch: 0 batch: 4 current batch loss: 2.302859306335449\n",
      "epoch: 0 batch: 5 current batch loss: 2.2672696113586426\n",
      "epoch: 0 batch: 6 current batch loss: 2.2536706924438477\n",
      "epoch: 0 batch: 7 current batch loss: 2.24422025680542\n",
      "epoch: 0 batch: 8 current batch loss: 2.2208704948425293\n",
      "epoch: 0 batch: 9 current batch loss: 2.168870449066162\n",
      "epoch: 0 batch: 10 current batch loss: 2.1297645568847656\n",
      "epoch: 0 batch: 11 current batch loss: 2.045072078704834\n",
      "epoch: 0 batch: 12 current batch loss: 1.96918523311615\n",
      "epoch: 0 batch: 13 current batch loss: 1.9073771238327026\n",
      "epoch: 0 batch: 14 current batch loss: 1.8204909563064575\n",
      "epoch: 0 batch: 15 current batch loss: 1.7263108491897583\n",
      "epoch: 0 batch: 16 current batch loss: 1.6664689779281616\n",
      "epoch: 0 batch: 17 current batch loss: 1.5915780067443848\n",
      "epoch: 0 batch: 18 current batch loss: 1.5132744312286377\n",
      "epoch: 0 batch: 19 current batch loss: 1.4549974203109741\n",
      "epoch: 0 batch: 20 current batch loss: 1.3896780014038086\n",
      "epoch: 0 batch: 21 current batch loss: 1.3193188905715942\n",
      "epoch: 0 batch: 22 current batch loss: 1.2680224180221558\n",
      "epoch: 0 batch: 23 current batch loss: 1.2361849546432495\n",
      "epoch: 0 batch: 24 current batch loss: 1.180603265762329\n",
      "epoch: 0 batch: 25 current batch loss: 1.1708568334579468\n",
      "epoch: 0 batch: 26 current batch loss: 1.133787751197815\n",
      "epoch: 0 batch: 27 current batch loss: 1.0713863372802734\n",
      "epoch: 0 batch: 28 current batch loss: 1.0341167449951172\n",
      "epoch: 0 batch: 29 current batch loss: 0.9784512519836426\n",
      "epoch: 1 batch: 0 current batch loss: 0.965184211730957\n",
      "epoch: 1 batch: 1 current batch loss: 0.9253491163253784\n",
      "epoch: 1 batch: 2 current batch loss: 0.9403601288795471\n",
      "epoch: 1 batch: 3 current batch loss: 0.8846655488014221\n",
      "epoch: 1 batch: 4 current batch loss: 0.881665050983429\n",
      "epoch: 1 batch: 5 current batch loss: 0.8115000128746033\n",
      "epoch: 1 batch: 6 current batch loss: 0.8002439737319946\n",
      "epoch: 1 batch: 7 current batch loss: 0.7695276141166687\n",
      "epoch: 1 batch: 8 current batch loss: 0.7678694725036621\n",
      "epoch: 1 batch: 9 current batch loss: 0.7439978718757629\n",
      "epoch: 1 batch: 10 current batch loss: 0.7003986239433289\n",
      "epoch: 1 batch: 11 current batch loss: 0.6517091393470764\n",
      "epoch: 1 batch: 12 current batch loss: 0.6679837107658386\n",
      "epoch: 1 batch: 13 current batch loss: 0.6595968008041382\n",
      "epoch: 1 batch: 14 current batch loss: 0.6279858350753784\n",
      "epoch: 1 batch: 15 current batch loss: 0.6064265966415405\n",
      "epoch: 1 batch: 16 current batch loss: 0.5782080888748169\n",
      "epoch: 1 batch: 17 current batch loss: 0.5634997487068176\n",
      "epoch: 1 batch: 18 current batch loss: 0.5667200088500977\n",
      "epoch: 1 batch: 19 current batch loss: 0.5138992667198181\n",
      "epoch: 1 batch: 20 current batch loss: 0.5256295204162598\n",
      "epoch: 1 batch: 21 current batch loss: 0.5157164335250854\n",
      "epoch: 1 batch: 22 current batch loss: 0.4907565712928772\n",
      "epoch: 1 batch: 23 current batch loss: 0.4685925841331482\n",
      "epoch: 1 batch: 24 current batch loss: 0.5072388052940369\n",
      "epoch: 1 batch: 25 current batch loss: 0.45889297127723694\n",
      "epoch: 1 batch: 26 current batch loss: 0.45923471450805664\n",
      "epoch: 1 batch: 27 current batch loss: 0.41105008125305176\n",
      "epoch: 1 batch: 28 current batch loss: 0.42676040530204773\n",
      "epoch: 1 batch: 29 current batch loss: 0.4145975708961487\n",
      "epoch: 2 batch: 0 current batch loss: 0.41877320408821106\n",
      "epoch: 2 batch: 1 current batch loss: 0.38184893131256104\n",
      "epoch: 2 batch: 2 current batch loss: 0.4010225534439087\n",
      "epoch: 2 batch: 3 current batch loss: 0.3724352717399597\n",
      "epoch: 2 batch: 4 current batch loss: 0.3967495560646057\n",
      "epoch: 2 batch: 5 current batch loss: 0.34048396348953247\n",
      "epoch: 2 batch: 6 current batch loss: 0.376029908657074\n",
      "epoch: 2 batch: 7 current batch loss: 0.35810554027557373\n",
      "epoch: 2 batch: 8 current batch loss: 0.34008291363716125\n",
      "epoch: 2 batch: 9 current batch loss: 0.3412262797355652\n",
      "epoch: 2 batch: 10 current batch loss: 0.3451109230518341\n",
      "epoch: 2 batch: 11 current batch loss: 0.3437204360961914\n",
      "epoch: 2 batch: 12 current batch loss: 0.3298370838165283\n",
      "epoch: 2 batch: 13 current batch loss: 0.3027603328227997\n",
      "epoch: 2 batch: 14 current batch loss: 0.3059099614620209\n",
      "epoch: 2 batch: 15 current batch loss: 0.33971548080444336\n",
      "epoch: 2 batch: 16 current batch loss: 0.28603777289390564\n",
      "epoch: 2 batch: 17 current batch loss: 0.2918721139431\n",
      "epoch: 2 batch: 18 current batch loss: 0.2839343249797821\n",
      "epoch: 2 batch: 19 current batch loss: 0.28250259160995483\n",
      "epoch: 2 batch: 20 current batch loss: 0.26443371176719666\n",
      "epoch: 2 batch: 21 current batch loss: 0.302890419960022\n",
      "epoch: 2 batch: 22 current batch loss: 0.26269301772117615\n",
      "epoch: 2 batch: 23 current batch loss: 0.3178963363170624\n",
      "epoch: 2 batch: 24 current batch loss: 0.28207239508628845\n",
      "epoch: 2 batch: 25 current batch loss: 0.29554882645606995\n",
      "epoch: 2 batch: 26 current batch loss: 0.29625123739242554\n",
      "epoch: 2 batch: 27 current batch loss: 0.25843754410743713\n",
      "epoch: 2 batch: 28 current batch loss: 0.265213280916214\n",
      "epoch: 2 batch: 29 current batch loss: 0.24667662382125854\n",
      "epoch: 3 batch: 0 current batch loss: 0.2584052085876465\n",
      "epoch: 3 batch: 1 current batch loss: 0.21928201615810394\n",
      "epoch: 3 batch: 2 current batch loss: 0.23000921308994293\n",
      "epoch: 3 batch: 3 current batch loss: 0.2507786452770233\n",
      "epoch: 3 batch: 4 current batch loss: 0.23677143454551697\n",
      "epoch: 3 batch: 5 current batch loss: 0.22397449612617493\n",
      "epoch: 3 batch: 6 current batch loss: 0.25482645630836487\n",
      "epoch: 3 batch: 7 current batch loss: 0.23434945940971375\n",
      "epoch: 3 batch: 8 current batch loss: 0.23878835141658783\n",
      "epoch: 3 batch: 9 current batch loss: 0.23036548495292664\n",
      "epoch: 3 batch: 10 current batch loss: 0.22163525223731995\n",
      "epoch: 3 batch: 11 current batch loss: 0.20709757506847382\n",
      "epoch: 3 batch: 12 current batch loss: 0.23929373919963837\n",
      "epoch: 3 batch: 13 current batch loss: 0.21205396950244904\n",
      "epoch: 3 batch: 14 current batch loss: 0.2343243509531021\n",
      "epoch: 3 batch: 15 current batch loss: 0.2234562188386917\n",
      "epoch: 3 batch: 16 current batch loss: 0.21420595049858093\n",
      "epoch: 3 batch: 17 current batch loss: 0.21505750715732574\n",
      "epoch: 3 batch: 18 current batch loss: 0.20931664109230042\n",
      "epoch: 3 batch: 19 current batch loss: 0.22024142742156982\n",
      "epoch: 3 batch: 20 current batch loss: 0.20470939576625824\n",
      "epoch: 3 batch: 21 current batch loss: 0.1984902322292328\n",
      "epoch: 3 batch: 22 current batch loss: 0.20599955320358276\n",
      "epoch: 3 batch: 23 current batch loss: 0.19669760763645172\n",
      "epoch: 3 batch: 24 current batch loss: 0.19957824051380157\n",
      "epoch: 3 batch: 25 current batch loss: 0.19339215755462646\n",
      "epoch: 3 batch: 26 current batch loss: 0.20738577842712402\n",
      "epoch: 3 batch: 27 current batch loss: 0.19792614877223969\n",
      "epoch: 3 batch: 28 current batch loss: 0.1869894415140152\n",
      "epoch: 3 batch: 29 current batch loss: 0.16700880229473114\n",
      "epoch: 4 batch: 0 current batch loss: 0.19839824736118317\n",
      "epoch: 4 batch: 1 current batch loss: 0.17373931407928467\n",
      "epoch: 4 batch: 2 current batch loss: 0.19511568546295166\n",
      "epoch: 4 batch: 3 current batch loss: 0.1844957172870636\n",
      "epoch: 4 batch: 4 current batch loss: 0.17114751040935516\n",
      "epoch: 4 batch: 5 current batch loss: 0.18904350697994232\n",
      "epoch: 4 batch: 6 current batch loss: 0.17074458301067352\n",
      "epoch: 4 batch: 7 current batch loss: 0.1629980057477951\n",
      "epoch: 4 batch: 8 current batch loss: 0.18231305480003357\n",
      "epoch: 4 batch: 9 current batch loss: 0.17549850046634674\n",
      "epoch: 4 batch: 10 current batch loss: 0.1646728217601776\n",
      "epoch: 4 batch: 11 current batch loss: 0.1776629388332367\n",
      "epoch: 4 batch: 12 current batch loss: 0.173502117395401\n",
      "epoch: 4 batch: 13 current batch loss: 0.15603099763393402\n",
      "epoch: 4 batch: 14 current batch loss: 0.158513143658638\n",
      "epoch: 4 batch: 15 current batch loss: 0.16488078236579895\n",
      "epoch: 4 batch: 16 current batch loss: 0.1867958903312683\n",
      "epoch: 4 batch: 17 current batch loss: 0.16756120324134827\n",
      "epoch: 4 batch: 18 current batch loss: 0.1718011349439621\n",
      "epoch: 4 batch: 19 current batch loss: 0.15997129678726196\n",
      "epoch: 4 batch: 20 current batch loss: 0.1437787264585495\n",
      "epoch: 4 batch: 21 current batch loss: 0.14354629814624786\n",
      "epoch: 4 batch: 22 current batch loss: 0.14623387157917023\n",
      "epoch: 4 batch: 23 current batch loss: 0.1411270797252655\n",
      "epoch: 4 batch: 24 current batch loss: 0.13875927031040192\n",
      "epoch: 4 batch: 25 current batch loss: 0.16529175639152527\n",
      "epoch: 4 batch: 26 current batch loss: 0.17220014333724976\n",
      "epoch: 4 batch: 27 current batch loss: 0.16019493341445923\n",
      "epoch: 4 batch: 28 current batch loss: 0.1569620668888092\n",
      "epoch: 4 batch: 29 current batch loss: 0.11593165248632431\n",
      "epoch: 5 batch: 0 current batch loss: 0.13899342715740204\n",
      "epoch: 5 batch: 1 current batch loss: 0.1455686092376709\n",
      "epoch: 5 batch: 2 current batch loss: 0.14440545439720154\n",
      "epoch: 5 batch: 3 current batch loss: 0.15672269463539124\n",
      "epoch: 5 batch: 4 current batch loss: 0.12848727405071259\n",
      "epoch: 5 batch: 5 current batch loss: 0.12786825001239777\n",
      "epoch: 5 batch: 6 current batch loss: 0.13890276849269867\n",
      "epoch: 5 batch: 7 current batch loss: 0.13567182421684265\n",
      "epoch: 5 batch: 8 current batch loss: 0.1505793035030365\n",
      "epoch: 5 batch: 9 current batch loss: 0.13613194227218628\n",
      "epoch: 5 batch: 10 current batch loss: 0.1383923888206482\n",
      "epoch: 5 batch: 11 current batch loss: 0.12151513993740082\n",
      "epoch: 5 batch: 12 current batch loss: 0.13840532302856445\n",
      "epoch: 5 batch: 13 current batch loss: 0.1263427436351776\n",
      "epoch: 5 batch: 14 current batch loss: 0.11767659336328506\n",
      "epoch: 5 batch: 15 current batch loss: 0.11014304310083389\n",
      "epoch: 5 batch: 16 current batch loss: 0.1333189606666565\n",
      "epoch: 5 batch: 17 current batch loss: 0.1308058500289917\n",
      "epoch: 5 batch: 18 current batch loss: 0.119565948843956\n",
      "epoch: 5 batch: 19 current batch loss: 0.1193479597568512\n",
      "epoch: 5 batch: 20 current batch loss: 0.13640530407428741\n",
      "epoch: 5 batch: 21 current batch loss: 0.11936965584754944\n",
      "epoch: 5 batch: 22 current batch loss: 0.15517087280750275\n",
      "epoch: 5 batch: 23 current batch loss: 0.12703996896743774\n",
      "epoch: 5 batch: 24 current batch loss: 0.12642453610897064\n",
      "epoch: 5 batch: 25 current batch loss: 0.14155516028404236\n",
      "epoch: 5 batch: 26 current batch loss: 0.12613452970981598\n",
      "epoch: 5 batch: 27 current batch loss: 0.12516430020332336\n",
      "epoch: 5 batch: 28 current batch loss: 0.11861051619052887\n",
      "epoch: 5 batch: 29 current batch loss: 0.11802546679973602\n",
      "epoch: 6 batch: 0 current batch loss: 0.10145142674446106\n",
      "epoch: 6 batch: 1 current batch loss: 0.12087976187467575\n",
      "epoch: 6 batch: 2 current batch loss: 0.10959870368242264\n",
      "epoch: 6 batch: 3 current batch loss: 0.09425350278615952\n",
      "epoch: 6 batch: 4 current batch loss: 0.1071828156709671\n",
      "epoch: 6 batch: 5 current batch loss: 0.09193525463342667\n",
      "epoch: 6 batch: 6 current batch loss: 0.1269589066505432\n",
      "epoch: 6 batch: 7 current batch loss: 0.12961508333683014\n",
      "epoch: 6 batch: 8 current batch loss: 0.10119584202766418\n",
      "epoch: 6 batch: 9 current batch loss: 0.1288081407546997\n",
      "epoch: 6 batch: 10 current batch loss: 0.10903379321098328\n",
      "epoch: 6 batch: 11 current batch loss: 0.11032544076442719\n",
      "epoch: 6 batch: 12 current batch loss: 0.09761351346969604\n",
      "epoch: 6 batch: 13 current batch loss: 0.11604028940200806\n",
      "epoch: 6 batch: 14 current batch loss: 0.10309065133333206\n",
      "epoch: 6 batch: 15 current batch loss: 0.11121219396591187\n",
      "epoch: 6 batch: 16 current batch loss: 0.09886740893125534\n",
      "epoch: 6 batch: 17 current batch loss: 0.10488598793745041\n",
      "epoch: 6 batch: 18 current batch loss: 0.11573327332735062\n",
      "epoch: 6 batch: 19 current batch loss: 0.10135635733604431\n",
      "epoch: 6 batch: 20 current batch loss: 0.1110779196023941\n",
      "epoch: 6 batch: 21 current batch loss: 0.10959888994693756\n",
      "epoch: 6 batch: 22 current batch loss: 0.11424975842237473\n",
      "epoch: 6 batch: 23 current batch loss: 0.11534745991230011\n",
      "epoch: 6 batch: 24 current batch loss: 0.10971245169639587\n",
      "epoch: 6 batch: 25 current batch loss: 0.10805884003639221\n",
      "epoch: 6 batch: 26 current batch loss: 0.11986875534057617\n",
      "epoch: 6 batch: 27 current batch loss: 0.11775577068328857\n",
      "epoch: 6 batch: 28 current batch loss: 0.09127901494503021\n",
      "epoch: 6 batch: 29 current batch loss: 0.10655979067087173\n",
      "epoch: 7 batch: 0 current batch loss: 0.10950100421905518\n",
      "epoch: 7 batch: 1 current batch loss: 0.0724264532327652\n",
      "epoch: 7 batch: 2 current batch loss: 0.09046027064323425\n",
      "epoch: 7 batch: 3 current batch loss: 0.09071814268827438\n",
      "epoch: 7 batch: 4 current batch loss: 0.08191230148077011\n",
      "epoch: 7 batch: 5 current batch loss: 0.0873805433511734\n",
      "epoch: 7 batch: 6 current batch loss: 0.08125637471675873\n",
      "epoch: 7 batch: 7 current batch loss: 0.09216402471065521\n",
      "epoch: 7 batch: 8 current batch loss: 0.1065351814031601\n",
      "epoch: 7 batch: 9 current batch loss: 0.0925128310918808\n",
      "epoch: 7 batch: 10 current batch loss: 0.08348644524812698\n",
      "epoch: 7 batch: 11 current batch loss: 0.10636992752552032\n",
      "epoch: 7 batch: 12 current batch loss: 0.11067216843366623\n",
      "epoch: 7 batch: 13 current batch loss: 0.08322605490684509\n",
      "epoch: 7 batch: 14 current batch loss: 0.09721396863460541\n",
      "epoch: 7 batch: 15 current batch loss: 0.07591097056865692\n",
      "epoch: 7 batch: 16 current batch loss: 0.07946283370256424\n",
      "epoch: 7 batch: 17 current batch loss: 0.09938478469848633\n",
      "epoch: 7 batch: 18 current batch loss: 0.09405434876680374\n",
      "epoch: 7 batch: 19 current batch loss: 0.0959172323346138\n",
      "epoch: 7 batch: 20 current batch loss: 0.10007980465888977\n",
      "epoch: 7 batch: 21 current batch loss: 0.07800738513469696\n",
      "epoch: 7 batch: 22 current batch loss: 0.07404100894927979\n",
      "epoch: 7 batch: 23 current batch loss: 0.08788143843412399\n",
      "epoch: 7 batch: 24 current batch loss: 0.0929555743932724\n",
      "epoch: 7 batch: 25 current batch loss: 0.09007064253091812\n",
      "epoch: 7 batch: 26 current batch loss: 0.08970659971237183\n",
      "epoch: 7 batch: 27 current batch loss: 0.07154668867588043\n",
      "epoch: 7 batch: 28 current batch loss: 0.08031564205884933\n",
      "epoch: 7 batch: 29 current batch loss: 0.07750372588634491\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
    "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "### Your task #4\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
   "metadata": {},
   "source": [
    "### Answers to task #4\n",
    "\n",
    "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
    "\n",
    "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - the second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3852462768554688 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.3506107330322266 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.405066967010498 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.3624980449676514 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.3286421298980713 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.2850940227508545 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.2619080543518066 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.2628300189971924 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.250396728515625 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.2316884994506836 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.18247652053833 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.11496901512146 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 2.0512163639068604 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.9840277433395386 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.902214527130127 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.8234964609146118 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.755976915359497 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.6529632806777954 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.5604521036148071 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.501114010810852 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.4300259351730347 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.3656818866729736 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.31107759475708 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.2857342958450317 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.2079447507858276 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.1520441770553589 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 1.1116398572921753 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 1.0590062141418457 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 1.0313218832015991 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.9901143312454224 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.9483060240745544 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.8874692320823669 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.8924764394760132 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.8746505379676819 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.8385481834411621 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.8399797081947327 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.7574394941329956 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.7434217929840088 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.7197818756103516 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.7197164297103882 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.7062708735466003 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 11 current batch loss: 0.6433975100517273 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 12 current batch loss: 0.6630722880363464 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 13 current batch loss: 0.6126289367675781 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 14 current batch loss: 0.6196541786193848 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 15 current batch loss: 0.5932836532592773 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 16 current batch loss: 0.5735191702842712 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 17 current batch loss: 0.5400123596191406 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 18 current batch loss: 0.5243397355079651 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 19 current batch loss: 0.5117571353912354 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 20 current batch loss: 0.5306100249290466 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 21 current batch loss: 0.4847278296947479 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 22 current batch loss: 0.4922187626361847 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 23 current batch loss: 0.44380882382392883 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 24 current batch loss: 0.450994074344635 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 25 current batch loss: 0.45380815863609314 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 26 current batch loss: 0.45299315452575684 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 27 current batch loss: 0.4387551546096802 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 28 current batch loss: 0.42979446053504944 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 29 current batch loss: 0.4567353129386902 current lr: 0.0009000000000000001\n",
      "epoch: 2 batch: 0 current batch loss: 0.40303996205329895 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 1 current batch loss: 0.41022810339927673 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 2 current batch loss: 0.3886173367500305 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 3 current batch loss: 0.38241374492645264 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 4 current batch loss: 0.36940839886665344 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 5 current batch loss: 0.3847900927066803 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 6 current batch loss: 0.377693772315979 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 7 current batch loss: 0.3822987377643585 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 8 current batch loss: 0.39870771765708923 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 9 current batch loss: 0.36626410484313965 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 10 current batch loss: 0.38145387172698975 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 11 current batch loss: 0.348955899477005 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 12 current batch loss: 0.3491246700286865 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 13 current batch loss: 0.3538711667060852 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 14 current batch loss: 0.32848629355430603 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 15 current batch loss: 0.3418785333633423 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 16 current batch loss: 0.3205704689025879 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 17 current batch loss: 0.3245175778865814 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 18 current batch loss: 0.3336678743362427 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 19 current batch loss: 0.31293174624443054 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 20 current batch loss: 0.3025768995285034 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 21 current batch loss: 0.3244352638721466 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 22 current batch loss: 0.28220492601394653 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 23 current batch loss: 0.29775047302246094 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 24 current batch loss: 0.3076731264591217 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 25 current batch loss: 0.30431076884269714 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 26 current batch loss: 0.3100418746471405 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 27 current batch loss: 0.27647724747657776 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 28 current batch loss: 0.2765071988105774 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 29 current batch loss: 0.2502533495426178 current lr: 0.0008100000000000001\n",
      "epoch: 3 batch: 0 current batch loss: 0.30614903569221497 current lr: 0.000729\n",
      "epoch: 3 batch: 1 current batch loss: 0.28093791007995605 current lr: 0.000729\n",
      "epoch: 3 batch: 2 current batch loss: 0.2936665713787079 current lr: 0.000729\n",
      "epoch: 3 batch: 3 current batch loss: 0.26756173372268677 current lr: 0.000729\n",
      "epoch: 3 batch: 4 current batch loss: 0.29366132616996765 current lr: 0.000729\n",
      "epoch: 3 batch: 5 current batch loss: 0.26926419138908386 current lr: 0.000729\n",
      "epoch: 3 batch: 6 current batch loss: 0.2590772807598114 current lr: 0.000729\n",
      "epoch: 3 batch: 7 current batch loss: 0.2656084895133972 current lr: 0.000729\n",
      "epoch: 3 batch: 8 current batch loss: 0.2703239619731903 current lr: 0.000729\n",
      "epoch: 3 batch: 9 current batch loss: 0.25677981972694397 current lr: 0.000729\n",
      "epoch: 3 batch: 10 current batch loss: 0.24954263865947723 current lr: 0.000729\n",
      "epoch: 3 batch: 11 current batch loss: 0.2933913469314575 current lr: 0.000729\n",
      "epoch: 3 batch: 12 current batch loss: 0.24510899186134338 current lr: 0.000729\n",
      "epoch: 3 batch: 13 current batch loss: 0.2550015449523926 current lr: 0.000729\n",
      "epoch: 3 batch: 14 current batch loss: 0.2795468866825104 current lr: 0.000729\n",
      "epoch: 3 batch: 15 current batch loss: 0.22949856519699097 current lr: 0.000729\n",
      "epoch: 3 batch: 16 current batch loss: 0.2594180405139923 current lr: 0.000729\n",
      "epoch: 3 batch: 17 current batch loss: 0.23113125562667847 current lr: 0.000729\n",
      "epoch: 3 batch: 18 current batch loss: 0.2432553917169571 current lr: 0.000729\n",
      "epoch: 3 batch: 19 current batch loss: 0.2617611289024353 current lr: 0.000729\n",
      "epoch: 3 batch: 20 current batch loss: 0.24795138835906982 current lr: 0.000729\n",
      "epoch: 3 batch: 21 current batch loss: 0.21069233119487762 current lr: 0.000729\n",
      "epoch: 3 batch: 22 current batch loss: 0.23122772574424744 current lr: 0.000729\n",
      "epoch: 3 batch: 23 current batch loss: 0.23191292583942413 current lr: 0.000729\n",
      "epoch: 3 batch: 24 current batch loss: 0.2190321683883667 current lr: 0.000729\n",
      "epoch: 3 batch: 25 current batch loss: 0.22608965635299683 current lr: 0.000729\n",
      "epoch: 3 batch: 26 current batch loss: 0.2066863626241684 current lr: 0.000729\n",
      "epoch: 3 batch: 27 current batch loss: 0.2231966257095337 current lr: 0.000729\n",
      "epoch: 3 batch: 28 current batch loss: 0.24196544289588928 current lr: 0.000729\n",
      "epoch: 3 batch: 29 current batch loss: 0.19403891265392303 current lr: 0.000729\n",
      "epoch: 4 batch: 0 current batch loss: 0.21840813755989075 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 1 current batch loss: 0.22129137814044952 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 2 current batch loss: 0.20761342346668243 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 3 current batch loss: 0.22478286921977997 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 4 current batch loss: 0.21586354076862335 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 5 current batch loss: 0.188369020819664 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 6 current batch loss: 0.21685099601745605 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 7 current batch loss: 0.18553893268108368 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 8 current batch loss: 0.19673624634742737 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 9 current batch loss: 0.2438001185655594 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 10 current batch loss: 0.20659410953521729 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 11 current batch loss: 0.2039858102798462 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 12 current batch loss: 0.2052944302558899 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 13 current batch loss: 0.20095817744731903 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 14 current batch loss: 0.21894869208335876 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 15 current batch loss: 0.1925904005765915 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 16 current batch loss: 0.19155317544937134 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 17 current batch loss: 0.1926121860742569 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 18 current batch loss: 0.2028476595878601 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 19 current batch loss: 0.19912874698638916 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 20 current batch loss: 0.1746380478143692 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 21 current batch loss: 0.20672689378261566 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 22 current batch loss: 0.19972431659698486 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 23 current batch loss: 0.16767896711826324 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 24 current batch loss: 0.17604725062847137 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 25 current batch loss: 0.20053493976593018 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 26 current batch loss: 0.19123519957065582 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 27 current batch loss: 0.21105150878429413 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 28 current batch loss: 0.19040818512439728 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 29 current batch loss: 0.18559013307094574 current lr: 0.0006561000000000001\n",
      "epoch: 5 batch: 0 current batch loss: 0.19125929474830627 current lr: 0.00059049\n",
      "epoch: 5 batch: 1 current batch loss: 0.1714789718389511 current lr: 0.00059049\n",
      "epoch: 5 batch: 2 current batch loss: 0.17959332466125488 current lr: 0.00059049\n",
      "epoch: 5 batch: 3 current batch loss: 0.18130376935005188 current lr: 0.00059049\n",
      "epoch: 5 batch: 4 current batch loss: 0.1788792461156845 current lr: 0.00059049\n",
      "epoch: 5 batch: 5 current batch loss: 0.21022816002368927 current lr: 0.00059049\n",
      "epoch: 5 batch: 6 current batch loss: 0.1868666559457779 current lr: 0.00059049\n",
      "epoch: 5 batch: 7 current batch loss: 0.16518235206604004 current lr: 0.00059049\n",
      "epoch: 5 batch: 8 current batch loss: 0.1751195341348648 current lr: 0.00059049\n",
      "epoch: 5 batch: 9 current batch loss: 0.17835761606693268 current lr: 0.00059049\n",
      "epoch: 5 batch: 10 current batch loss: 0.170265793800354 current lr: 0.00059049\n",
      "epoch: 5 batch: 11 current batch loss: 0.1395377665758133 current lr: 0.00059049\n",
      "epoch: 5 batch: 12 current batch loss: 0.16560468077659607 current lr: 0.00059049\n",
      "epoch: 5 batch: 13 current batch loss: 0.14949142932891846 current lr: 0.00059049\n",
      "epoch: 5 batch: 14 current batch loss: 0.1601921021938324 current lr: 0.00059049\n",
      "epoch: 5 batch: 15 current batch loss: 0.1533520221710205 current lr: 0.00059049\n",
      "epoch: 5 batch: 16 current batch loss: 0.18089112639427185 current lr: 0.00059049\n",
      "epoch: 5 batch: 17 current batch loss: 0.17832864820957184 current lr: 0.00059049\n",
      "epoch: 5 batch: 18 current batch loss: 0.143185555934906 current lr: 0.00059049\n",
      "epoch: 5 batch: 19 current batch loss: 0.16817925870418549 current lr: 0.00059049\n",
      "epoch: 5 batch: 20 current batch loss: 0.16611966490745544 current lr: 0.00059049\n",
      "epoch: 5 batch: 21 current batch loss: 0.17053519189357758 current lr: 0.00059049\n",
      "epoch: 5 batch: 22 current batch loss: 0.15760935842990875 current lr: 0.00059049\n",
      "epoch: 5 batch: 23 current batch loss: 0.17035727202892303 current lr: 0.00059049\n",
      "epoch: 5 batch: 24 current batch loss: 0.16029083728790283 current lr: 0.00059049\n",
      "epoch: 5 batch: 25 current batch loss: 0.1412055790424347 current lr: 0.00059049\n",
      "epoch: 5 batch: 26 current batch loss: 0.16119986772537231 current lr: 0.00059049\n",
      "epoch: 5 batch: 27 current batch loss: 0.16670522093772888 current lr: 0.00059049\n",
      "epoch: 5 batch: 28 current batch loss: 0.16925553977489471 current lr: 0.00059049\n",
      "epoch: 5 batch: 29 current batch loss: 0.15465806424617767 current lr: 0.00059049\n",
      "epoch: 6 batch: 0 current batch loss: 0.1903073787689209 current lr: 0.000531441\n",
      "epoch: 6 batch: 1 current batch loss: 0.1519659161567688 current lr: 0.000531441\n",
      "epoch: 6 batch: 2 current batch loss: 0.15270189940929413 current lr: 0.000531441\n",
      "epoch: 6 batch: 3 current batch loss: 0.14321623742580414 current lr: 0.000531441\n",
      "epoch: 6 batch: 4 current batch loss: 0.14857210218906403 current lr: 0.000531441\n",
      "epoch: 6 batch: 5 current batch loss: 0.14947509765625 current lr: 0.000531441\n",
      "epoch: 6 batch: 6 current batch loss: 0.15167629718780518 current lr: 0.000531441\n",
      "epoch: 6 batch: 7 current batch loss: 0.135122612118721 current lr: 0.000531441\n",
      "epoch: 6 batch: 8 current batch loss: 0.1620311439037323 current lr: 0.000531441\n",
      "epoch: 6 batch: 9 current batch loss: 0.14922109246253967 current lr: 0.000531441\n",
      "epoch: 6 batch: 10 current batch loss: 0.12393953651189804 current lr: 0.000531441\n",
      "epoch: 6 batch: 11 current batch loss: 0.11700262874364853 current lr: 0.000531441\n",
      "epoch: 6 batch: 12 current batch loss: 0.15702469646930695 current lr: 0.000531441\n",
      "epoch: 6 batch: 13 current batch loss: 0.17130698263645172 current lr: 0.000531441\n",
      "epoch: 6 batch: 14 current batch loss: 0.15245507657527924 current lr: 0.000531441\n",
      "epoch: 6 batch: 15 current batch loss: 0.13254112005233765 current lr: 0.000531441\n",
      "epoch: 6 batch: 16 current batch loss: 0.14774705469608307 current lr: 0.000531441\n",
      "epoch: 6 batch: 17 current batch loss: 0.14199934899806976 current lr: 0.000531441\n",
      "epoch: 6 batch: 18 current batch loss: 0.1746368706226349 current lr: 0.000531441\n",
      "epoch: 6 batch: 19 current batch loss: 0.125509575009346 current lr: 0.000531441\n",
      "epoch: 6 batch: 20 current batch loss: 0.13480669260025024 current lr: 0.000531441\n",
      "epoch: 6 batch: 21 current batch loss: 0.14164376258850098 current lr: 0.000531441\n",
      "epoch: 6 batch: 22 current batch loss: 0.11854290217161179 current lr: 0.000531441\n",
      "epoch: 6 batch: 23 current batch loss: 0.13712146878242493 current lr: 0.000531441\n",
      "epoch: 6 batch: 24 current batch loss: 0.12524853646755219 current lr: 0.000531441\n",
      "epoch: 6 batch: 25 current batch loss: 0.12244190275669098 current lr: 0.000531441\n",
      "epoch: 6 batch: 26 current batch loss: 0.1466296762228012 current lr: 0.000531441\n",
      "epoch: 6 batch: 27 current batch loss: 0.1447889804840088 current lr: 0.000531441\n",
      "epoch: 6 batch: 28 current batch loss: 0.14335455000400543 current lr: 0.000531441\n",
      "epoch: 6 batch: 29 current batch loss: 0.16914337873458862 current lr: 0.000531441\n",
      "epoch: 7 batch: 0 current batch loss: 0.12345565110445023 current lr: 0.0004782969\n",
      "epoch: 7 batch: 1 current batch loss: 0.13103879988193512 current lr: 0.0004782969\n",
      "epoch: 7 batch: 2 current batch loss: 0.14514125883579254 current lr: 0.0004782969\n",
      "epoch: 7 batch: 3 current batch loss: 0.11732055991888046 current lr: 0.0004782969\n",
      "epoch: 7 batch: 4 current batch loss: 0.13412167131900787 current lr: 0.0004782969\n",
      "epoch: 7 batch: 5 current batch loss: 0.12809385359287262 current lr: 0.0004782969\n",
      "epoch: 7 batch: 6 current batch loss: 0.12482777237892151 current lr: 0.0004782969\n",
      "epoch: 7 batch: 7 current batch loss: 0.1399262696504593 current lr: 0.0004782969\n",
      "epoch: 7 batch: 8 current batch loss: 0.14622065424919128 current lr: 0.0004782969\n",
      "epoch: 7 batch: 9 current batch loss: 0.12720532715320587 current lr: 0.0004782969\n",
      "epoch: 7 batch: 10 current batch loss: 0.1022685170173645 current lr: 0.0004782969\n",
      "epoch: 7 batch: 11 current batch loss: 0.12583328783512115 current lr: 0.0004782969\n",
      "epoch: 7 batch: 12 current batch loss: 0.13606399297714233 current lr: 0.0004782969\n",
      "epoch: 7 batch: 13 current batch loss: 0.1363307237625122 current lr: 0.0004782969\n",
      "epoch: 7 batch: 14 current batch loss: 0.13090461492538452 current lr: 0.0004782969\n",
      "epoch: 7 batch: 15 current batch loss: 0.12851685285568237 current lr: 0.0004782969\n",
      "epoch: 7 batch: 16 current batch loss: 0.12197185307741165 current lr: 0.0004782969\n",
      "epoch: 7 batch: 17 current batch loss: 0.11647836863994598 current lr: 0.0004782969\n",
      "epoch: 7 batch: 18 current batch loss: 0.12901684641838074 current lr: 0.0004782969\n",
      "epoch: 7 batch: 19 current batch loss: 0.12176888436079025 current lr: 0.0004782969\n",
      "epoch: 7 batch: 20 current batch loss: 0.12216699123382568 current lr: 0.0004782969\n",
      "epoch: 7 batch: 21 current batch loss: 0.12077364325523376 current lr: 0.0004782969\n",
      "epoch: 7 batch: 22 current batch loss: 0.13422910869121552 current lr: 0.0004782969\n",
      "epoch: 7 batch: 23 current batch loss: 0.12978805601596832 current lr: 0.0004782969\n",
      "epoch: 7 batch: 24 current batch loss: 0.11626435071229935 current lr: 0.0004782969\n",
      "epoch: 7 batch: 25 current batch loss: 0.11077478528022766 current lr: 0.0004782969\n",
      "epoch: 7 batch: 26 current batch loss: 0.11582276970148087 current lr: 0.0004782969\n",
      "epoch: 7 batch: 27 current batch loss: 0.12853504717350006 current lr: 0.0004782969\n",
      "epoch: 7 batch: 28 current batch loss: 0.1376575082540512 current lr: 0.0004782969\n",
      "epoch: 7 batch: 29 current batch loss: 0.1408080905675888 current lr: 0.0004782969\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "### Your task #5\n",
    "\n",
    "Well, it seems that we were able to get the learning rate much below 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "Some other schedulers you might want to experiment with: \n",
    "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
    "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
    "\n",
    "### Your task #6\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9685\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9612\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5c6ca-1055-4d9f-b6e0-3e4601f3919d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
