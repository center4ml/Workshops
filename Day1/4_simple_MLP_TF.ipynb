{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAmgHA43ujGw"
   },
   "source": [
    "This is an introduction to Tensorflow 2 with the Keras API. We will work with the MNIST dataset of hand-written digits and:\n",
    "*   classify them using logistic regression and multi-layer dense networks\n",
    "*   examine various network depths, activation functions and optimizers\n",
    "*   introduce convolutional networks and the LeNet5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLBuViOPv7Oa"
   },
   "source": [
    "Import Tensorflow, load the MNIST dataset using Tensorflow built-in tools and display some characteristics of the data and target arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwS4WAtBuiIW",
    "outputId": "5d31f1df-c311-4266-b17e-8a6612782a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "<class 'numpy.ndarray'> (60000, 28, 28) uint8 0 255\n",
      "<class 'numpy.ndarray'> (60000,) uint8 0 9\n",
      "<class 'numpy.ndarray'> (10000, 28, 28) uint8 0 255\n",
      "<class 'numpy.ndarray'> (10000,) uint8 0 9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(DATA0, TARGET0), (DATA1, TARGET1) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(type(DATA0), DATA0.shape, DATA0.dtype, DATA0.min(), DATA0.max())\n",
    "print(type(TARGET0), TARGET0.shape, TARGET0.dtype, TARGET0.min(), TARGET0.max())\n",
    "\n",
    "print(type(DATA1), DATA1.shape, DATA1.dtype, DATA1.min(), DATA1.max())\n",
    "print(type(TARGET1), TARGET1.shape, TARGET1.dtype, TARGET1.min(), TARGET1.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en3XxGZaxkr2"
   },
   "source": [
    "These are ordinary numpy arrays. There are 60 000 samples in the training dataset and 10 000 in the validation dataset. The targets are integer numbers from 0 to 9 labeling the digits. The data are 28x28 grayscale images with values of type uint8, ranging from 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7v7P3sADg7-"
   },
   "source": [
    "**Task.** Display the first image from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnD7N8gfEHEF"
   },
   "source": [
    "Dense networks take flat vectors of floating-point features and are best suited for inputs of the order of 1 rather than 255. So flatten the images, cast them to float32 and scale to the range from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeBjEJ1lMR9W"
   },
   "outputs": [],
   "source": [
    "DATA0 = DATA0.reshape(-1, 28 * 28).astype('float32') / 255.\n",
    "DATA1 = DATA1.reshape(-1, 28 * 28).astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-SOIgEN00vM"
   },
   "source": [
    "Build the simplest Tensorflow model. This is a single-layer dense network with softmax activation, equivalent to logistic regression. It has 10 outputs corresponding to digits from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt1_McQT17ls"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI2RBXmr2XhG"
   },
   "source": [
    "The activation functions, like softmax, sigmoid, relu, etc., can be treated as separate layers or equivalently added at the outputs of the dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrVwci7U3BoU"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Ht754M3ARg"
   },
   "source": [
    "The model has not been trained yet, so it will give incorrect predictions. But anyway pass the entire validation dataset thereto and display the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZKl_uxp4kU6",
    "outputId": "aa11bf98-14cf-4f6a-9a02-58cd405e7305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.11072053 0.07813346 0.09716906 ... 0.14569585 0.07165894 0.13054055]\n",
      " [0.03729106 0.08779248 0.06638742 ... 0.08476997 0.18627657 0.12411804]\n",
      " [0.07035261 0.10220047 0.09934175 ... 0.1036746  0.10641313 0.13612045]\n",
      " ...\n",
      " [0.05512704 0.16106337 0.11587422 ... 0.04533249 0.05776504 0.19524232]\n",
      " [0.06082381 0.10118923 0.13928978 ... 0.08741697 0.09241118 0.13897476]\n",
      " [0.03037542 0.06121332 0.22899926 ... 0.04853882 0.08292364 0.09486538]], shape=(10000, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "PROBABILITY1 = model(DATA1)\n",
    "print(PROBABILITY1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpwV3oAw5WkG"
   },
   "source": [
    "The output is not a numpy array but a tf.Tensor, an internal format on which Tensorflow operates. Convert it back to a numpy array for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MjlcSTW5zbb",
    "outputId": "6166e5ee-fa18-48fa-ac1a-d46ec1628c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11072053 0.07813346 0.09716906 ... 0.14569585 0.07165894 0.13054055]\n",
      " [0.03729106 0.08779248 0.06638742 ... 0.08476997 0.18627657 0.12411804]\n",
      " [0.07035261 0.10220047 0.09934175 ... 0.1036746  0.10641313 0.13612045]\n",
      " ...\n",
      " [0.05512704 0.16106337 0.11587422 ... 0.04533249 0.05776504 0.19524232]\n",
      " [0.06082381 0.10118923 0.13928978 ... 0.08741697 0.09241118 0.13897476]\n",
      " [0.03037542 0.06121332 0.22899926 ... 0.04853882 0.08292364 0.09486538]]\n"
     ]
    }
   ],
   "source": [
    "PROBABILITY1 = model(DATA1).numpy()\n",
    "print(PROBABILITY1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eozES9NZ6GRb"
   },
   "source": [
    "Each row of this array contains 10 probabilities of the sample representing digits from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfpP1s_SBA2w"
   },
   "source": [
    "**Task.** Run the code again from where the model is constructed and inspect the resulting probabilities. Why are these probabilities different every time the code is run? How does this relate to scaling the inputs to the range from 0 to 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWU-5EN-7X7l"
   },
   "source": [
    "**Task.** Given the probabilities, calculate the digits to which the samples are classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHyhWslq9aWN"
   },
   "source": [
    "**Task.** Compare the labels from the model with the correct ones and calculate the classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibghl9Qf-mbc"
   },
   "source": [
    "**Task.** Why is this accuracy close to 0.1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAxxsr5S_bGl"
   },
   "source": [
    "Associate the model with an optimizer, a loss function and a metric like accuracy. Here we use the simplest stochastic gradient descent as the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAnJzvmq_zHg"
   },
   "outputs": [],
   "source": [
    "model.compile('sgd',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euHfKTMaDgiy"
   },
   "source": [
    "Once the model is associated with a loss function and a metric, calculate their values on any dataset by calling a built-in method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNSXFvd_D8ZZ",
    "outputId": "e97cb7b3-b8db-42bb-b6e7-d47955cd4d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3108 - accuracy: 0.1297\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.3189 - accuracy: 0.1176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3188958168029785, 0.11760000139474869]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(DATA0, TARGET0)\n",
    "model.evaluate(DATA1, TARGET1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyz5ntS1AJur"
   },
   "source": [
    "The accuracy on the validation dataset is indeed equal to what we calculated manually. Now train the model on the training dataset. Do only a few epochs for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxHA190WAWrl",
    "outputId": "cbd8b495-5d23-4a11-a293-cbf13d75084a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.7690 - accuracy: 0.8211\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4540 - accuracy: 0.8825\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4017 - accuracy: 0.8928\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3755 - accuracy: 0.8978\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3591 - accuracy: 0.9013\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3474 - accuracy: 0.9036\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3383 - accuracy: 0.9060\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3314 - accuracy: 0.9079\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3255 - accuracy: 0.9094\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3206 - accuracy: 0.9107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e7fb2a390>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(DATA0, TARGET0, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfLV0jShHcw2"
   },
   "source": [
    "The built-in training loop displays the loss and accuracy on the training dataset after each epoch. It also prints some other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibUM4grfLinC"
   },
   "source": [
    "**Task.** What does the progress bar indicate?  What does the number of 1 875 stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29qcJlvDLUBT"
   },
   "source": [
    "The accuracy on the training dataset may not be representative for new samples due to the so-called overfitting. To get a more reliable estimation of the model accuracy, calculate it on the validation dataset. Also change the batch size and do more epochs to perform a full training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEqqjdTFIGQ_",
    "outputId": "5b79d2cd-3d7e-4ec5-ea85-8e0d02810618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3169 - accuracy: 0.9120 - val_loss: 0.3044 - val_accuracy: 0.9168\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3157 - accuracy: 0.9122 - val_loss: 0.3033 - val_accuracy: 0.9164\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3145 - accuracy: 0.9126 - val_loss: 0.3027 - val_accuracy: 0.9165\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3132 - accuracy: 0.9129 - val_loss: 0.3017 - val_accuracy: 0.9159\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3122 - accuracy: 0.9132 - val_loss: 0.3007 - val_accuracy: 0.9161\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3111 - accuracy: 0.9139 - val_loss: 0.3002 - val_accuracy: 0.9171\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3101 - accuracy: 0.9140 - val_loss: 0.2989 - val_accuracy: 0.9168\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3091 - accuracy: 0.9143 - val_loss: 0.2981 - val_accuracy: 0.9165\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3081 - accuracy: 0.9143 - val_loss: 0.2979 - val_accuracy: 0.9172\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3073 - accuracy: 0.9147 - val_loss: 0.2968 - val_accuracy: 0.9167\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3063 - accuracy: 0.9152 - val_loss: 0.2961 - val_accuracy: 0.9178\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3054 - accuracy: 0.9153 - val_loss: 0.2958 - val_accuracy: 0.9182\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3047 - accuracy: 0.9156 - val_loss: 0.2948 - val_accuracy: 0.9179\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3039 - accuracy: 0.9159 - val_loss: 0.2944 - val_accuracy: 0.9175\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3031 - accuracy: 0.9160 - val_loss: 0.2937 - val_accuracy: 0.9178\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3023 - accuracy: 0.9161 - val_loss: 0.2935 - val_accuracy: 0.9183\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3016 - accuracy: 0.9164 - val_loss: 0.2926 - val_accuracy: 0.9184\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3009 - accuracy: 0.9164 - val_loss: 0.2921 - val_accuracy: 0.9191\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3002 - accuracy: 0.9165 - val_loss: 0.2916 - val_accuracy: 0.9182\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2996 - accuracy: 0.9167 - val_loss: 0.2911 - val_accuracy: 0.9180\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2989 - accuracy: 0.9172 - val_loss: 0.2905 - val_accuracy: 0.9187\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2983 - accuracy: 0.9171 - val_loss: 0.2903 - val_accuracy: 0.9190\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2977 - accuracy: 0.9173 - val_loss: 0.2899 - val_accuracy: 0.9189\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2971 - accuracy: 0.9175 - val_loss: 0.2893 - val_accuracy: 0.9186\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2965 - accuracy: 0.9178 - val_loss: 0.2889 - val_accuracy: 0.9190\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2959 - accuracy: 0.9179 - val_loss: 0.2885 - val_accuracy: 0.9188\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2954 - accuracy: 0.9180 - val_loss: 0.2884 - val_accuracy: 0.9192\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2948 - accuracy: 0.9185 - val_loss: 0.2877 - val_accuracy: 0.9192\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2943 - accuracy: 0.9183 - val_loss: 0.2876 - val_accuracy: 0.9196\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2938 - accuracy: 0.9186 - val_loss: 0.2871 - val_accuracy: 0.9189\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2933 - accuracy: 0.9190 - val_loss: 0.2865 - val_accuracy: 0.9192\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2928 - accuracy: 0.9186 - val_loss: 0.2863 - val_accuracy: 0.9195\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2923 - accuracy: 0.9188 - val_loss: 0.2861 - val_accuracy: 0.9194\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2919 - accuracy: 0.9192 - val_loss: 0.2856 - val_accuracy: 0.9196\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2914 - accuracy: 0.9190 - val_loss: 0.2854 - val_accuracy: 0.9199\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2910 - accuracy: 0.9194 - val_loss: 0.2852 - val_accuracy: 0.9201\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2905 - accuracy: 0.9193 - val_loss: 0.2846 - val_accuracy: 0.9203\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2901 - accuracy: 0.9195 - val_loss: 0.2848 - val_accuracy: 0.9197\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2897 - accuracy: 0.9198 - val_loss: 0.2841 - val_accuracy: 0.9198\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2893 - accuracy: 0.9198 - val_loss: 0.2838 - val_accuracy: 0.9202\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2889 - accuracy: 0.9196 - val_loss: 0.2838 - val_accuracy: 0.9203\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2885 - accuracy: 0.9201 - val_loss: 0.2834 - val_accuracy: 0.9201\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2881 - accuracy: 0.9198 - val_loss: 0.2831 - val_accuracy: 0.9200\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2877 - accuracy: 0.9201 - val_loss: 0.2830 - val_accuracy: 0.9198\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2873 - accuracy: 0.9200 - val_loss: 0.2828 - val_accuracy: 0.9203\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2870 - accuracy: 0.9204 - val_loss: 0.2824 - val_accuracy: 0.9200\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2866 - accuracy: 0.9204 - val_loss: 0.2821 - val_accuracy: 0.9203\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2863 - accuracy: 0.9203 - val_loss: 0.2820 - val_accuracy: 0.9198\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2859 - accuracy: 0.9205 - val_loss: 0.2817 - val_accuracy: 0.9202\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2856 - accuracy: 0.9204 - val_loss: 0.2814 - val_accuracy: 0.9202\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2853 - accuracy: 0.9208 - val_loss: 0.2813 - val_accuracy: 0.9202\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2849 - accuracy: 0.9208 - val_loss: 0.2810 - val_accuracy: 0.9204\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2845 - accuracy: 0.9208 - val_loss: 0.2809 - val_accuracy: 0.9212\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2843 - accuracy: 0.9209 - val_loss: 0.2806 - val_accuracy: 0.9207\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2840 - accuracy: 0.9212 - val_loss: 0.2802 - val_accuracy: 0.9209\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2837 - accuracy: 0.9214 - val_loss: 0.2804 - val_accuracy: 0.9204\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2834 - accuracy: 0.9209 - val_loss: 0.2801 - val_accuracy: 0.9204\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2831 - accuracy: 0.9212 - val_loss: 0.2799 - val_accuracy: 0.9207\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2828 - accuracy: 0.9214 - val_loss: 0.2799 - val_accuracy: 0.9207\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2825 - accuracy: 0.9215 - val_loss: 0.2796 - val_accuracy: 0.9214\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2822 - accuracy: 0.9215 - val_loss: 0.2795 - val_accuracy: 0.9208\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2819 - accuracy: 0.9215 - val_loss: 0.2792 - val_accuracy: 0.9213\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2817 - accuracy: 0.9219 - val_loss: 0.2790 - val_accuracy: 0.9213\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2814 - accuracy: 0.9217 - val_loss: 0.2789 - val_accuracy: 0.9210\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2811 - accuracy: 0.9218 - val_loss: 0.2788 - val_accuracy: 0.9216\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2808 - accuracy: 0.9220 - val_loss: 0.2785 - val_accuracy: 0.9221\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2806 - accuracy: 0.9220 - val_loss: 0.2784 - val_accuracy: 0.9217\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2803 - accuracy: 0.9221 - val_loss: 0.2782 - val_accuracy: 0.9217\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2800 - accuracy: 0.9220 - val_loss: 0.2781 - val_accuracy: 0.9219\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2798 - accuracy: 0.9222 - val_loss: 0.2780 - val_accuracy: 0.9221\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2796 - accuracy: 0.9221 - val_loss: 0.2778 - val_accuracy: 0.9223\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2793 - accuracy: 0.9224 - val_loss: 0.2774 - val_accuracy: 0.9216\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2791 - accuracy: 0.9225 - val_loss: 0.2777 - val_accuracy: 0.9219\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2789 - accuracy: 0.9225 - val_loss: 0.2774 - val_accuracy: 0.9221\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2786 - accuracy: 0.9225 - val_loss: 0.2773 - val_accuracy: 0.9224\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2784 - accuracy: 0.9226 - val_loss: 0.2771 - val_accuracy: 0.9225\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2781 - accuracy: 0.9225 - val_loss: 0.2769 - val_accuracy: 0.9225\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2779 - accuracy: 0.9229 - val_loss: 0.2769 - val_accuracy: 0.9225\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2777 - accuracy: 0.9229 - val_loss: 0.2765 - val_accuracy: 0.9224\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2775 - accuracy: 0.9227 - val_loss: 0.2764 - val_accuracy: 0.9221\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2773 - accuracy: 0.9229 - val_loss: 0.2764 - val_accuracy: 0.9222\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2771 - accuracy: 0.9230 - val_loss: 0.2761 - val_accuracy: 0.9225\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2769 - accuracy: 0.9229 - val_loss: 0.2763 - val_accuracy: 0.9227\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2767 - accuracy: 0.9230 - val_loss: 0.2760 - val_accuracy: 0.9221\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2765 - accuracy: 0.9232 - val_loss: 0.2763 - val_accuracy: 0.9221\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2763 - accuracy: 0.9232 - val_loss: 0.2757 - val_accuracy: 0.9225\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2761 - accuracy: 0.9230 - val_loss: 0.2756 - val_accuracy: 0.9223\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2759 - accuracy: 0.9233 - val_loss: 0.2756 - val_accuracy: 0.9224\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2757 - accuracy: 0.9234 - val_loss: 0.2754 - val_accuracy: 0.9221\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2755 - accuracy: 0.9232 - val_loss: 0.2756 - val_accuracy: 0.9223\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2753 - accuracy: 0.9232 - val_loss: 0.2755 - val_accuracy: 0.9225\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2751 - accuracy: 0.9234 - val_loss: 0.2753 - val_accuracy: 0.9225\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2749 - accuracy: 0.9237 - val_loss: 0.2751 - val_accuracy: 0.9220\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2747 - accuracy: 0.9236 - val_loss: 0.2750 - val_accuracy: 0.9221\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2745 - accuracy: 0.9238 - val_loss: 0.2754 - val_accuracy: 0.9223\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2744 - accuracy: 0.9239 - val_loss: 0.2749 - val_accuracy: 0.9220\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2741 - accuracy: 0.9239 - val_loss: 0.2747 - val_accuracy: 0.9222\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2740 - accuracy: 0.9236 - val_loss: 0.2746 - val_accuracy: 0.9224\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2738 - accuracy: 0.9241 - val_loss: 0.2747 - val_accuracy: 0.9226\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2736 - accuracy: 0.9240 - val_loss: 0.2744 - val_accuracy: 0.9224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e7fac1dd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLmQlid5C9Lc"
   },
   "source": [
    "**Task.** With the batch of 100, a single epoch executes roughly twice faster than with the default batch of 32. Why? What may be the other consequences of changing the batch size from 32 to 100?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHsObtF1P9V7"
   },
   "source": [
    "Although this is not absolutely necessary, evaluate the model on the training and validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I51pvoXxQCtJ",
    "outputId": "4a776e24-7c47-46cf-b7b3-950c40d6388a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2732 - accuracy: 0.9241\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2744 - accuracy: 0.9224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27443674206733704, 0.9223999977111816]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(DATA0, TARGET0)\n",
    "model.evaluate(DATA1, TARGET1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUhoGce-NWdP"
   },
   "source": [
    "**Task.** The validation accuracy displayed after the last training epoch is equal to the validation accuracy evaluated after completing the training. This is not the case for the accuracy on the training dataset. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls-nwb99M4-A"
   },
   "source": [
    "In the following, by the validation accuracy of a model, we will always mean the maximum value listed during training, even if it is not in the last epoch. This is because Tensorflow can stop training after any epoch and thus produce a model with validation accuracy corresponding to that epoch. Now display a summary of the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5pbSr7HNkzv",
    "outputId": "e9c9aa67-9506-4093-8cf9-fbefeca49d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-1lOAUvwWAo"
   },
   "source": [
    "**Task.** Why does the number of weights equal 7 850?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN12-s84N0uI"
   },
   "source": [
    "**Task.** In this example, the training and validation accuracies are very close to each other, which means that there is almost no overfitting. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZRU5wG-OU3a"
   },
   "source": [
    "In order to increase the overall accuracy, add a second layer. The number of neurons in that layer may be, for instance, the geometric mean of the number of inputs and outputs, that is, sqrt(10 * 784) = 89, or the next power of two, that is, 128. Use the classical sigmoid activation for this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFzq6fwtQaIi",
    "outputId": "1a5c2d38-ef87-41ff-8024-f37c2c93b7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.9791 - accuracy: 0.5615 - val_loss: 1.6498 - val_accuracy: 0.7549\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4040 - accuracy: 0.7703 - val_loss: 1.1588 - val_accuracy: 0.8092\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0275 - accuracy: 0.8112 - val_loss: 0.8809 - val_accuracy: 0.8315\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.8195 - accuracy: 0.8343 - val_loss: 0.7269 - val_accuracy: 0.8520\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6985 - accuracy: 0.8486 - val_loss: 0.6331 - val_accuracy: 0.8647\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.6208 - accuracy: 0.8586 - val_loss: 0.5694 - val_accuracy: 0.8691\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5667 - accuracy: 0.8659 - val_loss: 0.5239 - val_accuracy: 0.8783\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.5269 - accuracy: 0.8721 - val_loss: 0.4900 - val_accuracy: 0.8838\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4963 - accuracy: 0.8764 - val_loss: 0.4637 - val_accuracy: 0.8864\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4721 - accuracy: 0.8796 - val_loss: 0.4420 - val_accuracy: 0.8888\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4524 - accuracy: 0.8833 - val_loss: 0.4245 - val_accuracy: 0.8921\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4360 - accuracy: 0.8861 - val_loss: 0.4100 - val_accuracy: 0.8924\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4222 - accuracy: 0.8885 - val_loss: 0.3979 - val_accuracy: 0.8947\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4105 - accuracy: 0.8902 - val_loss: 0.3873 - val_accuracy: 0.8968\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4002 - accuracy: 0.8923 - val_loss: 0.3782 - val_accuracy: 0.8979\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3913 - accuracy: 0.8936 - val_loss: 0.3701 - val_accuracy: 0.8991\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3833 - accuracy: 0.8953 - val_loss: 0.3629 - val_accuracy: 0.9015\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3762 - accuracy: 0.8962 - val_loss: 0.3570 - val_accuracy: 0.9010\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3698 - accuracy: 0.8979 - val_loss: 0.3505 - val_accuracy: 0.9040\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3640 - accuracy: 0.8987 - val_loss: 0.3455 - val_accuracy: 0.9051\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3586 - accuracy: 0.9001 - val_loss: 0.3406 - val_accuracy: 0.9060\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3538 - accuracy: 0.9009 - val_loss: 0.3365 - val_accuracy: 0.9063\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3493 - accuracy: 0.9022 - val_loss: 0.3330 - val_accuracy: 0.9072\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3451 - accuracy: 0.9031 - val_loss: 0.3286 - val_accuracy: 0.9079\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3412 - accuracy: 0.9037 - val_loss: 0.3252 - val_accuracy: 0.9096\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3376 - accuracy: 0.9047 - val_loss: 0.3219 - val_accuracy: 0.9095\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3342 - accuracy: 0.9057 - val_loss: 0.3190 - val_accuracy: 0.9104\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3310 - accuracy: 0.9065 - val_loss: 0.3163 - val_accuracy: 0.9107\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3279 - accuracy: 0.9071 - val_loss: 0.3133 - val_accuracy: 0.9122\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3250 - accuracy: 0.9076 - val_loss: 0.3112 - val_accuracy: 0.9133\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3224 - accuracy: 0.9090 - val_loss: 0.3085 - val_accuracy: 0.9141\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3197 - accuracy: 0.9094 - val_loss: 0.3067 - val_accuracy: 0.9149\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3173 - accuracy: 0.9099 - val_loss: 0.3042 - val_accuracy: 0.9152\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3149 - accuracy: 0.9107 - val_loss: 0.3023 - val_accuracy: 0.9152\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3125 - accuracy: 0.9114 - val_loss: 0.3003 - val_accuracy: 0.9148\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3104 - accuracy: 0.9119 - val_loss: 0.2983 - val_accuracy: 0.9165\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3083 - accuracy: 0.9126 - val_loss: 0.2959 - val_accuracy: 0.9171\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3062 - accuracy: 0.9127 - val_loss: 0.2946 - val_accuracy: 0.9174\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3043 - accuracy: 0.9135 - val_loss: 0.2925 - val_accuracy: 0.9181\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3024 - accuracy: 0.9137 - val_loss: 0.2910 - val_accuracy: 0.9181\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3005 - accuracy: 0.9144 - val_loss: 0.2898 - val_accuracy: 0.9188\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2988 - accuracy: 0.9143 - val_loss: 0.2883 - val_accuracy: 0.9193\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2970 - accuracy: 0.9153 - val_loss: 0.2867 - val_accuracy: 0.9191\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2953 - accuracy: 0.9156 - val_loss: 0.2848 - val_accuracy: 0.9197\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2937 - accuracy: 0.9160 - val_loss: 0.2836 - val_accuracy: 0.9203\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2920 - accuracy: 0.9164 - val_loss: 0.2822 - val_accuracy: 0.9200\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2905 - accuracy: 0.9169 - val_loss: 0.2808 - val_accuracy: 0.9199\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2889 - accuracy: 0.9172 - val_loss: 0.2793 - val_accuracy: 0.9211\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2874 - accuracy: 0.9176 - val_loss: 0.2779 - val_accuracy: 0.9214\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2859 - accuracy: 0.9179 - val_loss: 0.2771 - val_accuracy: 0.9215\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2845 - accuracy: 0.9187 - val_loss: 0.2755 - val_accuracy: 0.9217\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2830 - accuracy: 0.9188 - val_loss: 0.2749 - val_accuracy: 0.9221\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2817 - accuracy: 0.9190 - val_loss: 0.2733 - val_accuracy: 0.9218\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2802 - accuracy: 0.9195 - val_loss: 0.2722 - val_accuracy: 0.9223\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2788 - accuracy: 0.9197 - val_loss: 0.2713 - val_accuracy: 0.9224\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2775 - accuracy: 0.9204 - val_loss: 0.2698 - val_accuracy: 0.9228\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2762 - accuracy: 0.9206 - val_loss: 0.2685 - val_accuracy: 0.9233\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2749 - accuracy: 0.9209 - val_loss: 0.2676 - val_accuracy: 0.9242\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2736 - accuracy: 0.9214 - val_loss: 0.2662 - val_accuracy: 0.9233\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2723 - accuracy: 0.9217 - val_loss: 0.2653 - val_accuracy: 0.9240\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2711 - accuracy: 0.9224 - val_loss: 0.2643 - val_accuracy: 0.9245\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2699 - accuracy: 0.9223 - val_loss: 0.2633 - val_accuracy: 0.9256\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2686 - accuracy: 0.9229 - val_loss: 0.2623 - val_accuracy: 0.9248\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2675 - accuracy: 0.9235 - val_loss: 0.2611 - val_accuracy: 0.9253\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2662 - accuracy: 0.9240 - val_loss: 0.2597 - val_accuracy: 0.9256\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2650 - accuracy: 0.9239 - val_loss: 0.2587 - val_accuracy: 0.9260\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2638 - accuracy: 0.9243 - val_loss: 0.2581 - val_accuracy: 0.9261\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2627 - accuracy: 0.9247 - val_loss: 0.2568 - val_accuracy: 0.9263\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2615 - accuracy: 0.9254 - val_loss: 0.2559 - val_accuracy: 0.9262\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2604 - accuracy: 0.9255 - val_loss: 0.2549 - val_accuracy: 0.9269\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2593 - accuracy: 0.9260 - val_loss: 0.2540 - val_accuracy: 0.9273\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2581 - accuracy: 0.9262 - val_loss: 0.2528 - val_accuracy: 0.9278\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2570 - accuracy: 0.9263 - val_loss: 0.2516 - val_accuracy: 0.9279\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2559 - accuracy: 0.9269 - val_loss: 0.2510 - val_accuracy: 0.9278\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2548 - accuracy: 0.9274 - val_loss: 0.2499 - val_accuracy: 0.9288\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2537 - accuracy: 0.9276 - val_loss: 0.2487 - val_accuracy: 0.9292\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2526 - accuracy: 0.9282 - val_loss: 0.2478 - val_accuracy: 0.9293\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2515 - accuracy: 0.9281 - val_loss: 0.2470 - val_accuracy: 0.9292\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2504 - accuracy: 0.9285 - val_loss: 0.2461 - val_accuracy: 0.9297\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2493 - accuracy: 0.9289 - val_loss: 0.2452 - val_accuracy: 0.9303\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2483 - accuracy: 0.9292 - val_loss: 0.2441 - val_accuracy: 0.9305\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2472 - accuracy: 0.9293 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2462 - accuracy: 0.9297 - val_loss: 0.2426 - val_accuracy: 0.9315\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2451 - accuracy: 0.9301 - val_loss: 0.2417 - val_accuracy: 0.9316\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2441 - accuracy: 0.9305 - val_loss: 0.2405 - val_accuracy: 0.9317\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2431 - accuracy: 0.9308 - val_loss: 0.2396 - val_accuracy: 0.9321\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2421 - accuracy: 0.9309 - val_loss: 0.2386 - val_accuracy: 0.9328\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2410 - accuracy: 0.9313 - val_loss: 0.2376 - val_accuracy: 0.9330\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2400 - accuracy: 0.9312 - val_loss: 0.2364 - val_accuracy: 0.9323\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2391 - accuracy: 0.9316 - val_loss: 0.2357 - val_accuracy: 0.9329\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2381 - accuracy: 0.9321 - val_loss: 0.2350 - val_accuracy: 0.9336\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2370 - accuracy: 0.9324 - val_loss: 0.2341 - val_accuracy: 0.9345\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2361 - accuracy: 0.9325 - val_loss: 0.2331 - val_accuracy: 0.9337\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2351 - accuracy: 0.9327 - val_loss: 0.2321 - val_accuracy: 0.9335\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2341 - accuracy: 0.9330 - val_loss: 0.2311 - val_accuracy: 0.9342\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2331 - accuracy: 0.9335 - val_loss: 0.2305 - val_accuracy: 0.9347\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2322 - accuracy: 0.9337 - val_loss: 0.2299 - val_accuracy: 0.9345\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2313 - accuracy: 0.9337 - val_loss: 0.2287 - val_accuracy: 0.9351\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2303 - accuracy: 0.9344 - val_loss: 0.2282 - val_accuracy: 0.9359\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2294 - accuracy: 0.9346 - val_loss: 0.2271 - val_accuracy: 0.9353\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (100, 128)                100480    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (100, 10)                 1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('sgd',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZrZ1JA5Ivtm"
   },
   "source": [
    "**Task.** Explain the number of weights in this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PibNHQJYRVpB"
   },
   "source": [
    "Adding the second layer improved the accuracy surprisingly little. Change the activation function of the added layer to relu, which is one of the most used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NXdUG_jR0lu",
    "outputId": "7518730f-083a-48a1-9d31-42034ae7aca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0572 - accuracy: 0.7561 - val_loss: 0.5544 - val_accuracy: 0.8716\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.4851 - accuracy: 0.8784 - val_loss: 0.4069 - val_accuracy: 0.8948\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3955 - accuracy: 0.8938 - val_loss: 0.3555 - val_accuracy: 0.9032\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3549 - accuracy: 0.9019 - val_loss: 0.3282 - val_accuracy: 0.9096\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3299 - accuracy: 0.9081 - val_loss: 0.3090 - val_accuracy: 0.9133\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3115 - accuracy: 0.9124 - val_loss: 0.2924 - val_accuracy: 0.9185\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2968 - accuracy: 0.9164 - val_loss: 0.2808 - val_accuracy: 0.9219\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2845 - accuracy: 0.9198 - val_loss: 0.2714 - val_accuracy: 0.9236\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2737 - accuracy: 0.9232 - val_loss: 0.2627 - val_accuracy: 0.9269\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2641 - accuracy: 0.9261 - val_loss: 0.2534 - val_accuracy: 0.9287\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2554 - accuracy: 0.9284 - val_loss: 0.2463 - val_accuracy: 0.9308\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2475 - accuracy: 0.9306 - val_loss: 0.2397 - val_accuracy: 0.9318\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2399 - accuracy: 0.9327 - val_loss: 0.2325 - val_accuracy: 0.9339\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2330 - accuracy: 0.9344 - val_loss: 0.2268 - val_accuracy: 0.9350\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2265 - accuracy: 0.9364 - val_loss: 0.2216 - val_accuracy: 0.9369\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2205 - accuracy: 0.9384 - val_loss: 0.2165 - val_accuracy: 0.9389\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2148 - accuracy: 0.9401 - val_loss: 0.2105 - val_accuracy: 0.9385\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2096 - accuracy: 0.9417 - val_loss: 0.2064 - val_accuracy: 0.9408\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.2047 - accuracy: 0.9430 - val_loss: 0.2012 - val_accuracy: 0.9413\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1999 - accuracy: 0.9446 - val_loss: 0.1977 - val_accuracy: 0.9428\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1955 - accuracy: 0.9456 - val_loss: 0.1936 - val_accuracy: 0.9431\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1913 - accuracy: 0.9469 - val_loss: 0.1894 - val_accuracy: 0.9444\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9480 - val_loss: 0.1860 - val_accuracy: 0.9459\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1833 - accuracy: 0.9492 - val_loss: 0.1831 - val_accuracy: 0.9463\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1797 - accuracy: 0.9502 - val_loss: 0.1801 - val_accuracy: 0.9476\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1761 - accuracy: 0.9507 - val_loss: 0.1762 - val_accuracy: 0.9493\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1727 - accuracy: 0.9522 - val_loss: 0.1739 - val_accuracy: 0.9497\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1695 - accuracy: 0.9528 - val_loss: 0.1708 - val_accuracy: 0.9502\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1664 - accuracy: 0.9537 - val_loss: 0.1675 - val_accuracy: 0.9512\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1632 - accuracy: 0.9548 - val_loss: 0.1651 - val_accuracy: 0.9521\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1604 - accuracy: 0.9553 - val_loss: 0.1625 - val_accuracy: 0.9523\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1576 - accuracy: 0.9563 - val_loss: 0.1611 - val_accuracy: 0.9520\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1551 - accuracy: 0.9569 - val_loss: 0.1576 - val_accuracy: 0.9541\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1523 - accuracy: 0.9576 - val_loss: 0.1557 - val_accuracy: 0.9544\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1499 - accuracy: 0.9586 - val_loss: 0.1530 - val_accuracy: 0.9546\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1474 - accuracy: 0.9594 - val_loss: 0.1514 - val_accuracy: 0.9560\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1451 - accuracy: 0.9599 - val_loss: 0.1494 - val_accuracy: 0.9562\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1428 - accuracy: 0.9606 - val_loss: 0.1469 - val_accuracy: 0.9570\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1406 - accuracy: 0.9612 - val_loss: 0.1454 - val_accuracy: 0.9575\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1385 - accuracy: 0.9619 - val_loss: 0.1439 - val_accuracy: 0.9572\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1364 - accuracy: 0.9625 - val_loss: 0.1412 - val_accuracy: 0.9581\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1344 - accuracy: 0.9631 - val_loss: 0.1398 - val_accuracy: 0.9586\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1324 - accuracy: 0.9636 - val_loss: 0.1379 - val_accuracy: 0.9586\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1305 - accuracy: 0.9643 - val_loss: 0.1366 - val_accuracy: 0.9593\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1287 - accuracy: 0.9648 - val_loss: 0.1351 - val_accuracy: 0.9589\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1269 - accuracy: 0.9651 - val_loss: 0.1337 - val_accuracy: 0.9594\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1251 - accuracy: 0.9657 - val_loss: 0.1319 - val_accuracy: 0.9605\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1233 - accuracy: 0.9660 - val_loss: 0.1310 - val_accuracy: 0.9611\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1217 - accuracy: 0.9668 - val_loss: 0.1300 - val_accuracy: 0.9618\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1201 - accuracy: 0.9671 - val_loss: 0.1286 - val_accuracy: 0.9611\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1185 - accuracy: 0.9679 - val_loss: 0.1268 - val_accuracy: 0.9624\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1169 - accuracy: 0.9681 - val_loss: 0.1255 - val_accuracy: 0.9623\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1155 - accuracy: 0.9688 - val_loss: 0.1248 - val_accuracy: 0.9629\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1140 - accuracy: 0.9689 - val_loss: 0.1243 - val_accuracy: 0.9632\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9692 - val_loss: 0.1226 - val_accuracy: 0.9635\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1112 - accuracy: 0.9697 - val_loss: 0.1215 - val_accuracy: 0.9637\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1099 - accuracy: 0.9695 - val_loss: 0.1198 - val_accuracy: 0.9645\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1085 - accuracy: 0.9702 - val_loss: 0.1194 - val_accuracy: 0.9652\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1073 - accuracy: 0.9706 - val_loss: 0.1183 - val_accuracy: 0.9654\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1059 - accuracy: 0.9708 - val_loss: 0.1173 - val_accuracy: 0.9655\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1048 - accuracy: 0.9714 - val_loss: 0.1167 - val_accuracy: 0.9662\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1036 - accuracy: 0.9718 - val_loss: 0.1147 - val_accuracy: 0.9664\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1024 - accuracy: 0.9720 - val_loss: 0.1146 - val_accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1013 - accuracy: 0.9722 - val_loss: 0.1135 - val_accuracy: 0.9666\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1001 - accuracy: 0.9729 - val_loss: 0.1125 - val_accuracy: 0.9671\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0990 - accuracy: 0.9730 - val_loss: 0.1115 - val_accuracy: 0.9670\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9733 - val_loss: 0.1103 - val_accuracy: 0.9679\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0968 - accuracy: 0.9737 - val_loss: 0.1098 - val_accuracy: 0.9681\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9741 - val_loss: 0.1091 - val_accuracy: 0.9684\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0948 - accuracy: 0.9745 - val_loss: 0.1084 - val_accuracy: 0.9681\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9745 - val_loss: 0.1076 - val_accuracy: 0.9680\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0928 - accuracy: 0.9749 - val_loss: 0.1065 - val_accuracy: 0.9691\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0919 - accuracy: 0.9749 - val_loss: 0.1064 - val_accuracy: 0.9684\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0909 - accuracy: 0.9750 - val_loss: 0.1053 - val_accuracy: 0.9692\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9755 - val_loss: 0.1050 - val_accuracy: 0.9691\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0890 - accuracy: 0.9758 - val_loss: 0.1050 - val_accuracy: 0.9684\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9758 - val_loss: 0.1036 - val_accuracy: 0.9693\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0873 - accuracy: 0.9762 - val_loss: 0.1030 - val_accuracy: 0.9695\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0864 - accuracy: 0.9764 - val_loss: 0.1022 - val_accuracy: 0.9702\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0856 - accuracy: 0.9767 - val_loss: 0.1017 - val_accuracy: 0.9703\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9768 - val_loss: 0.1012 - val_accuracy: 0.9700\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0840 - accuracy: 0.9771 - val_loss: 0.1005 - val_accuracy: 0.9703\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0832 - accuracy: 0.9772 - val_loss: 0.0997 - val_accuracy: 0.9713\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9701\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9715\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9779 - val_loss: 0.0979 - val_accuracy: 0.9715\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0801 - accuracy: 0.9781 - val_loss: 0.0977 - val_accuracy: 0.9711\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9783 - val_loss: 0.0972 - val_accuracy: 0.9712\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0787 - accuracy: 0.9786 - val_loss: 0.0969 - val_accuracy: 0.9712\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9790 - val_loss: 0.0961 - val_accuracy: 0.9712\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9789 - val_loss: 0.0958 - val_accuracy: 0.9713\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9793 - val_loss: 0.0950 - val_accuracy: 0.9720\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0759 - accuracy: 0.9796 - val_loss: 0.0950 - val_accuracy: 0.9719\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0752 - accuracy: 0.9796 - val_loss: 0.0940 - val_accuracy: 0.9720\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0745 - accuracy: 0.9799 - val_loss: 0.0939 - val_accuracy: 0.9722\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0739 - accuracy: 0.9799 - val_loss: 0.0933 - val_accuracy: 0.9724\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0732 - accuracy: 0.9799 - val_loss: 0.0929 - val_accuracy: 0.9728\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0726 - accuracy: 0.9802 - val_loss: 0.0925 - val_accuracy: 0.9723\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9804 - val_loss: 0.0919 - val_accuracy: 0.9724\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9807 - val_loss: 0.0914 - val_accuracy: 0.9728\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (100, 128)                100480    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (100, 10)                 1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('sgd',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RH6_mxR8XSco"
   },
   "source": [
    "Now the improvement is very significant. The difference is because sigmoid saturates on both sides, yields vanishing gradients there, and effectively stucks the minimization. Relu does not saturate on the positive side and causes no such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xoKXgKAOp02"
   },
   "source": [
    "**Task.** The training takes less physical time with the relu activation. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO6CebTyQ6si"
   },
   "source": [
    "Now switch the optimizer to Adam, which is one of the best and most used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUB3kB-vdLda",
    "outputId": "4781f8af-6eab-42b4-b9ab-9264a0b3de99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.3331 - accuracy: 0.9088 - val_loss: 0.1761 - val_accuracy: 0.9493\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1514 - accuracy: 0.9565 - val_loss: 0.1310 - val_accuracy: 0.9613\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.1086 - accuracy: 0.9690 - val_loss: 0.1084 - val_accuracy: 0.9670\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0846 - accuracy: 0.9752 - val_loss: 0.0904 - val_accuracy: 0.9727\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0683 - accuracy: 0.9802 - val_loss: 0.0836 - val_accuracy: 0.9744\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0555 - accuracy: 0.9837 - val_loss: 0.0804 - val_accuracy: 0.9754\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0461 - accuracy: 0.9864 - val_loss: 0.0725 - val_accuracy: 0.9779\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0386 - accuracy: 0.9885 - val_loss: 0.0808 - val_accuracy: 0.9747\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0314 - accuracy: 0.9909 - val_loss: 0.0734 - val_accuracy: 0.9771\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0274 - accuracy: 0.9926 - val_loss: 0.0710 - val_accuracy: 0.9787\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0717 - val_accuracy: 0.9779\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0188 - accuracy: 0.9953 - val_loss: 0.0755 - val_accuracy: 0.9782\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0164 - accuracy: 0.9959 - val_loss: 0.0732 - val_accuracy: 0.9795\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 0.0810 - val_accuracy: 0.9779\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.0754 - val_accuracy: 0.9786\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0101 - accuracy: 0.9976 - val_loss: 0.0802 - val_accuracy: 0.9785\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 0.0834 - val_accuracy: 0.9780\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0071 - accuracy: 0.9986 - val_loss: 0.0972 - val_accuracy: 0.9746\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.0827 - val_accuracy: 0.9773\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0808 - val_accuracy: 0.9789\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0986 - val_accuracy: 0.9763\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0878 - val_accuracy: 0.9792\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0862 - val_accuracy: 0.9799\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0938 - val_accuracy: 0.9772\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0969 - val_accuracy: 0.9792\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0896 - val_accuracy: 0.9784\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0919 - val_accuracy: 0.9783\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.1289 - val_accuracy: 0.9722\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.0910 - val_accuracy: 0.9794\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0900 - val_accuracy: 0.9797\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.6699e-04 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9806\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.4367e-04 - accuracy: 1.0000 - val_loss: 0.0923 - val_accuracy: 0.9804\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.1227 - val_accuracy: 0.9755\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.1021 - val_accuracy: 0.9797\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 9.8227e-04 - accuracy: 0.9999 - val_loss: 0.1037 - val_accuracy: 0.9785\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.3977e-04 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9803\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.0143e-04 - accuracy: 1.0000 - val_loss: 0.0979 - val_accuracy: 0.9813\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.5991e-04 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9810\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.1635 - val_accuracy: 0.9696\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.1243 - val_accuracy: 0.9762\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.1064 - val_accuracy: 0.9798\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.4051e-04 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 0.9799\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.3424e-04 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9800\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.9063e-04 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9800\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.7031e-04 - accuracy: 1.0000 - val_loss: 0.1045 - val_accuracy: 0.9800\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4814e-04 - accuracy: 1.0000 - val_loss: 0.1063 - val_accuracy: 0.9802\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.5207e-04 - accuracy: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.9795\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.1264 - val_accuracy: 0.9760\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.1118 - val_accuracy: 0.9806\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.4504e-04 - accuracy: 0.9999 - val_loss: 0.1107 - val_accuracy: 0.9809\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.4044e-04 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9803\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4004e-04 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9804\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1854e-04 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9806\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0394e-04 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9808\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 9.2430e-05 - accuracy: 1.0000 - val_loss: 0.1128 - val_accuracy: 0.9807\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 8.1092e-05 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9801\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 7.4843e-05 - accuracy: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.9801\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0839e-04 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 0.9795\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.1257 - val_accuracy: 0.9782\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.1246 - val_accuracy: 0.9792\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.5357e-04 - accuracy: 1.0000 - val_loss: 0.1191 - val_accuracy: 0.9805\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.1180e-04 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9808\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 8.6447e-05 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.9807\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 7.3829e-05 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.9808\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 6.5151e-05 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 0.9804\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.7153e-05 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9805\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.0801e-05 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9807\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.3984e-05 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9804\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.9210e-05 - accuracy: 1.0000 - val_loss: 0.1229 - val_accuracy: 0.9807\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.0917e-05 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9802\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.1406 - val_accuracy: 0.9769\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.1362 - val_accuracy: 0.9785\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.8964e-04 - accuracy: 0.9999 - val_loss: 0.1308 - val_accuracy: 0.9800\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.1054e-04 - accuracy: 0.9999 - val_loss: 0.1290 - val_accuracy: 0.9803\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 7.0196e-05 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9798\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.1629e-05 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9802\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.3714e-05 - accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9802\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.8068e-05 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9803\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.3175e-05 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9802\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.9212e-05 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9803\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.5702e-05 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9806\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2281e-05 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9802\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.9542e-05 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9804\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.7305e-05 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9801\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0412e-04 - accuracy: 1.0000 - val_loss: 0.1626 - val_accuracy: 0.9753\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.1570 - val_accuracy: 0.9773\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.5828e-04 - accuracy: 0.9998 - val_loss: 0.1532 - val_accuracy: 0.9775\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.3020e-04 - accuracy: 1.0000 - val_loss: 0.1462 - val_accuracy: 0.9784\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 5.4850e-05 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.9785\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 4.1543e-05 - accuracy: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.9790\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 3.5224e-05 - accuracy: 1.0000 - val_loss: 0.1443 - val_accuracy: 0.9793\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.9910e-05 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 0.9793\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.5708e-05 - accuracy: 1.0000 - val_loss: 0.1453 - val_accuracy: 0.9796\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 2.2376e-05 - accuracy: 1.0000 - val_loss: 0.1449 - val_accuracy: 0.9794\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.9220e-05 - accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 0.9793\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.6802e-05 - accuracy: 1.0000 - val_loss: 0.1462 - val_accuracy: 0.9796\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.4330e-05 - accuracy: 1.0000 - val_loss: 0.1460 - val_accuracy: 0.9795\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.2762e-05 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9797\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 1.0966e-05 - accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9792\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 9.8227e-06 - accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 0.9784\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (100, 128)                100480    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (100, 10)                 1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('adam',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJB1CEClbNVM"
   },
   "source": [
    "The final validation accuracy is only slightly higher but Adam achieved it in a much smaller number of epochs than SGD. From now on, we will always use the Adam optimizer and the relu activation function. It is impressive that such a simple two-layer network correctly classifies 100% of the training dataset. This means that the lower accuracy on the validation dataset is uniquely due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IKKOhQxVdb3"
   },
   "source": [
    "**Task.** Why is there a significant overfitting here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3luXuOdSZ3q"
   },
   "source": [
    "**Task.** Since there is overfitting caused by the already large number of model weights, it is dubious whether adding a third layer would still improve things, because it will increase the number of weights even more. Check this by adding a third layer. Adjust the layer sizes, the batch size, and the number of epochs to obtain a maximum validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd_I4gqdXR-F"
   },
   "source": [
    "Adding the third layer improved the validation accuracy only slightly. But this is still an achievement because the closer we are to 100%, the more difficult it is to gain accuracy. Note that even a slight increase in accuracy leads to a significant reduction in the error rate, which is complementary to accuracy. Here for instance, increasing the accuracy from 0.980 to 0.985 reduces the error rate from 2% to 1.5%, that is, by a factor of 4/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNTzk7lVZTbh"
   },
   "source": [
    "**Task.** Check that adding a fourth dense layer does not further increase the validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krJXXCN2dX6o"
   },
   "source": [
    "Since adding more dense layers does not further increase the validation accuracy, we must resort to other means. There are dedicated techniques to reduce overfitting, but it seems more interesting to introduce convolutional layers. First, load the data once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsDGyBvpd6bQ"
   },
   "outputs": [],
   "source": [
    "(DATA0, TARGET0), (DATA1, TARGET1) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLchy6nYecYj"
   },
   "source": [
    "Convolutional layers require the data to be four-dimensional, with the subsequent dimensions being the batch size, the image height, the image width, and the number of channels. The number of channels equals three for RGB images and one for grayscale images. We deal with grayscale images but still the fourth dimension needs to be added with the size of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZk8Ozh3fb1w"
   },
   "outputs": [],
   "source": [
    "DATA0 = DATA0[:, :, :, None].astype('float32') / 255.\n",
    "DATA1 = DATA1[:, :, :, None].astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-quzqVAAhQ1q"
   },
   "source": [
    "Construct a convolutional layer, pass some data through it, convert the output to a numpy array, and print its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFnkW125hQH7",
    "outputId": "2f097906-451a-4703-b50c-594a445a6b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 24, 24, 8)\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.Conv2D(8, 5, activation = 'relu')\n",
    "\n",
    "OUTPUT1 = layer(DATA1).numpy()\n",
    "print(OUTPUT1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zd28wIYjCjW"
   },
   "source": [
    "This sample convolutional layer has 8 output channels, which means that it produces an output with 8 channels. It also has a filter of size 5, which means that every output pixel is a linear combination of input pixels in a 5x5 square. Consequenntly, the layer reduces the image size by 4 in each dimension, from 28 to 24 in this case. In order to perform image classification, the output from the convolutional layer must be flattened and passed to a dense layer with softmax activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1AyQ4vTlIQ6",
    "outputId": "ffb61ddc-4ff7-449e-b621-e950fbc1e836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 5s 4ms/step - loss: 0.3291 - accuracy: 0.9089 - val_loss: 0.1681 - val_accuracy: 0.9518\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.1260 - accuracy: 0.9654 - val_loss: 0.0887 - val_accuracy: 0.9726\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0801 - accuracy: 0.9775 - val_loss: 0.0669 - val_accuracy: 0.9796\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0616 - accuracy: 0.9820 - val_loss: 0.0642 - val_accuracy: 0.9796\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0516 - accuracy: 0.9851 - val_loss: 0.0573 - val_accuracy: 0.9826\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0451 - accuracy: 0.9864 - val_loss: 0.0586 - val_accuracy: 0.9822\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0385 - accuracy: 0.9886 - val_loss: 0.0587 - val_accuracy: 0.9827\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0341 - accuracy: 0.9898 - val_loss: 0.0572 - val_accuracy: 0.9816\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.0631 - val_accuracy: 0.9810\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0265 - accuracy: 0.9922 - val_loss: 0.0649 - val_accuracy: 0.9815\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.0647 - val_accuracy: 0.9815\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 0.0593 - val_accuracy: 0.9831\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.0617 - val_accuracy: 0.9832\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0169 - accuracy: 0.9950 - val_loss: 0.0653 - val_accuracy: 0.9829\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0148 - accuracy: 0.9960 - val_loss: 0.0707 - val_accuracy: 0.9828\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.0735 - val_accuracy: 0.9806\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.0695 - val_accuracy: 0.9826\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0785 - val_accuracy: 0.9811\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0093 - accuracy: 0.9977 - val_loss: 0.0740 - val_accuracy: 0.9834\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.0768 - val_accuracy: 0.9819\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0743 - val_accuracy: 0.9827\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0785 - val_accuracy: 0.9823\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0847 - val_accuracy: 0.9813\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0861 - val_accuracy: 0.9818\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0867 - val_accuracy: 0.9829\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.0879 - val_accuracy: 0.9822\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.0884 - val_accuracy: 0.9821\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0892 - val_accuracy: 0.9830\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0945 - val_accuracy: 0.9827\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0975 - val_accuracy: 0.9812\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0974 - val_accuracy: 0.9818\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0981 - val_accuracy: 0.9815\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0963 - val_accuracy: 0.9825\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.1035 - val_accuracy: 0.9827\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.1006 - val_accuracy: 0.9827\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.1070 - val_accuracy: 0.9812\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.1164 - val_accuracy: 0.9803\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.1037 - val_accuracy: 0.9829\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1095 - val_accuracy: 0.9824\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.1144 - val_accuracy: 0.9821\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.1070 - val_accuracy: 0.9833\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.1092 - val_accuracy: 0.9828\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 7.2065e-04 - accuracy: 0.9999 - val_loss: 0.1082 - val_accuracy: 0.9826\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 4.6221e-04 - accuracy: 1.0000 - val_loss: 0.1100 - val_accuracy: 0.9827\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 2.7050e-04 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9830\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.0499e-04 - accuracy: 1.0000 - val_loss: 0.1103 - val_accuracy: 0.9835\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.1207 - val_accuracy: 0.9800\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.1149 - val_accuracy: 0.9814\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 8.9575e-04 - accuracy: 0.9998 - val_loss: 0.1136 - val_accuracy: 0.9818\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.7228e-04 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 0.9825\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.7133e-04 - accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9825\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.4281e-04 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9825\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 1.1979e-04 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 0.9819\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.0169e-04 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9824\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.0416e-04 - accuracy: 1.0000 - val_loss: 0.1237 - val_accuracy: 0.9824\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.1250 - val_accuracy: 0.9803\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.1131 - val_accuracy: 0.9840\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 2.5497e-04 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 0.9835\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.5048e-04 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9836\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.0769e-04 - accuracy: 1.0000 - val_loss: 0.1170 - val_accuracy: 0.9842\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 9.1940e-05 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9834\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 8.0712e-05 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9840\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 7.3874e-05 - accuracy: 1.0000 - val_loss: 0.1207 - val_accuracy: 0.9836\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 6.2632e-05 - accuracy: 1.0000 - val_loss: 0.1228 - val_accuracy: 0.9833\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.1290 - val_accuracy: 0.9822\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.1254 - val_accuracy: 0.9824\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.5138e-04 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.9826\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 1.1326e-04 - accuracy: 1.0000 - val_loss: 0.1247 - val_accuracy: 0.9826\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 8.7512e-05 - accuracy: 1.0000 - val_loss: 0.1251 - val_accuracy: 0.9825\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 7.4601e-05 - accuracy: 1.0000 - val_loss: 0.1259 - val_accuracy: 0.9828\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 6.3362e-05 - accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9831\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 5.8874e-05 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9831\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 5.1033e-05 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9829\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.1327 - val_accuracy: 0.9818\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1290 - val_accuracy: 0.9834\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 2.9731e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9832\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.0208e-04 - accuracy: 1.0000 - val_loss: 0.1281 - val_accuracy: 0.9837\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 6.8661e-05 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9840\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 5.6327e-05 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9838\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 4.9531e-05 - accuracy: 1.0000 - val_loss: 0.1324 - val_accuracy: 0.9841\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 4.2638e-05 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9840\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.7269e-05 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9836\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.3885e-05 - accuracy: 1.0000 - val_loss: 0.1362 - val_accuracy: 0.9835\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 7.3709e-04 - accuracy: 0.9998 - val_loss: 0.1800 - val_accuracy: 0.9783\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1401 - val_accuracy: 0.9827\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 5.0085e-04 - accuracy: 0.9999 - val_loss: 0.1394 - val_accuracy: 0.9818\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.1425e-04 - accuracy: 1.0000 - val_loss: 0.1365 - val_accuracy: 0.9826\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 5.8675e-05 - accuracy: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.9827\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 4.6470e-05 - accuracy: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.9829\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.9743e-05 - accuracy: 1.0000 - val_loss: 0.1390 - val_accuracy: 0.9826\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.4473e-05 - accuracy: 1.0000 - val_loss: 0.1394 - val_accuracy: 0.9829\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.0279e-05 - accuracy: 1.0000 - val_loss: 0.1398 - val_accuracy: 0.9828\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 2.6107e-05 - accuracy: 1.0000 - val_loss: 0.1424 - val_accuracy: 0.9827\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 2.3931e-05 - accuracy: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.9833\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 2.0885e-05 - accuracy: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.9834\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.8938e-05 - accuracy: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.9829\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.1743 - val_accuracy: 0.9786\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.1412 - val_accuracy: 0.9821\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 3.9875e-04 - accuracy: 0.9998 - val_loss: 0.1415 - val_accuracy: 0.9831\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 8.8632e-05 - accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.9836\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (100, 24, 24, 8)          208       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (100, 4608)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 10)                 46090     \n",
      "=================================================================\n",
      "Total params: 46,298\n",
      "Trainable params: 46,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, 5, activation = 'relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('adam',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkwPiC7dy4f2"
   },
   "source": [
    "**Task.** Why does the covolutional layer have 208 weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m740T4u1z-C6"
   },
   "source": [
    "**Task.** Why does the total number of weights equal 46 298?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJxOnkUEowSc"
   },
   "source": [
    "This two-layer model with 46 298 weights is almost as good the three-layer dense model with 218 058 weights! This is partly because convolutional layers have very little weights, so they are less prone to overfitting than dense layers. Convolutional networks frequently use pooling layers that reduce the spatial image size twice in each direction by retaining only the pixel with maximum activation from each 2x2 square. Pooling acts independently in each channel. Add pooling after the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFm3ng9Ds3NE",
    "outputId": "aaa3e12c-fca9-4987-ec6c-351d341cc14a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.3959 - accuracy: 0.8923 - val_loss: 0.1729 - val_accuracy: 0.9520\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.1500 - accuracy: 0.9573 - val_loss: 0.1149 - val_accuracy: 0.9670\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.1097 - accuracy: 0.9686 - val_loss: 0.0889 - val_accuracy: 0.9738\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0885 - accuracy: 0.9747 - val_loss: 0.0766 - val_accuracy: 0.9764\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0753 - accuracy: 0.9785 - val_loss: 0.0716 - val_accuracy: 0.9779\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0662 - accuracy: 0.9806 - val_loss: 0.0636 - val_accuracy: 0.9783\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0596 - accuracy: 0.9819 - val_loss: 0.0595 - val_accuracy: 0.9804\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0530 - accuracy: 0.9842 - val_loss: 0.0559 - val_accuracy: 0.9813\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0492 - accuracy: 0.9853 - val_loss: 0.0560 - val_accuracy: 0.9814\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0458 - accuracy: 0.9863 - val_loss: 0.0540 - val_accuracy: 0.9821\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0425 - accuracy: 0.9870 - val_loss: 0.0533 - val_accuracy: 0.9825\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0392 - accuracy: 0.9885 - val_loss: 0.0512 - val_accuracy: 0.9831\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0375 - accuracy: 0.9887 - val_loss: 0.0519 - val_accuracy: 0.9822\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.0521 - val_accuracy: 0.9830\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 0.0534 - val_accuracy: 0.9831\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 0.0501 - val_accuracy: 0.9834\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.0540 - val_accuracy: 0.9840\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0281 - accuracy: 0.9914 - val_loss: 0.0519 - val_accuracy: 0.9835\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0267 - accuracy: 0.9919 - val_loss: 0.0518 - val_accuracy: 0.9840\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0247 - accuracy: 0.9925 - val_loss: 0.0534 - val_accuracy: 0.9847\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.0579 - val_accuracy: 0.9821\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0218 - accuracy: 0.9930 - val_loss: 0.0515 - val_accuracy: 0.9843\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9938 - val_loss: 0.0581 - val_accuracy: 0.9826\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0199 - accuracy: 0.9941 - val_loss: 0.0603 - val_accuracy: 0.9823\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.0569 - val_accuracy: 0.9835\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0178 - accuracy: 0.9951 - val_loss: 0.0577 - val_accuracy: 0.9836\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0559 - val_accuracy: 0.9832\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0164 - accuracy: 0.9956 - val_loss: 0.0574 - val_accuracy: 0.9833\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.0568 - val_accuracy: 0.9835\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.0630 - val_accuracy: 0.9820\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.0605 - val_accuracy: 0.9835\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0595 - val_accuracy: 0.9835\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 0.0599 - val_accuracy: 0.9839\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.0618 - val_accuracy: 0.9837\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0675 - val_accuracy: 0.9819\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0648 - val_accuracy: 0.9835\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.0657 - val_accuracy: 0.9833\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0103 - accuracy: 0.9972 - val_loss: 0.0656 - val_accuracy: 0.9836\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.0700 - val_accuracy: 0.9822\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.0666 - val_accuracy: 0.9834\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.0657 - val_accuracy: 0.9832\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.0697 - val_accuracy: 0.9832\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0084 - accuracy: 0.9979 - val_loss: 0.0686 - val_accuracy: 0.9835\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0740 - val_accuracy: 0.9812\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0713 - val_accuracy: 0.9825\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0713 - val_accuracy: 0.9826\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0714 - val_accuracy: 0.9837\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0796 - val_accuracy: 0.9816\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0807 - val_accuracy: 0.9811\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0748 - val_accuracy: 0.9825\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.0775 - val_accuracy: 0.9826\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0787 - val_accuracy: 0.9829\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.0821 - val_accuracy: 0.9825\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0812 - val_accuracy: 0.9822\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0832 - val_accuracy: 0.9814\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0829 - val_accuracy: 0.9822\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.0918 - val_accuracy: 0.9807\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0805 - val_accuracy: 0.9836\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0877 - val_accuracy: 0.9820\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0901 - val_accuracy: 0.9809\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0875 - val_accuracy: 0.9831\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0870 - val_accuracy: 0.9818\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0904 - val_accuracy: 0.9815\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0893 - val_accuracy: 0.9819\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0882 - val_accuracy: 0.9832\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0948 - val_accuracy: 0.9812\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0968 - val_accuracy: 0.9817\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.1020 - val_accuracy: 0.9817\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0948 - val_accuracy: 0.9808\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0957 - val_accuracy: 0.9815\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.1015 - val_accuracy: 0.9812\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.1028 - val_accuracy: 0.9815\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0983 - val_accuracy: 0.9827\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.1046 - val_accuracy: 0.9809\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.1010 - val_accuracy: 0.9810\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1015 - val_accuracy: 0.9823\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.1056 - val_accuracy: 0.9812\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.1059 - val_accuracy: 0.9811\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.1078 - val_accuracy: 0.9810\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1036 - val_accuracy: 0.9811\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.1053 - val_accuracy: 0.9815\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1134 - val_accuracy: 0.9798\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.1147 - val_accuracy: 0.9819\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.1111 - val_accuracy: 0.9810\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.1036 - val_accuracy: 0.9827\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 9.2214e-04 - accuracy: 0.9999 - val_loss: 0.1031 - val_accuracy: 0.9821\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.1193 - val_accuracy: 0.9809\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.1147 - val_accuracy: 0.9809\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.1097 - val_accuracy: 0.9818\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 8.0979e-04 - accuracy: 0.9999 - val_loss: 0.1088 - val_accuracy: 0.9815\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 6.2881e-04 - accuracy: 1.0000 - val_loss: 0.1057 - val_accuracy: 0.9818\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1303 - val_accuracy: 0.9798\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.1111 - val_accuracy: 0.9819\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.1128 - val_accuracy: 0.9812\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 6.9514e-04 - accuracy: 1.0000 - val_loss: 0.1123 - val_accuracy: 0.9817\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 5.1870e-04 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9814\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 9.9216e-04 - accuracy: 0.9998 - val_loss: 0.1301 - val_accuracy: 0.9796\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.1217 - val_accuracy: 0.9807\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (100, 24, 24, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (100, 12, 12, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 1152)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (100, 10)                 11530     \n",
      "=================================================================\n",
      "Total params: 11,738\n",
      "Trainable params: 11,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, 5, activation = 'relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('adam',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBiXolyC3WUp"
   },
   "source": [
    "**Task.** Examine the total number of weights in this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NokxCnEkvpzg"
   },
   "source": [
    "Pooling inevitably looses some information, so the current network has worse validation accuracy, but only a bit worse. Convolutional networks are often built of pairs of convolutional and pooling layers. Add another such pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blh3oaDOA5vD",
    "outputId": "f00ba8bf-5492-4652-f734-455905d53788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 4s 5ms/step - loss: 0.3673 - accuracy: 0.8940 - val_loss: 0.1421 - val_accuracy: 0.9596\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.1247 - accuracy: 0.9629 - val_loss: 0.0912 - val_accuracy: 0.9736\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0924 - accuracy: 0.9723 - val_loss: 0.0725 - val_accuracy: 0.9773\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0745 - accuracy: 0.9770 - val_loss: 0.0591 - val_accuracy: 0.9811\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.0488 - val_accuracy: 0.9849\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9849\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 0.0504 - val_accuracy: 0.9829\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0452 - accuracy: 0.9863 - val_loss: 0.0431 - val_accuracy: 0.9857\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0425 - accuracy: 0.9871 - val_loss: 0.0409 - val_accuracy: 0.9870\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0385 - accuracy: 0.9882 - val_loss: 0.0362 - val_accuracy: 0.9884\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.0377 - val_accuracy: 0.9879\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0336 - accuracy: 0.9895 - val_loss: 0.0348 - val_accuracy: 0.9888\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0320 - accuracy: 0.9901 - val_loss: 0.0379 - val_accuracy: 0.9884\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0296 - accuracy: 0.9909 - val_loss: 0.0376 - val_accuracy: 0.9880\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0278 - accuracy: 0.9917 - val_loss: 0.0343 - val_accuracy: 0.9885\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.0355 - val_accuracy: 0.9889\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.0374 - val_accuracy: 0.9877\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0237 - accuracy: 0.9926 - val_loss: 0.0367 - val_accuracy: 0.9890\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.0388 - val_accuracy: 0.9885\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.0356 - val_accuracy: 0.9888\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.0372 - val_accuracy: 0.9888\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.0401 - val_accuracy: 0.9884\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0183 - accuracy: 0.9941 - val_loss: 0.0362 - val_accuracy: 0.9879\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0367 - val_accuracy: 0.9894\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0355 - val_accuracy: 0.9900\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.0379 - val_accuracy: 0.9896\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.0390 - val_accuracy: 0.9901\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.0375 - val_accuracy: 0.9894\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.0391 - val_accuracy: 0.9904\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0383 - val_accuracy: 0.9899\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0383 - val_accuracy: 0.9896\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.0486 - val_accuracy: 0.9866\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 0.0377 - val_accuracy: 0.9901\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0419 - val_accuracy: 0.9897\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0412 - val_accuracy: 0.9894\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0416 - val_accuracy: 0.9899\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0444 - val_accuracy: 0.9895\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0519 - val_accuracy: 0.9892\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0495 - val_accuracy: 0.9879\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.0449 - val_accuracy: 0.9899\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0471 - val_accuracy: 0.9890\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0495 - val_accuracy: 0.9884\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0513 - val_accuracy: 0.9896\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0506 - val_accuracy: 0.9894\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0511 - val_accuracy: 0.9896\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0511 - val_accuracy: 0.9889\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0581 - val_accuracy: 0.9889\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 0.0591 - val_accuracy: 0.9894\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0517 - val_accuracy: 0.9896\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.0509 - val_accuracy: 0.9903\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0525 - val_accuracy: 0.9885\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0605 - val_accuracy: 0.9884\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0711 - val_accuracy: 0.9872\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0560 - val_accuracy: 0.9882\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0061 - accuracy: 0.9978 - val_loss: 0.0592 - val_accuracy: 0.9886\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0598 - val_accuracy: 0.9898\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0577 - val_accuracy: 0.9883\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0589 - val_accuracy: 0.9885\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0584 - val_accuracy: 0.9894\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0776 - val_accuracy: 0.9858\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0609 - val_accuracy: 0.9879\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0647 - val_accuracy: 0.9879\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0617 - val_accuracy: 0.9885\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0713 - val_accuracy: 0.9882\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0656 - val_accuracy: 0.9880\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0672 - val_accuracy: 0.9894\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0606 - val_accuracy: 0.9897\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0649 - val_accuracy: 0.9897\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0642 - val_accuracy: 0.9894\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0770 - val_accuracy: 0.9880\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0768 - val_accuracy: 0.9885\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0690 - val_accuracy: 0.9883\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0654 - val_accuracy: 0.9892\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 7.8686e-04 - accuracy: 0.9999 - val_loss: 0.0781 - val_accuracy: 0.9874\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.0713 - val_accuracy: 0.9890\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0780 - val_accuracy: 0.9884\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0793 - val_accuracy: 0.9880\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0863 - val_accuracy: 0.9876\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0825 - val_accuracy: 0.9877\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0729 - val_accuracy: 0.9885\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0777 - val_accuracy: 0.9881\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0721 - val_accuracy: 0.9898\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0788 - val_accuracy: 0.9878\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0747 - val_accuracy: 0.9888\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 9.6186e-04 - accuracy: 0.9998 - val_loss: 0.0731 - val_accuracy: 0.9899\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 6.1922e-04 - accuracy: 0.9998 - val_loss: 0.0802 - val_accuracy: 0.9894\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0851 - val_accuracy: 0.9884\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0796 - val_accuracy: 0.9886\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0832 - val_accuracy: 0.9889\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0933 - val_accuracy: 0.9877\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0897 - val_accuracy: 0.9880\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0822 - val_accuracy: 0.9886\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0839 - val_accuracy: 0.9893\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 6.0123e-04 - accuracy: 0.9999 - val_loss: 0.0830 - val_accuracy: 0.9884\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0975 - val_accuracy: 0.9862\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.0053 - accuracy: 0.9979 - val_loss: 0.0800 - val_accuracy: 0.9882\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 5.3102e-04 - accuracy: 0.9999 - val_loss: 0.0769 - val_accuracy: 0.9893\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 1.6364e-04 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9890\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 7.1822e-05 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 0.9900\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 5.1957e-05 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9897\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (100, 24, 24, 8)          208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (100, 12, 12, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (100, 8, 8, 16)           3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (100, 4, 4, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 10)                 2570      \n",
      "=================================================================\n",
      "Total params: 5,994\n",
      "Trainable params: 5,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, 5, activation = 'relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(16, 5, activation = 'relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('adam',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "model.fit(DATA0, TARGET0, batch_size = 100, epochs = 100, validation_data = (DATA1, TARGET1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQPxklkk73k4"
   },
   "source": [
    "**Task.** How to explain the 3 216 weights in the second convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sOpoODf6Ud9"
   },
   "source": [
    "Each pair of convolutional and pooling layers reduces the spatial image size more than four times. It is a common practice to inrease the number of output channels twice in each subsequent convolutional layer, as we did. In this way, each pair reduces the number of features roughly twice. This in turn significantly reduces the number of weights in the final dense layer and in the entire net, thus making it less and less overfitting. This network has less weights than any previous one, and yet this is the first time that the validation accuracy exceeds 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ_iJkfv-Wve"
   },
   "source": [
    "**Task.** Add a third pair of convolutional and pooling layers. What number of output channels and what filter size should be used in the third convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP__mnVbCdu-"
   },
   "source": [
    "The validation accuracy is slightly higher than with two pairs of convolutional and pooling layers, but there is a better way to improve this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGnvlvLwCJie"
   },
   "source": [
    "**Task.** In the case of dense networks, three layers was the optimal number. It seems that with convolutional networks two pairs of convolutional and pooling layers are a good solution. How to combine these observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihjHQpe8EAED"
   },
   "source": [
    "**Task.** Try other architectures, vary the number of layers, the number of channels, the filter sizes etc., to get an even better accuracy on the validation dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_tasks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
