{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAmgHA43ujGw"
   },
   "source": [
    "# Introduction to the [Tensorflow](https://www.tensorflow.org/tutorials) framework with the Keras API. \n",
    "\n",
    "We will work with the MNIST dataset of hand-written digits. The main task is to recognise the digit wisible on image. We will perfomr following actions: \n",
    "* image classification using logistic regression \n",
    "* image classification using multi-layer dense neural networks\n",
    "* examine various network depths, activation functions and optimizers\n",
    "\n",
    "We will be using a high level [Keras](https://keras.io/getting_started/intro_to_keras_for_researchers/) Application Programming Interface (API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLBuViOPv7Oa",
    "tags": []
   },
   "source": [
    "# Environment preparation\n",
    "\n",
    "* importing of necessary libraries\n",
    "* if given library is not present in software environment it can be installed with \"magic\" commands:\n",
    "\n",
    "```\n",
    "! pip install matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwS4WAtBuiIW",
    "outputId": "e96e4863-7f84-4394-d365-2ba87355b1a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#An eycandy colored text can be printer using package colored\n",
    "from termcolor import colored\n",
    "\n",
    "#The Tensorflow library \n",
    "import tensorflow as tf\n",
    "\n",
    "#Package installation command\n",
    "!pip install matplotlib\n",
    "\n",
    "#The plotting library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "* load the test data from a library of benchmark dataset\n",
    "* print basic information on imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(DATA0, TARGET0), (DATA1, TARGET1) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(colored(\"The training dataset:\",\"blue\"))\n",
    "print(\"Features. type: {}, shape: {}, min/max: {}/{}\".format(type(DATA0), DATA0.shape, DATA0.min(), DATA0.max()))\n",
    "print(\"Labels. type: {}, shape: {}, min/max: {}/{}\".format(type(TARGET0), TARGET0.shape, TARGET0.min(), TARGET0.max()))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(colored(\"The validation dataset:\",\"blue\"))\n",
    "print(\"Features. type: {}, shape: {}, min/max: {}/{}\".format(type(DATA1), DATA1.shape, DATA1.min(), DATA1.max()))\n",
    "print(\"Labels. type: {}, shape: {}, min/max: {}/{}\".format(type(TARGET1), TARGET1.shape, TARGET1.min(), TARGET1.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en3XxGZaxkr2"
   },
   "source": [
    "These are ordinary numpy arrays. There are 60 000 samples in the training dataset and 10 000 in the validation dataset. <br> The targets are integer numbers from 0 to 9 labeling the digits. The data are 28x28 grayscale images with values of type uint8, ranging from 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7v7P3sADg7-"
   },
   "source": [
    "## Visual data inspection\n",
    "\n",
    "* display the first image from the training dataset\n",
    "* display 100th image from \n",
    "* find index of first image with \"9\" in the trainin dataset and display relevant image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "qb3FAR9ODqwJ",
    "outputId": "b1e93738-00c1-4c5f-e975-1331cc34a0d1"
   },
   "outputs": [],
   "source": [
    "plt.subplot(131) #nrows ncolumns index\n",
    "plt.imshow(DATA0[0]);\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(DATA0[99]);\n",
    "\n",
    "index  = np.argmax(TARGET0==9)\n",
    "print(\"Index of first \\\"9\\\" in the training dataset is {}\".format(index))\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(DATA0[index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'> Please: </span> \n",
    "\n",
    "* plot image of the image of the first \"7\" in the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index  = ...\n",
    "print(\"Index of first \\\"7\\\" in the training dataset is {}\".format(index))\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnD7N8gfEHEF"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Ofter tha input data has to be preprocessed to be useful in the training process. Dense networks take flat vectors of floating-point features and are best suited for inputs of the order of 1 rather than 255. \n",
    "\n",
    "* flatten images - change the 2D arrays into a 1D vector. Remember we have many such images, so the shape change is:\n",
    "\n",
    "```\n",
    "(nExamples, nX, nY) -> (nExamples, nX*nY)\n",
    "```\n",
    "\n",
    "* rescale the input values fo [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeBjEJ1lMR9W"
   },
   "outputs": [],
   "source": [
    "(DATA0, TARGET0), (DATA1, TARGET1) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(colored(\"Before preprocessing.\",\"blue\"), \"Pixel (14,14) from the first example of training data: {}\".format(DATA0[0,14,14]))\n",
    "\n",
    "DATA0 = DATA0.reshape(-1, 28 * 28)\n",
    "DATA0 = DATA0.astype('float32')\n",
    "DATA0 = DATA0/float(DATA0.max())\n",
    "\n",
    "print(colored(\"After preprocessing.\",\"blue\"), \"Pixel (14,14) from the first example of raining data: {}\".format(DATA0[0,14*28+14]))\n",
    "\n",
    "#Combine all sten into a single line\n",
    "DATA1 = DATA1.reshape(-1, 28 * 28).astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm4OpcSvW9tK"
   },
   "source": [
    "## A logistic regression model\n",
    "\n",
    "Suppose we have $I$ data samples of $J$ features each and want to classify them to $K$ classes. Let $X_{ij}$ denote feature $j$ of sample $i$. In logistic regression, we first calculate the logits: \n",
    "\n",
    "$$A_{ik}=b_k+\\sum_jX_{ij}w_{jk}$$ \n",
    "\n",
    "where $b_k$ and $w_{jk}$ are called biases and weights. Then we calculate the probabilities that sample $i$ belongs to class $k$ according to the softmax formula: \n",
    "\n",
    "$$P_{ik}=\\frac{\\exp(A_{ik})}{\\sum_{k'}\\exp(A_{ik'})}$$ \n",
    "\n",
    "We then classify each sample to the class with the highest probability. The biases and weights are determined by training the classifier on a dataset $X_{ij}$ where sample $i$ belongs to class $k_i$. This is done by minimizing the cross-entropy loss \n",
    "\n",
    "$$L=-\\sum_{ik}\\delta_{kk_i}\\log P_{ik}$$\n",
    "\n",
    "by using a variant of stochastic gradient descent. \n",
    "\n",
    "Let us implement the above model using the Tensorflow Keras interface.\n",
    "\n",
    "* the model is defines as a set of subsequent layers\n",
    "* dense layer is a set of \"neurons\" that each take a vectorial input, and return a single value. With default setting this corresponds to $A_{i,j}$ with fixed j.\n",
    "  The $k$ index runs over examples provided at the input\n",
    "* we need a single neuron for each digi, to the dense layesr has size 10  \n",
    "* the output of all neurons has to be normalised using the $P_{ik}$ formula - this is done by the ```tf.keras.layers.Softmax``` layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt1_McQT17ls"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI2RBXmr2XhG"
   },
   "source": [
    "* the activation functions, like ```softmax```, ```sigmoid```, ```relu```, etc., can be treated as separate layers or equivalently added at the outputs of the dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrVwci7U3BoU"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Ht754M3ARg"
   },
   "source": [
    "* the weighs of the model are now at some random values, sicne the model has not been trained yet, \n",
    "* we can pass the validation dataset, but the results will be random as well\n",
    "* the result of the model is a 2D array(tensor): `(nExamples, nNeurons)`\n",
    "* **Note** the output type is `tf.Tensor`. Sometimes is is necessary to cast it to numpy array (we will not use this today):\n",
    "\n",
    "```\n",
    "PROBABILITY1 = PROBABILITY1.numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZKl_uxp4kU6",
    "outputId": "e6c69a5c-1425-4491-bd9d-b577818a06ec"
   },
   "outputs": [],
   "source": [
    "PROBABILITY1 = model(DATA1)\n",
    "\n",
    "print(colored(\"Model output shape:\",\"blue\"),PROBABILITY1.shape)\n",
    "print(colored(\"Digits probabilities for the first validation example:\",\"blue\"), PROBABILITY1[0])\n",
    "print(colored(\"Maximum probability is for digit:\",\"blue\"), np.argmax(PROBABILITY1[0]))\n",
    "print(colored(\"The first example label is:\",\"blue\"),TARGET1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBSg0ptiTThE"
   },
   "source": [
    "## <span style='color:red'> Please: </span> \n",
    "\n",
    "Check if the probality normalisation is correct:\n",
    "\n",
    "* use `tf.math.reduce_sum(input_tensor, axis)` function to sum all columns of the output probability for all the xemaples\n",
    "* print the result for the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCbqTokITThE",
    "outputId": "e67f96b8-185b-4462-bb81-b221003d8afb"
   },
   "outputs": [],
   "source": [
    "SUM1 = ...\n",
    "print(colored(\"Sum of the probabilities for the first example:\",\"blue\"),SUM1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfpP1s_SBA2w"
   },
   "source": [
    "## <span style='color:red'> Please: </span> \n",
    "\n",
    "* create the model again print the probabilities for the first exmample in the cell below\n",
    "* execute the cell a few times\n",
    "* observe is the model output changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "\n",
    "PROBABILITY1 = ...\n",
    "\n",
    "print(colored(\"Model output shape:\",\"blue\"), ...)\n",
    "print(colored(\"Digits probabilities for the first validation example:\",\"blue\"), ...)\n",
    "print(colored(\"Maximum probability is for digit:\",\"blue\"), ...)\n",
    "print(colored(\"The first example label is:\",\"blue\"), ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHyhWslq9aWN"
   },
   "source": [
    "## Performace metric - accuracy\n",
    "\n",
    "We have been looking at the output of the model for single exmaple. We need to have a metric stating the model performance on a whole set of examples.\n",
    "Here we will use ```accuracy``` - fraction of correctly assigned labels\n",
    "\n",
    "* create w vector of model labels - indices where the model prediction is maximal\n",
    "* create a vector of ```True/False``` for correct/incorrect label assignement\n",
    "* calculate the mean value of this vector - this is the accuracy in our case\n",
    "* **what is the expected accuracy for random guessing of the digit?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7wPKlMe9skq",
    "outputId": "b164e3fc-8505-4deb-e213-7ee803a36848"
   },
   "outputs": [],
   "source": [
    "LABEL1 = np.argmax(PROBABILITY1, axis=1)\n",
    "ACCURACY1 = (LABEL1 == TARGET1)\n",
    "\n",
    "print(colored(\"Is the model answer correct?\",\"blue\"),ACCURACY1)\n",
    "print(colored(\"Fraction of correct model answers:\",\"blue\"),ACCURACY1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAxxsr5S_bGl"
   },
   "source": [
    "## Model training\n",
    "\n",
    "To find the model weighs that will be the \"best\" we need:\n",
    "\n",
    "* **optimizer** - an algoritm that will adapt weights to minimize the . We will use `stochastic gradient descent (sgd)`\n",
    "* **loss function** - the definition of \"the best\" - the function that will be minimalize by optimizer We will use `sparse categorical crossentropy`\n",
    "* **metris** - a metric of interest for the model user - we will use `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAnJzvmq_zHg"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euHfKTMaDgiy"
   },
   "source": [
    "Once the model is associated with a loss function and a metric, calculate their values on any dataset by calling a built-in method:\n",
    "\n",
    "* the evaluation runs in batches. Default batch size is 32\n",
    "* a single run througn the whole data set is an `epoch`\n",
    "* for the training data we have 60000/32 = 1875 steps per epoch\n",
    "* loss anf metrics are averaged over the data processed, so it changes during looping through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNSXFvd_D8ZZ",
    "outputId": "89feeb99-ed91-4c1e-ccd8-3899288b3537"
   },
   "outputs": [],
   "source": [
    "model.evaluate(DATA0, TARGET0)\n",
    "model.evaluate(DATA1, TARGET1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyz5ntS1AJur"
   },
   "source": [
    "The accuracy on the validation dataset is indeed equal to what we calculated manually. Now train the model on the training dataset. Do only 10 epochs for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxHA190WAWrl",
    "outputId": "a0bd1f63-4a65-44b3-e6f0-c94c7fe932cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x=DATA0, y=TARGET0, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29qcJlvDLUBT"
   },
   "source": [
    "The accuracy on the training dataset may not be representative for new samples due to the so-called overfitting.\n",
    "\n",
    "* to get a more reliable estimation of the model accuracy, evaluate metrics on a validation dataset\n",
    "* change the batch size and do more epochs to perform a full training. Larger batch size will allow to better explot the GPU capabilities, as more data will be processed in parallel\n",
    "* use ``%%time`` magic command to measurte the cell exetuction time\n",
    "* use `verbose=0` to silent the printout. It is much more convenient to plot the metric values\n",
    "* keep the output of the `model.fit()` method in a variable. The ouput is training history, and can by used for plotting the metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEqqjdTFIGQ_",
    "outputId": "f1ead664-cb75-47c0-84bc-bcfb6da9ade5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(DATA0, TARGET0, batch_size = 128, epochs = 10, validation_data = (DATA1, TARGET1), verbose=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLmQlid5C9Lc"
   },
   "source": [
    "Note that with the batch of 128, a single epoch executes roughly twice faster than with the default batch of 32. Why? What may be the other consequences of changing the batch size from 32 to 128?\n",
    "\n",
    "Even though the batch of 128 is 3 times larger than the default batch of 32, Tensorflow is optimized so that processing the larger batch takes only 1.5 more time than processing the smaller one (3ms vs 2ms). But there are 3 times less large batches per epoch than there are small ones, which translates to 3 times less minimization steps. This gives 3 / 1.5 = 2 times less time per epoch with the larger batch (2s vs 4s). Less minimization steps per epoch imply that more epochs may be needed to achieve the same result. On the other hand, larger batch gives a better estimation of the real gradient so the minimization is less noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHsObtF1P9V7"
   },
   "source": [
    "Although this is not absolutely necessary, evaluate the model on the training and validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I51pvoXxQCtJ",
    "outputId": "d5b7bf0b-6f32-4509-8685-a593e8ff4126"
   },
   "outputs": [],
   "source": [
    "model.evaluate(DATA0, TARGET0);\n",
    "model.evaluate(DATA1, TARGET1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUhoGce-NWdP"
   },
   "source": [
    "**Question.** The validation accuracy displayed after the last training epoch is equal to the validation accuracy evaluated after completing the training. This is not the case for the accuracy on the training dataset. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgZHfu5COIUF"
   },
   "source": [
    "**Solution.** Accuracy is calculated as a mean over all batches in the concerned dataset. The validation accuracy displayed during training is calculated after completing each epoch. But the training accuracy displayed during training is not calculated in such a way, because this would require iterating over the entire training dataset after completing each epoch and could be time-consuming. Instead, the training accuracy displayed during training is calculated during the epoch. But during the epoch, a minimization step is performed after each batch, so subsequent batches correspond to different model weights. Consequently, the displayed training accuracy does not correspond to any fixed set of model weights and is only an approximation to the real accuracy on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls-nwb99M4-A"
   },
   "source": [
    "In the following, by the validation accuracy of a model, we will always mean the maximum value listed during training, even if it is not in the last epoch. This is because Tensorflow can stop training after any epoch and thus produce a model with validation accuracy corresponding to that epoch. \n",
    "\n",
    "* print a summary of the model architecture\n",
    "* create a plot with layers description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5pbSr7HNkzv",
    "outputId": "e9c9aa67-9506-4093-8cf9-fbefeca49d0d"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, 'ML_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-1lOAUvwWAo"
   },
   "source": [
    "**Question** Why does the number of weights equal 7 850?\n",
    "\n",
    "**Solution.** To produce its output, a dense layer multiplies the input by a matrix whose one dimension is the number of inputs and the other is the number of outputs. That matrix has 784 * 10 = 7 840 elements in this case. Then, the layer adds a bias to each output, so there are 10 biases. Together, there are 7 840 + 10 = 7 850 weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN12-s84N0uI"
   },
   "source": [
    "**Task.** In this example, the training and validation accuracies are very close to each other, which means that there is almost no overfitting. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7SAFRHlSSyW"
   },
   "source": [
    "**Solution.** Because there are much more samples in the training dataset than there are model weights. So, the problem is overdetermined and there are not enough degrees of freedom to adjust the model too much to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training history\n",
    "\n",
    "It is good to monitor the evolution of the loss function and the metric duriong the training\n",
    "\n",
    "* inspect the content of the `history` variable initializez by the output of the `model.fit()` method\n",
    "* plot the accuracy as a function of the epoch numer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"Content of the history object:\", \"blue\"),history.history.keys())\n",
    "print(colored(\"Accuracy on the traiing dataset:\",\"blue\"), history.history['accuracy'])\n",
    "\n",
    "def plotTrainingHistory(history):\n",
    "\n",
    "    fig, axes= plt.subplots(1,2,figsize=(10,5))\n",
    "    history = history.history\n",
    "    axes[0].plot(history['accuracy'])\n",
    "    axes[0].plot(history['val_accuracy'])\n",
    "    axes[0].set_ylabel('accuracy')\n",
    "    axes[0].set_xlabel('epoch')\n",
    "    axes[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "    axes[1].set_ylabel('loss function')\n",
    "    axes[1].set_xlabel('epoch')\n",
    "    axes[1].legend(['train', 'validation'], loc='upper left')    \n",
    "    \n",
    "plotTrainingHistory(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'> Please: </span> \n",
    "\n",
    "* finish the `plotTrainingHistory(history)` function by adding a plot for the loss function evolution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingHistory(history):\n",
    "\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "plotTrainingHistory(history)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZRU5wG-OU3a"
   },
   "source": [
    "## Model extension\n",
    "\n",
    "* add a second layer, **a hidden layer**, between the input, and output layers\n",
    "* choosing the hidden layers parameters is part of the ML art. A rule of thumb for the number of neurons in that layer may be, the geometric mean of the number of inputs and outputs, that is:\n",
    "\n",
    "$$\n",
    "\\sqrt{10 \\cdot 784} = 89\n",
    "$$\n",
    "\n",
    "or the next power of two, that is, 128. \n",
    "* use the sigmoid activation function for the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFzq6fwtQaIi",
    "outputId": "1a5c2d38-ef87-41ff-8024-f37c2c93b7fa"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('sgd',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "history = model.fit(DATA0, TARGET0, batch_size = 128, epochs = 100, validation_data = (DATA1, TARGET1), verbose=0)\n",
    "\n",
    "model.summary()\n",
    "model.evaluate(DATA0, TARGET0);\n",
    "model.evaluate(DATA1, TARGET1);\n",
    "tf.keras.utils.plot_model(model, 'ML_model.png', show_shapes=True)\n",
    "plotTrainingHistory(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZrZ1JA5Ivtm"
   },
   "source": [
    "**Task.** Explain the number of weights in this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a476XmIoI4jQ"
   },
   "source": [
    "**Solution.** The first layer has 784 inputs and 128 outputs, so 784 * 128 + 128 = 100 480 weights. The second layer has 128 inputs and 10 outputs, so 128 * 10 + 10 = 1 290 weights. Together, there are 100 480 + 1 290 = 101 770 weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PibNHQJYRVpB"
   },
   "source": [
    "Adding the second layer improved the accuracy surprisingly little. \n",
    "\n",
    "* change the activation function of the added layer to `relu`, which is one of the most used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NXdUG_jR0lu",
    "outputId": "7518730f-083a-48a1-9d31-42034ae7aca8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('sgd',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "history_sgd = model.fit(DATA0, TARGET0, batch_size = 128, epochs = 100, validation_data = (DATA1, TARGET1), verbose=0)\n",
    "model.summary()\n",
    "model.evaluate(DATA0, TARGET0);\n",
    "model.evaluate(DATA1, TARGET1);\n",
    "plotTrainingHistory(history_sgd)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RH6_mxR8XSco"
   },
   "source": [
    "Now the improvement is very significant. The difference is because sigmoid saturates on both sides, yields vanishing gradients there, and effectively stucks the minimization. Relu does not saturate on the positive side and causes no such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xoKXgKAOp02"
   },
   "source": [
    "**Question.** The training takes less physical time with the relu activation. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLd0VST7QIWM"
   },
   "source": [
    "**Solution.** Relu does not contain nonlinear functions, like exponents, and is therefore faster to calculate and differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO6CebTyQ6si"
   },
   "source": [
    "* change the optimizer algorithm to `Adaptive Momentum (Adam)`, which is one of the best and most used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUB3kB-vdLda",
    "outputId": "4781f8af-6eab-42b4-b9ab-9264a0b3de99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')])\n",
    "\n",
    "model.compile('adam',\n",
    "              'sparse_categorical_crossentropy',\n",
    "              'accuracy')\n",
    "\n",
    "history_adam = model.fit(DATA0, TARGET0, batch_size = 128, epochs = 100, validation_data = (DATA1, TARGET1), verbose=0)\n",
    "model.summary()\n",
    "model.evaluate(DATA0, TARGET0);\n",
    "model.evaluate(DATA1, TARGET1);\n",
    "\n",
    "print(colored(\"SGD optimizer final validation accuracy:\",\"blue\"), history_sgd.history[\"val_accuracy\"][-1])\n",
    "plotTrainingHistory(history_sgd)\n",
    "\n",
    "print(colored(\"ADAM optimizer final validation accuracy:\",\"blue\"), history_adam.history[\"val_accuracy\"][-1])\n",
    "plotTrainingHistory(history_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJB1CEClbNVM"
   },
   "source": [
    "The final validation accuracy is only slightly higher but Adam achieved it in a much smaller number of epochs than SGD. From now on, we will always use the Adam optimizer and the relu activation function, and 32 epochs.\n",
    "\n",
    "It is impressive that such a simple two-layer network correctly classifies 100% of the training dataset. \n",
    "**This means that the lower accuracy on the validation dataset is uniquely due to overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IKKOhQxVdb3"
   },
   "source": [
    "**Question.** Why is there a significant overfitting here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZCQVKdYVn5F"
   },
   "source": [
    "**Solution.** Because the number of model weights is significantly higher than the number of training samples. The problem is now underdetermined and gives room for excessive adjustment of the model weights to this particular training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3luXuOdSZ3q"
   },
   "source": [
    "Since there is overfitting caused by the already large number of model weights, it is dubious whether adding a third layer would still improve things, because it will increase the number of weights even more. Check this by adding a third layer. \n",
    "\n",
    "## <span style='color:red'> Please: </span> \n",
    "\n",
    "* create a model with three layers with 256, 65, 10 neutrons\n",
    "* the last layer is the output layer, so it should have `activation = 'softmax'\n",
    "* other layers should have activation = 'relu'\n",
    "* use `adam` optimizer\n",
    "* use `batch_size = 32`, and run the optimization for `epochs = 32`\n",
    "* submit the accuaracy wih three digits precision: 0.XXX on the validation dataset [here](https://docs.google.com/forms/d/1ZqW4bwP9jVepDUWlHnBkWzrlzQrVkyujjnsrNxoBPrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brPLf_ZrSZTJ",
    "outputId": "ffeb83d2-c119-42db-c798-06f0193024d5"
   },
   "outputs": [],
   "source": [
    "model = ...\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history = model.fit(...)\n",
    "model.summary()\n",
    "model.evaluate(DATA0, TARGET0);\n",
    "model.evaluate(DATA1, TARGET1);\n",
    "plotTrainingHistory(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd_I4gqdXR-F"
   },
   "source": [
    "Adding the third layer improved the validation accuracy only slightly. But this is still an achievement because the closer we are to 100%, the more difficult it is to gain accuracy. Note that even a slight increase in accuracy leads to a significant reduction in the error rate, which is complementary to accuracy. Here for instance, increasing the accuracy from 0.980 to 0.985 reduces the error rate from 2% to 1.5%, that is, by a factor of 4/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNTzk7lVZTbh"
   },
   "source": [
    "## <span style='color:red'> Please: </span> \n",
    "\n",
    "* check that adding a fourth dense layer does not further increase the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kTwti-scFZz",
    "outputId": "1b4a0c7e-03d1-48f3-944a-4c1ce5bc6688"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "model = ...\n",
    "\n",
    "model.compile(...)\n",
    "\n",
    "history = model.fit(...)\n",
    "\n",
    "model.summary()\n",
    "model.evaluate(DATA0, TARGET0);\n",
    "model.evaluate(DATA1, TARGET1);\n",
    "plotTrainingHistory(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krJXXCN2dX6o"
   },
   "source": [
    "Since adding more dense layers does not further increase the validation accuracy, we must resort to other means. \n",
    "\n",
    "There are two paths:\n",
    "\n",
    "* model regularisation - dedicated techniques to reduce overfitting, \n",
    "* model modification - use of different type of layers. The `convolutional layers` are specifically designed for image analyses.\n",
    "\n",
    "..but this is a story for an medium level ML Workshop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model deployment\n",
    "\n",
    "After obtaining a satifcatory model, one still need to deliver it to \"client\". Often this requires some additional work.\n",
    "\n",
    "* please test the model on your own digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def testModelOnMyDigits(model):    \n",
    "    #Code created by M.Fila@UW\n",
    "    if not os.path.isdir(\"colab_freehands\"):\n",
    "        !git clone https://github.com/m-fila/colab_freehands.git\n",
    "\n",
    "    from colab_freehands.canvas import Canvas  \n",
    "    canvas = Canvas(line_width=2)\n",
    "    example = (\n",
    "        canvas.to_array(size=(20, 20), margin=(4, 4), dtype=np.float32, weighted=True) / 255\n",
    "    )\n",
    "    predictions = model(np.expand_dims(example, (0, -1)))\n",
    "    plt.imshow(example, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\n",
    "        \"Predicted class: {} ({:.0f}%)\".format(\n",
    "            np.argmax(predictions), np.max(predictions) * 100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "testModelOnMyDigits(model)    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
