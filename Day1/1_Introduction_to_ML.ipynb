{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258f4ec1-8dfe-4031-998f-15b8bf342b50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Machine learning introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666a0e3-8463-48e0-b185-97572de7d9d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28b97c-a0a5-4311-a5c5-f8de44c4b814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3426c-4ee8-415a-90c4-9ecddb860678",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Machine Learning in a nutshell\n",
    "\n",
    "Any Machine Learning (ML) model makes an attempt to construct a function from\n",
    "the `n` dimensional input space (`features`) to the ``m`` dimensional output space (`targets` or `labels`):\n",
    "\n",
    "$$\\Huge{\n",
    "R^{n} \\rightarrow R^{m}\n",
    "}\n",
    "$$\n",
    "\n",
    "Basing on the output data there are four main flavours of ML models:\n",
    "\n",
    "* according to the type of the output space:\n",
    "    * **regression** - the output space is the ussual $R^{m}$\n",
    "    * **categorisation** - the output space is a discrete set of possibilities - ``categories``. The model is expected to provide probability that example ``x`` belongs to class ``y``.  \n",
    "* according to the avalaibility of the data in the output space - ``labels``:\n",
    "    * **supervised learning** - a full set of input: ``X``, and output: ``Y`` values is available: ``data=(X,Y)``\n",
    "    * **unsupervised learing** - only the input data: ``X`` is avalable. ``Y`` values are usually unknown: ``data=(X)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458cf57-1a3a-4d4c-94c7-663b003982fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here capital X, Y denote a set of feature, or label values. A single ``example`` from the full dataset will be denoted with small letterrs: \n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "(x,y) \\in (X,Y)\n",
    "}\n",
    "$$\n",
    "\n",
    "The $X \\rightarrow Y$  correspondence is ussually not deterministic (mostly due to lack of our knowlwdge, e,g, ``X`` does not cover all the variables controlling the ``Y`` value) the ML models provide some probabilistic estimates, like expectation value:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "   f(x) = \\int_{Y} y \\cdot p(x,y) dy = <y(x)>\n",
    "}\n",
    "$$\n",
    "where $p(x,y)$ is a joint probability ditribution for (x,y) pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fda399-f690-4217-9e67-514074880190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Linear model - an archetypic ML model.\n",
    "\n",
    "We will play with a simple, therefore easy to understand linear model:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "x = (x_{1}, ...,x_{m}), y = (y) \\\\\n",
    "f(x) = \\theta_{0} + \\theta_{i} \\cdot x_{i}\n",
    "}\n",
    "$$\n",
    "\n",
    "This example will allow us to introduce basic ML concepts of `loss function` and `gradient descent`.\n",
    "We will also learn basic programming techniques using numpy arrays parallel manipulations.\n",
    "\n",
    "We will use a rank 1 ```x``` and ```y``` arrays: a single dimensional case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8ac98-93a5-4f27-b8cb-2137d7dcbbbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "First we generate the data:\n",
    "\n",
    "1) draw a random set X\n",
    "2) draw a random set Y (y values for every x value). The Y will have a random noise ($\\mathbf{\\epsilon}$), around the ideal case (equal `0` for a moment):\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    "y = \\theta_{0} + \\theta_{1} \\cdot x + \\epsilon \\\\\n",
    "}\n",
    "$$\n",
    "3) define a linear model:\n",
    "$$\n",
    "\\huge{\n",
    "f(x) = \\theta_{0} + \\theta_{1} \\cdot x\n",
    "}\n",
    "$$\n",
    "4) perform a ``training``\n",
    "\n",
    "5) estimate the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f1e49-c20d-424d-b0b1-cb912aedc99b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Generate the X set\n",
    "\n",
    "Here the ``X`` is just a set of random values in one dimention. This means we have one feature, `n=1`, and `nPoints` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857cf43-d041-4e7d-929a-9354c2282f68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "minX = 0\n",
    "maxX = 1\n",
    "nPoints = 5\n",
    "x_raw = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "\n",
    "print(colored(\"Shape of the X array:\",\"blue\"),x_raw.shape)\n",
    "\n",
    "#it will be convenient to represent X as a an array: nPoints rows, one column\n",
    "x_raw = np.reshape(x_raw, (-1,1))\n",
    "print(colored(\"Shape of the X array:\",\"blue\"),x_raw.shape)\n",
    "print(colored(\"First few examples:\\n\",\"blue\"),x_raw[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822ebbf-a27c-419d-8d7d-8884e9cb4ccf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Generate Y according to given formula, without any noise:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y = \\theta_{0} + \\theta_{1} \\cdot x \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884f4e3-a9a0-478f-a042-ce8ddea82053",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define coefficients as a array\n",
    "theta = (1,3)\n",
    "\n",
    "# calculate the y values\n",
    "y = theta[0] + theta[1]*x_raw\n",
    "print(colored(\"First few Y values:\\n\",\"blue\"), y[:10])\n",
    "\n",
    "# calculate the y values using a more compact formula\n",
    "# substitute each X value by a pair (1,X):\n",
    "ones = np.ones(x_raw.shape[0])\n",
    "x = np.column_stack((ones, x_raw))\n",
    "\n",
    "print(colored(\"Extended X shape:\\n\",\"blue\"),x.shape)\n",
    "print(colored(\"First few X values:\\n\",\"blue\"),x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1013198-2feb-411b-b893-aed17c6353ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#write linear formula as a element-wise multiplication,\n",
    "#followed by sum of columns: along axis=1 direction\n",
    "y = np.sum(theta*x, axis=1)\n",
    "print(colored(\"First few Y values:\\n\",\"blue\"), y[:10])\n",
    "\n",
    "#change Y to a rank-2 array:\n",
    "#row is particular example\n",
    "#column - is a target at this example\n",
    "y = np.reshape(y,(-1,1))\n",
    "print(colored(\"First few Y values:\\n\",\"blue\"), y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095152b-4a8d-41e8-a20a-7bb9e115c3ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Training: a loss function\n",
    "\n",
    "We find the estimates of our model parameters: $a = (a_{0}, a_{1})$ as those which provide a model ``f(X)`` that minimizes a **loss function:**\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y)\n",
    "}\n",
    "$$\n",
    "\n",
    "Most popular loss functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5c05e-8f35-4d17-8e15-0ec9c23eca12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **regression task:** mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y) = \\frac{1}{N} \\sum_{X} (f(x) - y)^{2}\n",
    "}\n",
    "$$\n",
    "\n",
    "where `N` in the size of the dataset (X,Y). We **never** use all the data points at hand here! ``X`` is a sub sample - a **training** set. \n",
    "Well, never, unless we can have infinite amount of data - as in our case, where we can always generate more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e5a9b-77af-467c-a5ce-1566c3cfa703",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* **categorisation task:** cross entropy or likelihood:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y) = -\\log P_{model}(Y|X) = \\\\ \n",
    "-\\sum_{X} \\log f_{correct~class}(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "**Notes**: \n",
    "\n",
    "* sometimes a $\\frac{1}{N}$ normalising factor is included. It is not important for the training, but important if one wants to compare the loss value for samples of different size.\n",
    "\n",
    "* model provides probability that example belongs to class ``i``: $f(x) = (p_{0}(x), p_{1}(x),...p_{m-1}(x))$ (note here f(x) is a vector, with rank `m` - equal to number of classes). We select only the component corresponding to the correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901821a-7798-4a1d-8c4f-b2f26839732a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's calculate the MSE loss function on our data for some random values of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f9de7-5e01-46b8-899f-54c1937ab5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_theta = 0\n",
    "max_theta = 3\n",
    "nParams = 2\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "print(colored(\"Some random parameters are:\",\"blue\"),theta_model)\n",
    "\n",
    "#calculate the model result on the data\n",
    "#use keepdims=1 to get correct output shape\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "\n",
    "#calculate the loss function\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Loss function with random parameters:\",\"blue\"), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26f0c5-d4f0-460b-bf3b-3749a5fc5648",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "\n",
    "* add a block of code that will check the loss function value with model parameters the same as used for sample generation.\n",
    "What is the expected result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8265035-f83c-4e85-8207-f9f02c9a6cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "y_model = np.sum(theta*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Loss function with nominal parameters:\",\"blue\"), loss)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710eaeda-4dc9-47e3-9452-dc0cb6449ad2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The training\n",
    "\n",
    "How the parameters of the model are found? \n",
    "\n",
    "`We calculate gradient vector of the loss function wrt. the parameters, and go opposite to the gradient to the minimim of the loss function.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477b706-20f7-4c95-8b4c-785f31c9281b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The algorithm is as follows:\n",
    "\n",
    "1) define starting values for $\\theta$ parameters: `init_theta`\n",
    "\n",
    "2) select a sub sample of examples to be used for the loss function calculation: a `batch`\n",
    "\n",
    "3) calculate the loss function gradient. In our case this is:\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    "\\nabla_{\\theta} L = \\nabla_{\\theta} \\frac{1}{N} \\sum_{X} (\\theta^{T} \\cdot x - y)^{2} = \\\\\n",
    "\\frac{2}{N} \\sum_{X} (\\theta^{T} \\cdot x - y)x\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c36dc-50ff-41dc-a0f3-06c645cd6e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "4) update the $\\theta$ parameter proportionally to the gradient, with proportionality parameter $\\alpha$:\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    " \\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla_{\\theta} L\n",
    "}\n",
    "$$\n",
    "5) loop with batches through the whole dataset\n",
    "\n",
    "6) repeat reading the whole dataset many times. One pass through the whole dataset is called a `epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd55e06-19f9-47ae-91be-16be7be25494",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(x_batch, y_batch, init_theta, alpha, verbose=False):\n",
    "    \n",
    "    theta = init_theta \n",
    "    if verbose:\n",
    "        print(colored(\"initial theta:\\n\",\"blue\"), theta)\n",
    "    #model response\n",
    "    y_model = np.sum(theta*x_batch, axis=1, keepdims=True)\n",
    "    if verbose:\n",
    "        print(colored(\"model response:\\n\",\"blue\"),y_model[:10])\n",
    "    #calculate the update value\n",
    "    delta = -alpha*(y_model - y_batch)*x_batch\n",
    "    if verbose:\n",
    "        print(colored(\"delta for each example:\\n\",\"blue\"),delta[:10])\n",
    "    #average the update value over the examples provides\n",
    "    delta = np.mean(delta, axis=0)\n",
    "    if verbose:\n",
    "        print(colored(\"delta averaged over examples:\\n\",\"blue\"),delta)\n",
    "    #apply the update to theta\n",
    "    theta = theta + delta\n",
    "    if verbose:\n",
    "        print(colored(\"final theta:\\n\",\"blue\"), theta)\n",
    "    return theta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081ccf5-0b19-4914-91e4-11c3f62202af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test the function on a small batch\n",
    "x_batch = x[:3]\n",
    "y_batch = y[:3]\n",
    "print(colored(\"X test:\\n\",\"blue\"),x_batch)\n",
    "print(colored(\"Y test:\\n\",\"blue\"),y_batch)\n",
    "\n",
    "#choose random initial parameters values\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "alpha = 1\n",
    "minDelta = 1E-3\n",
    "batch_gradient_descent(x_batch, y_batch, theta_model, alpha, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c0f6c-fa20-4042-ae9e-01f8c0bbf247",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "* provide input parameters to the `batch_gradient_descent()` for which you can easily (=without any calculations!) evaluate the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e1f53-6093-4718-9780-d06e31f4b608",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "batch_gradient_descent(x_batch, y_batch, theta, alpha, True)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713f3cc-68a9-4736-8ffc-75aeb4f1771f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The training loop\n",
    "\n",
    "* write a loop with `batches` over the dataset: `epoch`\n",
    "* calculate the loss function for each epoch\n",
    "\n",
    "For simplicity, we will take the whole dataset as a single batch. Usually batch size is a compromise between memory capacity, and number of parameters updates we want to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f116d9-e261-405b-a4b7-7336dc76596a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Theta value before training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with initial parameters:\",\"blue\"), loss)\n",
    "\n",
    "alpha = 1\n",
    "nEpochs = 100\n",
    "\n",
    "for iEpoch in range(nEpochs):\n",
    "    x_batch = x\n",
    "    y_batch = y\n",
    "    theta_model = batch_gradient_descent(x_batch, y_batch, theta_model, alpha)\n",
    "    \n",
    "    y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "    loss = np.mean((y_model - y)**2)\n",
    "    #print(colored(\"Epoch:\",\"blue\"), iEpoch, colored(\"loss:\",\"blue\"),loss)\n",
    "    \n",
    "print(colored(\"Theta value after training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with final parameters:\",\"blue\"), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58e98a-2476-4b93-bdfd-380c5f8fd48e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Model performance on training data\n",
    "\n",
    "* plot the model results and investigate by eye how well it works on the `training` data. Unfortunately in real application eye inspection is usually not possible as data is multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773573e-6d3c-43f1-8a4f-088720c1bcc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#plot the data points. Strip the \"1\" column from the features array\n",
    "ax.plot(x[:,1],y, \"bo\", label=\"training data\")\n",
    "\n",
    "#plot the model result\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "ax.plot(x[:,1],y_model, \"rs\", label=\"model result\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7776d1-1242-4b88-9378-ce3d7a6b5a78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Model performance on test data\n",
    "\n",
    "* we investigate how the model works for data not used for training - the `test` dataset. The performance estimate has to be always made on a separate sample - the `test` dataset.\n",
    "\n",
    "Create a test dataset, and make plot of the model result and the test data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e68ce-9c8a-44ec-84f9-827c56da8b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#create the test dataset\n",
    "nPoints = 15\n",
    "x_test = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "x_test = np.reshape(x_test, (-1,1))\n",
    "ones = np.ones(x_test.shape[0])\n",
    "x_test = np.column_stack((ones, x_test))\n",
    "y_test = np.sum(theta*x_test, axis=1)\n",
    "\n",
    "#plot the data points\n",
    "ax.plot(x_test[:,1],y_test, \"bo\", label=\"test data\")\n",
    "\n",
    "#plot the model result\n",
    "y_model = np.sum(theta_model*x_test, axis=1, keepdims=True)\n",
    "ax.plot(x_test[:,1],y_model, \"rs\", label=\"model result\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a983bbb-6c2c-4d18-96e8-f1e485c3b5d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Improve the model performance - run a longer training.\n",
    "\n",
    "Our task is a very simple, but the model does not work very well - this is because we have made not enough model parameter update iterations.\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* run the training for increasing number of iterations - until you will be satisfied with the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c56607-2331-4397-abb9-a8533349a26f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Run the training on noisy data\n",
    "\n",
    "So far the model performance was the same on training and test data. This was due to the fact there were no statistical fluctuations between the two datasets, and also the model is very simple, and can not adapt to much to data.\n",
    "\n",
    "Now we will use a noisy dataset for training to see the effect of fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2d6b5-bd5d-4d12-a61b-ddb960cb4e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Generate data with Gaussian noise added to the pure formula:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y_{with~noise} = y + \\epsilon \\\\\n",
    "= Normal(\\mu=y, \\sigma=1)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1059ec-b354-4278-bcec-dfe7e680e1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nPoints = 5\n",
    "x = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "\n",
    "ones = np.ones(x.shape[0])\n",
    "x = np.column_stack((ones, x))\n",
    "y = np.sum(theta*x, axis=1, keepdims=True)\n",
    "\n",
    "#define the gaussian smearing parameters\n",
    "mu = y\n",
    "sigma = 1.0\n",
    "\n",
    "#generate the data \n",
    "y_with_noise = np.random.default_rng().normal(mu, sigma)\n",
    "\n",
    "print(colored(\"Shape of the Y array:\",\"blue\"),y_with_noise.shape)\n",
    "print(colored(\"First few Y values with noise:\\n\",\"blue\"), y_with_noise[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c467d0-8908-4256-8ad7-3de9fd827ced",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Make a control plot of noise data\n",
    "\n",
    "Plot a histogram of `Y_noise - Y`. Check if it looks as expected.\n",
    "\n",
    "**Please**\n",
    "\n",
    "* generate more points if the histogram does not allow to judge the data quality\n",
    "* go back to 5 data points for further exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cff4cd-4b36-4a9a-8db2-fb0e25d15911",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#control the size of the plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# plot a histogram of fidderence between noisy and clean values\n",
    "ax.hist(y_with_noise-y, bins=20, density=True, label=r\"$y_{noise} - y$\");\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a64c0-0381-4537-a6b3-e7589adfc1c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Run the training loop on the noisy data.\n",
    "\n",
    "What is the minimal value of the MSE on the noisy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827278bc-da53-4fa7-8efb-411c10c39282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y_with_noise)**2)\n",
    "print(colored(\"Theta value before training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with initial parameters:\",\"blue\"), loss)\n",
    "\n",
    "alpha = 0.1\n",
    "nEpochs = 10\n",
    "for iEpoch in range(nEpochs):\n",
    "    x_batch = x\n",
    "    y_batch = y_with_noise\n",
    "    theta_model = batch_gradient_descent(x_batch, y_batch, theta_model, alpha)\n",
    "    \n",
    "    y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "    loss = np.mean((y_model - y_with_noise)**2)\n",
    "    #print(colored(\"Epoch:\",\"blue\"), iEpoch, colored(\"loss:\",\"blue\"),loss)\n",
    "    \n",
    "print(colored(\"Theta value after training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with final parameters:\",\"blue\"), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9d317-0c3c-4fc6-ab09-afd9390b3c2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* plot the model result on the training, noisy data\n",
    "* plot the model result on the test, clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000fe87-4f12-4830-8293-996c92636127",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "#plot the data points\n",
    "ax[0].plot(x[:,1],y_with_noise, \"bo\", label=\"training data\")\n",
    "\n",
    "#plot the model result\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "ax[0].plot(x[:,1],y_model, \"rs\", label=\"model result\")\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].legend();\n",
    "\n",
    "#create the test dataset\n",
    "nPoints = 15\n",
    "x_test = np.random.default_rng().uniform(minX, maxX, nPoints).reshape((-1,1))\n",
    "ones = np.ones(x_test.shape[0])\n",
    "x_test = np.column_stack((ones, x_test))\n",
    "y_test = np.sum(theta*x_test, axis=1)\n",
    "\n",
    "#plot the data points. Y of the test data correspond to mean Y of the noisy data.\n",
    "ax[1].plot(x_test[:,1],y_test, \"bo\", label=\"mean target value\")\n",
    "\n",
    "#plot the model result\n",
    "y_model = np.sum(theta_model*x_test, axis=1, keepdims=True)\n",
    "ax[1].plot(x_test[:,1],y_model, \"rs\", label=\"model result\")\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e11dd7-a84c-4096-bb8d-9e0c1dfc5a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## More data. \n",
    "\n",
    "to get better training results we need more data. More data will allow to average over statistical fluctiations.\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* generate a larger training dataset - 10000 points\n",
    "* run the training again\n",
    "* remake the plots on training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e66ea-3281-4426-bb05-598c54ca63ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Nonlinear models: neural networks\n",
    "\n",
    "# An universal approximation theorem [Cybenko, 1989](https://link.springer.com/article/10.1007/BF02551274):\n",
    "\n",
    "Let's define a ``neuron`` function on $R^{n} \\rightarrow R$:\n",
    "$$\n",
    "\\huge{\n",
    " f(\\theta, x) = A(\\sum_{i=1}^{N} \\theta_{i} x_{i} + \\beta)\n",
    "}\n",
    "$$\n",
    "\n",
    "where `A` - activation function: any function of a single argument that fulfills requirements:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    " \\lim_{x\\rightarrow -\\infty} f(x) \\rightarrow 0 \\\\\n",
    " \\lim_{x\\rightarrow +\\infty} f(x) \\rightarrow 1 \\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5363e-4815-4384-b3c7-48d8dc214bf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Every continuous function on $R^{n} \\rightarrow R$ can be approximated in basis of neural functions (=one layer of neurons).\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y(x) \\simeq \\sum_{k} w_{k} f_{k}(\\theta_{k}, x)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8cc3cc-2118-415d-a842-6a5ddd68b6ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We will not implement the gradients, and the training loop for the neural networks by hand. We will use pyTorch framework, which has all the components ready. This is the topic of the next session: a simple multi layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f86415-6ddd-448e-90b0-37ba81bc089e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A task for the end of the session.\n",
    "\n",
    "The statistical positional parameter of the Y population to which the model converges depends on the loss function. With the MSE loss function the model converges to the mean of target popolation. With mean absolute error (MAE):\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y) = \\frac{1}{N} \\sum_{X} |f(x) - y|\n",
    "}\n",
    "$$\n",
    "\n",
    "the model converges to **median** of the Y distribution.\n",
    "\n",
    "**Your task is to verify this statement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd74f35-e709-4da3-9a70-680054d780b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Please:**\n",
    "\n",
    "1) modify the `batch_gradient_descent()` function so it will use the MAE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c9667-556e-4907-9dc5-a1097e326603",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(x_batch, y_batch, init_theta, alpha, verbose=False):\n",
    "    #BEGIN_SOLUTION\n",
    "    theta = init_theta \n",
    "    if verbose:\n",
    "        print(colored(\"initial theta:\\n\",\"blue\"), theta)\n",
    "    #model response\n",
    "    y_model = np.sum(theta*x_batch, axis=1, keepdims=True)\n",
    "    if verbose:\n",
    "        print(colored(\"model response:\\n\",\"blue\"),y_model[:10])\n",
    "    #calculate the update value\n",
    "    delta = -alpha*np.sign(y_model - y_batch)*x_batch\n",
    "    if verbose:\n",
    "        print(colored(\"delta for each example:\\n\",\"blue\"),delta[:10])\n",
    "    #sum the update value over the examples provides\n",
    "    delta = np.mean(delta, axis=0)\n",
    "    if verbose:\n",
    "        print(colored(\"delta averaged over examples:\\n\",\"blue\"),delta)\n",
    "    #apply the update to theta\n",
    "    theta = theta + delta\n",
    "    if verbose:\n",
    "        print(colored(\"final theta:\\n\",\"blue\"), theta)\n",
    "    return theta \n",
    "    #END_SOLUTION\n",
    "\n",
    "#test the function on a small batch\n",
    "x_batch = x[:3]\n",
    "y_batch = y[:3]\n",
    "print(colored(\"X test:\\n\",\"blue\"),x_batch)\n",
    "print(colored(\"Y test:\\n\",\"blue\"),y_batch)\n",
    "\n",
    "#choose random initial parameters values\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "alpha = 1\n",
    "minDelta = 1E-3\n",
    "print(colored(\"\\nTest on random parameters\\n\",\"blue\"))\n",
    "batch_gradient_descent(x_batch, y_batch, theta_model, alpha, True)\n",
    "\n",
    "\n",
    "print(colored(\"\\nTest on nominal parameters\\n\",\"blue\"))\n",
    "batch_gradient_descent(x_batch, y_batch, theta, alpha, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639ec3c-e65d-4962-a7cd-94b8ac1b5a03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "2) modify the noise distribution from symmetric Gaussian  to assymetric exponetial (so mean and median have different values):\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y = \\theta_{0} + \\theta_{1} \\cdot x \\\\\n",
    "y_{with~noise} = Exp(\\mu=y)\n",
    "}\n",
    "$$\n",
    "\n",
    "and generate 15 000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97191ac5-0cdf-47aa-b500-29cdebfb46be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#generate the X sample\n",
    "nPoints = 15000\n",
    "x = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "\n",
    "# add the column of \"1\"\n",
    "ones = np.ones(x.shape[0])\n",
    "x = np.column_stack((ones, x))\n",
    "\n",
    "#calculate the mean values of Y \n",
    "y = np.sum(theta*x, axis=1).reshape((-1,1))\n",
    "\n",
    "#generate the data with noise\n",
    "#BEGIN_SOLUTION\n",
    "y_with_noise = np.random.default_rng().exponential(y)\n",
    "#END_SOLUTION\n",
    "\n",
    "print(colored(\"Trainig target shape:\",\"blue\"),y_with_noise.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d4883-f19c-4c63-bd2b-81e1e34ccf0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "3) perform training. Use `alpha=0.1`, `nEpochs = 20000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a266f3-ecac-40dc-8fe4-598118e6bf1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set the initial theta values to random numbers\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "#calculate the MAE loss with initial values\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "#BEGIN_SOLUTION\n",
    "loss = np.mean(np.abs(y_model - y_with_noise))\n",
    "#END_SOLUTION\n",
    "print(colored(\"Theta value before training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with initial parameters:\",\"blue\"), loss)\n",
    "\n",
    "#run the training loop\n",
    "alpha = 0.1\n",
    "nEpochs = 20000\n",
    "#BEGIN_SOLUTION\n",
    "for iEpoch in range(nEpochs):\n",
    "    x_batch = x\n",
    "    y_batch = y_with_noise\n",
    "    theta_model = batch_gradient_descent(x_batch, y_batch, theta_model, alpha)\n",
    "    y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "    loss = np.mean(np.abs(y_model - y_with_noise))\n",
    "#END_SOLUTION    \n",
    "    \n",
    "#print the MAE loss with final parameters    \n",
    "print(colored(\"Theta value after training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with final parameters:\",\"blue\"), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68053a0c-fd38-460a-8e18-d41b373f728e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "4) make control plots:\n",
    "* plot the model result on the training, noisy data\n",
    "* plot the model result on the test data - clean, without noise. The Y values of the clean dataset correspond to an average value of the noisy Y\n",
    "* plot clean data with Y values modified to show the median value: \n",
    "    \n",
    "$$ \n",
    "\\huge{\n",
    "y_{median} = \\ln(2) \\cdot y_{mean}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf74cf5-6ea5-44ad-8e83-ac47e0f6765b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "#plot the data points with noise\n",
    "ax[0].plot(x[:,1],y_with_noise, \"bo\", label=\"training data\")\n",
    "\n",
    "#plot the model results on the training data\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "ax[0].plot(x[:,1],y_model, \"rs\", label=\"model result\")\n",
    "\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].legend();\n",
    "\n",
    "#create the test dataset\n",
    "nPoints = 15\n",
    "x_test = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "x_test = np.reshape(x_test, (-1,1))\n",
    "ones = np.ones(x_test.shape[0])\n",
    "x_test = np.column_stack((ones, x_test))\n",
    "y_test = np.sum(theta*x_test, axis=1)\n",
    "\n",
    "#plot the data points for the test dataset. y values correspond to mean of the Y sample\n",
    "ax[1].plot(x_test[:,1],y_test, \"go\", label=\"mean target value\")\n",
    "\n",
    "#plot the y values corresponding to median of the Y sample\n",
    "#BEGIN_SOLUTION\n",
    "ax[1].plot(x_test[:,1],np.log(2)*y_test, \"bo\", label=\"median target value\")\n",
    "#END_SOLUTION\n",
    "\n",
    "#plot the model result on the test data\n",
    "y_model = np.sum(theta_model*x_test, axis=1, keepdims=True)\n",
    "ax[1].plot(x_test[:,1],y_model, \"rs\", label=\"model result\")\n",
    "\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
