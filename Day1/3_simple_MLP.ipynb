{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers separated by nonlinear components (also called *activation functions*). A nonlinear component in-between the linear layers is essential: without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. Also, please note that it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
    "\n",
    "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. \n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/snowakowski/venv/jupyter/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #1\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #2\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1306604762738429, 0.30810780385646264)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([2, 9, 9,  ..., 8, 6, 0])\n",
      "1 -th batch labels : tensor([9, 8, 6,  ..., 2, 3, 2])\n",
      "2 -th batch labels : tensor([8, 8, 0,  ..., 9, 6, 3])\n",
      "3 -th batch labels : tensor([6, 9, 3,  ..., 6, 8, 1])\n",
      "4 -th batch labels : tensor([2, 1, 6,  ..., 9, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your taks #3\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i==0:\n",
    "            print(i, \"-th batch inputs :\", batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
    "                                          #that output from a preceding layer serves as input to the next layer \n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.333528995513916\n",
      "epoch: 0 batch: 1 current batch loss: 2.435713052749634\n",
      "epoch: 0 batch: 2 current batch loss: 2.3393359184265137\n",
      "epoch: 0 batch: 3 current batch loss: 2.302150249481201\n",
      "epoch: 0 batch: 4 current batch loss: 2.279115915298462\n",
      "epoch: 0 batch: 5 current batch loss: 2.2784056663513184\n",
      "epoch: 0 batch: 6 current batch loss: 2.276996612548828\n",
      "epoch: 0 batch: 7 current batch loss: 2.2470388412475586\n",
      "epoch: 0 batch: 8 current batch loss: 2.207780599594116\n",
      "epoch: 0 batch: 9 current batch loss: 2.1458518505096436\n",
      "epoch: 0 batch: 10 current batch loss: 2.0947844982147217\n",
      "epoch: 0 batch: 11 current batch loss: 2.0498242378234863\n",
      "epoch: 0 batch: 12 current batch loss: 1.9697555303573608\n",
      "epoch: 0 batch: 13 current batch loss: 1.8804320096969604\n",
      "epoch: 0 batch: 14 current batch loss: 1.7955191135406494\n",
      "epoch: 0 batch: 15 current batch loss: 1.7175859212875366\n",
      "epoch: 0 batch: 16 current batch loss: 1.6202588081359863\n",
      "epoch: 0 batch: 17 current batch loss: 1.537744164466858\n",
      "epoch: 0 batch: 18 current batch loss: 1.4589673280715942\n",
      "epoch: 0 batch: 19 current batch loss: 1.403168797492981\n",
      "epoch: 0 batch: 20 current batch loss: 1.3240656852722168\n",
      "epoch: 0 batch: 21 current batch loss: 1.263817310333252\n",
      "epoch: 0 batch: 22 current batch loss: 1.224497675895691\n",
      "epoch: 0 batch: 23 current batch loss: 1.1636232137680054\n",
      "epoch: 0 batch: 24 current batch loss: 1.1586836576461792\n",
      "epoch: 0 batch: 25 current batch loss: 1.0849268436431885\n",
      "epoch: 0 batch: 26 current batch loss: 1.0186898708343506\n",
      "epoch: 0 batch: 27 current batch loss: 0.9924493432044983\n",
      "epoch: 0 batch: 28 current batch loss: 0.9343705773353577\n",
      "epoch: 0 batch: 29 current batch loss: 0.9327220916748047\n",
      "epoch: 1 batch: 0 current batch loss: 0.9098114371299744\n",
      "epoch: 1 batch: 1 current batch loss: 0.8801904916763306\n",
      "epoch: 1 batch: 2 current batch loss: 0.8097479343414307\n",
      "epoch: 1 batch: 3 current batch loss: 0.7876101732254028\n",
      "epoch: 1 batch: 4 current batch loss: 0.7699107527732849\n",
      "epoch: 1 batch: 5 current batch loss: 0.7218905687332153\n",
      "epoch: 1 batch: 6 current batch loss: 0.684557318687439\n",
      "epoch: 1 batch: 7 current batch loss: 0.6774870753288269\n",
      "epoch: 1 batch: 8 current batch loss: 0.668256938457489\n",
      "epoch: 1 batch: 9 current batch loss: 0.6474102735519409\n",
      "epoch: 1 batch: 10 current batch loss: 0.6347058415412903\n",
      "epoch: 1 batch: 11 current batch loss: 0.5963797569274902\n",
      "epoch: 1 batch: 12 current batch loss: 0.5809571146965027\n",
      "epoch: 1 batch: 13 current batch loss: 0.5727798342704773\n",
      "epoch: 1 batch: 14 current batch loss: 0.5736017227172852\n",
      "epoch: 1 batch: 15 current batch loss: 0.5120958089828491\n",
      "epoch: 1 batch: 16 current batch loss: 0.553791880607605\n",
      "epoch: 1 batch: 17 current batch loss: 0.5112139582633972\n",
      "epoch: 1 batch: 18 current batch loss: 0.4888373613357544\n",
      "epoch: 1 batch: 19 current batch loss: 0.4639084041118622\n",
      "epoch: 1 batch: 20 current batch loss: 0.45001956820487976\n",
      "epoch: 1 batch: 21 current batch loss: 0.44505077600479126\n",
      "epoch: 1 batch: 22 current batch loss: 0.44952458143234253\n",
      "epoch: 1 batch: 23 current batch loss: 0.4274042248725891\n",
      "epoch: 1 batch: 24 current batch loss: 0.4339788854122162\n",
      "epoch: 1 batch: 25 current batch loss: 0.4071514904499054\n",
      "epoch: 1 batch: 26 current batch loss: 0.4365348815917969\n",
      "epoch: 1 batch: 27 current batch loss: 0.37191087007522583\n",
      "epoch: 1 batch: 28 current batch loss: 0.3948463499546051\n",
      "epoch: 1 batch: 29 current batch loss: 0.39055827260017395\n",
      "epoch: 2 batch: 0 current batch loss: 0.36476099491119385\n",
      "epoch: 2 batch: 1 current batch loss: 0.3618010878562927\n",
      "epoch: 2 batch: 2 current batch loss: 0.3558690845966339\n",
      "epoch: 2 batch: 3 current batch loss: 0.3766185939311981\n",
      "epoch: 2 batch: 4 current batch loss: 0.3606005907058716\n",
      "epoch: 2 batch: 5 current batch loss: 0.35460996627807617\n",
      "epoch: 2 batch: 6 current batch loss: 0.3489651381969452\n",
      "epoch: 2 batch: 7 current batch loss: 0.3454268276691437\n",
      "epoch: 2 batch: 8 current batch loss: 0.3472530245780945\n",
      "epoch: 2 batch: 9 current batch loss: 0.3581397831439972\n",
      "epoch: 2 batch: 10 current batch loss: 0.32419055700302124\n",
      "epoch: 2 batch: 11 current batch loss: 0.34209752082824707\n",
      "epoch: 2 batch: 12 current batch loss: 0.32263171672821045\n",
      "epoch: 2 batch: 13 current batch loss: 0.3198153078556061\n",
      "epoch: 2 batch: 14 current batch loss: 0.33390483260154724\n",
      "epoch: 2 batch: 15 current batch loss: 0.3171769976615906\n",
      "epoch: 2 batch: 16 current batch loss: 0.32415062189102173\n",
      "epoch: 2 batch: 17 current batch loss: 0.2984462082386017\n",
      "epoch: 2 batch: 18 current batch loss: 0.29558178782463074\n",
      "epoch: 2 batch: 19 current batch loss: 0.2815052568912506\n",
      "epoch: 2 batch: 20 current batch loss: 0.2869506776332855\n",
      "epoch: 2 batch: 21 current batch loss: 0.30490347743034363\n",
      "epoch: 2 batch: 22 current batch loss: 0.303320050239563\n",
      "epoch: 2 batch: 23 current batch loss: 0.26002901792526245\n",
      "epoch: 2 batch: 24 current batch loss: 0.2546050250530243\n",
      "epoch: 2 batch: 25 current batch loss: 0.28271085023880005\n",
      "epoch: 2 batch: 26 current batch loss: 0.28703901171684265\n",
      "epoch: 2 batch: 27 current batch loss: 0.2791977524757385\n",
      "epoch: 2 batch: 28 current batch loss: 0.26160115003585815\n",
      "epoch: 2 batch: 29 current batch loss: 0.2585848867893219\n",
      "epoch: 3 batch: 0 current batch loss: 0.26787132024765015\n",
      "epoch: 3 batch: 1 current batch loss: 0.24866434931755066\n",
      "epoch: 3 batch: 2 current batch loss: 0.2299787998199463\n",
      "epoch: 3 batch: 3 current batch loss: 0.2410610169172287\n",
      "epoch: 3 batch: 4 current batch loss: 0.2397359162569046\n",
      "epoch: 3 batch: 5 current batch loss: 0.2607267200946808\n",
      "epoch: 3 batch: 6 current batch loss: 0.24065905809402466\n",
      "epoch: 3 batch: 7 current batch loss: 0.25257107615470886\n",
      "epoch: 3 batch: 8 current batch loss: 0.26263412833213806\n",
      "epoch: 3 batch: 9 current batch loss: 0.23905780911445618\n",
      "epoch: 3 batch: 10 current batch loss: 0.2280692160129547\n",
      "epoch: 3 batch: 11 current batch loss: 0.23322592675685883\n",
      "epoch: 3 batch: 12 current batch loss: 0.24165302515029907\n",
      "epoch: 3 batch: 13 current batch loss: 0.2517521381378174\n",
      "epoch: 3 batch: 14 current batch loss: 0.23313158750534058\n",
      "epoch: 3 batch: 15 current batch loss: 0.22126774489879608\n",
      "epoch: 3 batch: 16 current batch loss: 0.22165276110172272\n",
      "epoch: 3 batch: 17 current batch loss: 0.22001159191131592\n",
      "epoch: 3 batch: 18 current batch loss: 0.21406374871730804\n",
      "epoch: 3 batch: 19 current batch loss: 0.20657359063625336\n",
      "epoch: 3 batch: 20 current batch loss: 0.18933118879795074\n",
      "epoch: 3 batch: 21 current batch loss: 0.22147291898727417\n",
      "epoch: 3 batch: 22 current batch loss: 0.2265574038028717\n",
      "epoch: 3 batch: 23 current batch loss: 0.19140572845935822\n",
      "epoch: 3 batch: 24 current batch loss: 0.19697347283363342\n",
      "epoch: 3 batch: 25 current batch loss: 0.2080511748790741\n",
      "epoch: 3 batch: 26 current batch loss: 0.2399354726076126\n",
      "epoch: 3 batch: 27 current batch loss: 0.17552682757377625\n",
      "epoch: 3 batch: 28 current batch loss: 0.20315514504909515\n",
      "epoch: 3 batch: 29 current batch loss: 0.21151304244995117\n",
      "epoch: 4 batch: 0 current batch loss: 0.1728297919034958\n",
      "epoch: 4 batch: 1 current batch loss: 0.19340883195400238\n",
      "epoch: 4 batch: 2 current batch loss: 0.19515195488929749\n",
      "epoch: 4 batch: 3 current batch loss: 0.1764441728591919\n",
      "epoch: 4 batch: 4 current batch loss: 0.18683569133281708\n",
      "epoch: 4 batch: 5 current batch loss: 0.17138756811618805\n",
      "epoch: 4 batch: 6 current batch loss: 0.21735462546348572\n",
      "epoch: 4 batch: 7 current batch loss: 0.17562882602214813\n",
      "epoch: 4 batch: 8 current batch loss: 0.16892758011817932\n",
      "epoch: 4 batch: 9 current batch loss: 0.16594798862934113\n",
      "epoch: 4 batch: 10 current batch loss: 0.17574556171894073\n",
      "epoch: 4 batch: 11 current batch loss: 0.17197568714618683\n",
      "epoch: 4 batch: 12 current batch loss: 0.19333839416503906\n",
      "epoch: 4 batch: 13 current batch loss: 0.18018889427185059\n",
      "epoch: 4 batch: 14 current batch loss: 0.17895255982875824\n",
      "epoch: 4 batch: 15 current batch loss: 0.16859284043312073\n",
      "epoch: 4 batch: 16 current batch loss: 0.17531682550907135\n",
      "epoch: 4 batch: 17 current batch loss: 0.17348763346672058\n",
      "epoch: 4 batch: 18 current batch loss: 0.14836256206035614\n",
      "epoch: 4 batch: 19 current batch loss: 0.14809656143188477\n",
      "epoch: 4 batch: 20 current batch loss: 0.16931872069835663\n",
      "epoch: 4 batch: 21 current batch loss: 0.19349420070648193\n",
      "epoch: 4 batch: 22 current batch loss: 0.177787646651268\n",
      "epoch: 4 batch: 23 current batch loss: 0.1418352723121643\n",
      "epoch: 4 batch: 24 current batch loss: 0.15527620911598206\n",
      "epoch: 4 batch: 25 current batch loss: 0.18372742831707\n",
      "epoch: 4 batch: 26 current batch loss: 0.16416922211647034\n",
      "epoch: 4 batch: 27 current batch loss: 0.1663382351398468\n",
      "epoch: 4 batch: 28 current batch loss: 0.15885576605796814\n",
      "epoch: 4 batch: 29 current batch loss: 0.14243662357330322\n",
      "epoch: 5 batch: 0 current batch loss: 0.14090226590633392\n",
      "epoch: 5 batch: 1 current batch loss: 0.17603231966495514\n",
      "epoch: 5 batch: 2 current batch loss: 0.1360226422548294\n",
      "epoch: 5 batch: 3 current batch loss: 0.15349321067333221\n",
      "epoch: 5 batch: 4 current batch loss: 0.15555907785892487\n",
      "epoch: 5 batch: 5 current batch loss: 0.15536174178123474\n",
      "epoch: 5 batch: 6 current batch loss: 0.13425637781620026\n",
      "epoch: 5 batch: 7 current batch loss: 0.1330057829618454\n",
      "epoch: 5 batch: 8 current batch loss: 0.15379954874515533\n",
      "epoch: 5 batch: 9 current batch loss: 0.1433769017457962\n",
      "epoch: 5 batch: 10 current batch loss: 0.14243346452713013\n",
      "epoch: 5 batch: 11 current batch loss: 0.1528603583574295\n",
      "epoch: 5 batch: 12 current batch loss: 0.16777214407920837\n",
      "epoch: 5 batch: 13 current batch loss: 0.13499513268470764\n",
      "epoch: 5 batch: 14 current batch loss: 0.14215071499347687\n",
      "epoch: 5 batch: 15 current batch loss: 0.11201891303062439\n",
      "epoch: 5 batch: 16 current batch loss: 0.1406361162662506\n",
      "epoch: 5 batch: 17 current batch loss: 0.13826926052570343\n",
      "epoch: 5 batch: 18 current batch loss: 0.11776085197925568\n",
      "epoch: 5 batch: 19 current batch loss: 0.136415496468544\n",
      "epoch: 5 batch: 20 current batch loss: 0.1402420997619629\n",
      "epoch: 5 batch: 21 current batch loss: 0.15491987764835358\n",
      "epoch: 5 batch: 22 current batch loss: 0.13804484903812408\n",
      "epoch: 5 batch: 23 current batch loss: 0.13594309985637665\n",
      "epoch: 5 batch: 24 current batch loss: 0.15021218359470367\n",
      "epoch: 5 batch: 25 current batch loss: 0.13391868770122528\n",
      "epoch: 5 batch: 26 current batch loss: 0.11567369103431702\n",
      "epoch: 5 batch: 27 current batch loss: 0.11140396445989609\n",
      "epoch: 5 batch: 28 current batch loss: 0.1358528882265091\n",
      "epoch: 5 batch: 29 current batch loss: 0.13922403752803802\n",
      "epoch: 6 batch: 0 current batch loss: 0.10261630266904831\n",
      "epoch: 6 batch: 1 current batch loss: 0.1153293177485466\n",
      "epoch: 6 batch: 2 current batch loss: 0.12577635049819946\n",
      "epoch: 6 batch: 3 current batch loss: 0.12149390578269958\n",
      "epoch: 6 batch: 4 current batch loss: 0.1088133230805397\n",
      "epoch: 6 batch: 5 current batch loss: 0.10813908278942108\n",
      "epoch: 6 batch: 6 current batch loss: 0.13131776452064514\n",
      "epoch: 6 batch: 7 current batch loss: 0.11854740232229233\n",
      "epoch: 6 batch: 8 current batch loss: 0.10997068881988525\n",
      "epoch: 6 batch: 9 current batch loss: 0.09544633328914642\n",
      "epoch: 6 batch: 10 current batch loss: 0.10382172465324402\n",
      "epoch: 6 batch: 11 current batch loss: 0.09896642714738846\n",
      "epoch: 6 batch: 12 current batch loss: 0.10842704027891159\n",
      "epoch: 6 batch: 13 current batch loss: 0.11623352020978928\n",
      "epoch: 6 batch: 14 current batch loss: 0.11111455410718918\n",
      "epoch: 6 batch: 15 current batch loss: 0.12629221379756927\n",
      "epoch: 6 batch: 16 current batch loss: 0.11595828086137772\n",
      "epoch: 6 batch: 17 current batch loss: 0.09092383831739426\n",
      "epoch: 6 batch: 18 current batch loss: 0.09798623621463776\n",
      "epoch: 6 batch: 19 current batch loss: 0.12104982882738113\n",
      "epoch: 6 batch: 20 current batch loss: 0.11038427799940109\n",
      "epoch: 6 batch: 21 current batch loss: 0.12641821801662445\n",
      "epoch: 6 batch: 22 current batch loss: 0.11625070124864578\n",
      "epoch: 6 batch: 23 current batch loss: 0.12233441323041916\n",
      "epoch: 6 batch: 24 current batch loss: 0.13472022116184235\n",
      "epoch: 6 batch: 25 current batch loss: 0.11150126159191132\n",
      "epoch: 6 batch: 26 current batch loss: 0.10652562975883484\n",
      "epoch: 6 batch: 27 current batch loss: 0.12398993223905563\n",
      "epoch: 6 batch: 28 current batch loss: 0.11312438547611237\n",
      "epoch: 6 batch: 29 current batch loss: 0.0815335214138031\n",
      "epoch: 7 batch: 0 current batch loss: 0.1057690754532814\n",
      "epoch: 7 batch: 1 current batch loss: 0.0944909080862999\n",
      "epoch: 7 batch: 2 current batch loss: 0.0956658273935318\n",
      "epoch: 7 batch: 3 current batch loss: 0.10931402444839478\n",
      "epoch: 7 batch: 4 current batch loss: 0.10716221481561661\n",
      "epoch: 7 batch: 5 current batch loss: 0.10289669781923294\n",
      "epoch: 7 batch: 6 current batch loss: 0.09346325695514679\n",
      "epoch: 7 batch: 7 current batch loss: 0.09860055148601532\n",
      "epoch: 7 batch: 8 current batch loss: 0.10548251867294312\n",
      "epoch: 7 batch: 9 current batch loss: 0.08808687329292297\n",
      "epoch: 7 batch: 10 current batch loss: 0.10466522723436356\n",
      "epoch: 7 batch: 11 current batch loss: 0.1111452579498291\n",
      "epoch: 7 batch: 12 current batch loss: 0.08329173177480698\n",
      "epoch: 7 batch: 13 current batch loss: 0.09702713042497635\n",
      "epoch: 7 batch: 14 current batch loss: 0.09451943635940552\n",
      "epoch: 7 batch: 15 current batch loss: 0.08534571528434753\n",
      "epoch: 7 batch: 16 current batch loss: 0.12018274515867233\n",
      "epoch: 7 batch: 17 current batch loss: 0.10165838152170181\n",
      "epoch: 7 batch: 18 current batch loss: 0.07446437329053879\n",
      "epoch: 7 batch: 19 current batch loss: 0.07655340433120728\n",
      "epoch: 7 batch: 20 current batch loss: 0.10364963859319687\n",
      "epoch: 7 batch: 21 current batch loss: 0.08585398644208908\n",
      "epoch: 7 batch: 22 current batch loss: 0.0757850930094719\n",
      "epoch: 7 batch: 23 current batch loss: 0.08255944401025772\n",
      "epoch: 7 batch: 24 current batch loss: 0.09008897095918655\n",
      "epoch: 7 batch: 25 current batch loss: 0.09081532806158066\n",
      "epoch: 7 batch: 26 current batch loss: 0.09651236236095428\n",
      "epoch: 7 batch: 27 current batch loss: 0.09550689905881882\n",
      "epoch: 7 batch: 28 current batch loss: 0.09367290139198303\n",
      "epoch: 7 batch: 29 current batch loss: 0.10221932828426361\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
    "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #4\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
   "metadata": {},
   "source": [
    "### Answers to task #4\n",
    "\n",
    "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
    "\n",
    "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - the second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.327831268310547 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.4297287464141846 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.3682901859283447 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.311244487762451 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.300467014312744 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.2878031730651855 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.2710886001586914 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.245722532272339 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.2093873023986816 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.1617181301116943 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.100027084350586 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.0292587280273438 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 1.962149977684021 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.8682949542999268 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.767924189567566 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.6790010929107666 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.6141891479492188 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.5267218351364136 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.4632099866867065 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.3821595907211304 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.330039381980896 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.305220365524292 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.2256758213043213 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.1697553396224976 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.1288373470306396 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.0760126113891602 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 1.037084937095642 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 0.9908576011657715 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 0.966318666934967 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.9642033576965332 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.9360055327415466 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.8789081573486328 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.8836690187454224 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.8195772767066956 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.8224940299987793 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.8078434467315674 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.7375063300132751 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.755325973033905 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.6606530547142029 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.7014908194541931 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.6758426427841187 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 11 current batch loss: 0.6603809595108032 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 12 current batch loss: 0.6340500712394714 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 13 current batch loss: 0.6325573325157166 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 14 current batch loss: 0.5752829313278198 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 15 current batch loss: 0.5553491711616516 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 16 current batch loss: 0.5573400855064392 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 17 current batch loss: 0.5253496766090393 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 18 current batch loss: 0.5223797559738159 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 19 current batch loss: 0.5360960960388184 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 20 current batch loss: 0.4957752525806427 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 21 current batch loss: 0.48997029662132263 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 22 current batch loss: 0.5169398784637451 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 23 current batch loss: 0.44925928115844727 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 24 current batch loss: 0.44125139713287354 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 25 current batch loss: 0.4448681175708771 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 26 current batch loss: 0.45971405506134033 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 27 current batch loss: 0.4252898693084717 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 28 current batch loss: 0.4189438223838806 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 29 current batch loss: 0.4558197259902954 current lr: 0.0009000000000000001\n",
      "epoch: 2 batch: 0 current batch loss: 0.4083781838417053 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 1 current batch loss: 0.3893350660800934 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 2 current batch loss: 0.3708503842353821 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 3 current batch loss: 0.3382035493850708 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 4 current batch loss: 0.3547408878803253 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 5 current batch loss: 0.359592467546463 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 6 current batch loss: 0.3530708849430084 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 7 current batch loss: 0.3667559027671814 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 8 current batch loss: 0.38671666383743286 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 9 current batch loss: 0.3286392390727997 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 10 current batch loss: 0.3358301520347595 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 11 current batch loss: 0.36382541060447693 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 12 current batch loss: 0.34033629298210144 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 13 current batch loss: 0.3601754307746887 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 14 current batch loss: 0.3369075357913971 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 15 current batch loss: 0.3303307890892029 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 16 current batch loss: 0.3320919871330261 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 17 current batch loss: 0.3124583065509796 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 18 current batch loss: 0.3156089186668396 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 19 current batch loss: 0.3003097474575043 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 20 current batch loss: 0.2934436500072479 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 21 current batch loss: 0.326820969581604 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 22 current batch loss: 0.27504706382751465 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 23 current batch loss: 0.29755374789237976 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 24 current batch loss: 0.2964424788951874 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 25 current batch loss: 0.31748443841934204 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 26 current batch loss: 0.3073490262031555 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 27 current batch loss: 0.2898050546646118 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 28 current batch loss: 0.3046455681324005 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 29 current batch loss: 0.27755555510520935 current lr: 0.0008100000000000001\n",
      "epoch: 3 batch: 0 current batch loss: 0.27144870162010193 current lr: 0.000729\n",
      "epoch: 3 batch: 1 current batch loss: 0.2572464346885681 current lr: 0.000729\n",
      "epoch: 3 batch: 2 current batch loss: 0.2509319484233856 current lr: 0.000729\n",
      "epoch: 3 batch: 3 current batch loss: 0.27529430389404297 current lr: 0.000729\n",
      "epoch: 3 batch: 4 current batch loss: 0.26335281133651733 current lr: 0.000729\n",
      "epoch: 3 batch: 5 current batch loss: 0.23837701976299286 current lr: 0.000729\n",
      "epoch: 3 batch: 6 current batch loss: 0.29051464796066284 current lr: 0.000729\n",
      "epoch: 3 batch: 7 current batch loss: 0.2722322940826416 current lr: 0.000729\n",
      "epoch: 3 batch: 8 current batch loss: 0.23377396166324615 current lr: 0.000729\n",
      "epoch: 3 batch: 9 current batch loss: 0.23851454257965088 current lr: 0.000729\n",
      "epoch: 3 batch: 10 current batch loss: 0.27574655413627625 current lr: 0.000729\n",
      "epoch: 3 batch: 11 current batch loss: 0.2717793881893158 current lr: 0.000729\n",
      "epoch: 3 batch: 12 current batch loss: 0.24621091783046722 current lr: 0.000729\n",
      "epoch: 3 batch: 13 current batch loss: 0.26317182183265686 current lr: 0.000729\n",
      "epoch: 3 batch: 14 current batch loss: 0.2210598587989807 current lr: 0.000729\n",
      "epoch: 3 batch: 15 current batch loss: 0.24726572632789612 current lr: 0.000729\n",
      "epoch: 3 batch: 16 current batch loss: 0.24656134843826294 current lr: 0.000729\n",
      "epoch: 3 batch: 17 current batch loss: 0.23622547090053558 current lr: 0.000729\n",
      "epoch: 3 batch: 18 current batch loss: 0.23598574101924896 current lr: 0.000729\n",
      "epoch: 3 batch: 19 current batch loss: 0.23014310002326965 current lr: 0.000729\n",
      "epoch: 3 batch: 20 current batch loss: 0.23819053173065186 current lr: 0.000729\n",
      "epoch: 3 batch: 21 current batch loss: 0.2626447081565857 current lr: 0.000729\n",
      "epoch: 3 batch: 22 current batch loss: 0.24229367077350616 current lr: 0.000729\n",
      "epoch: 3 batch: 23 current batch loss: 0.24244727194309235 current lr: 0.000729\n",
      "epoch: 3 batch: 24 current batch loss: 0.248185932636261 current lr: 0.000729\n",
      "epoch: 3 batch: 25 current batch loss: 0.24667097628116608 current lr: 0.000729\n",
      "epoch: 3 batch: 26 current batch loss: 0.24329029023647308 current lr: 0.000729\n",
      "epoch: 3 batch: 27 current batch loss: 0.23829777538776398 current lr: 0.000729\n",
      "epoch: 3 batch: 28 current batch loss: 0.21501831710338593 current lr: 0.000729\n",
      "epoch: 3 batch: 29 current batch loss: 0.18076734244823456 current lr: 0.000729\n",
      "epoch: 4 batch: 0 current batch loss: 0.21997858583927155 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 1 current batch loss: 0.2295507788658142 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 2 current batch loss: 0.20594792068004608 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 3 current batch loss: 0.21414552628993988 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 4 current batch loss: 0.20428477227687836 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 5 current batch loss: 0.21132823824882507 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 6 current batch loss: 0.22425638139247894 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 7 current batch loss: 0.21250322461128235 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 8 current batch loss: 0.2094913274049759 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 9 current batch loss: 0.19632785022258759 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 10 current batch loss: 0.18922361731529236 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 11 current batch loss: 0.19908006489276886 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 12 current batch loss: 0.18363197147846222 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 13 current batch loss: 0.2102237045764923 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 14 current batch loss: 0.19367508590221405 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 15 current batch loss: 0.2186008244752884 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 16 current batch loss: 0.2093663513660431 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 17 current batch loss: 0.17763887345790863 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 18 current batch loss: 0.16915184259414673 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 19 current batch loss: 0.17597605288028717 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 20 current batch loss: 0.19150730967521667 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 21 current batch loss: 0.1914857029914856 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 22 current batch loss: 0.1943235844373703 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 23 current batch loss: 0.18702831864356995 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 24 current batch loss: 0.18176695704460144 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 25 current batch loss: 0.2135278284549713 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 26 current batch loss: 0.21827426552772522 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 27 current batch loss: 0.19820992648601532 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 28 current batch loss: 0.19289614260196686 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 29 current batch loss: 0.1560620367527008 current lr: 0.0006561000000000001\n",
      "epoch: 5 batch: 0 current batch loss: 0.19187138974666595 current lr: 0.00059049\n",
      "epoch: 5 batch: 1 current batch loss: 0.16673719882965088 current lr: 0.00059049\n",
      "epoch: 5 batch: 2 current batch loss: 0.16640886664390564 current lr: 0.00059049\n",
      "epoch: 5 batch: 3 current batch loss: 0.18649132549762726 current lr: 0.00059049\n",
      "epoch: 5 batch: 4 current batch loss: 0.20510217547416687 current lr: 0.00059049\n",
      "epoch: 5 batch: 5 current batch loss: 0.14323830604553223 current lr: 0.00059049\n",
      "epoch: 5 batch: 6 current batch loss: 0.15976481139659882 current lr: 0.00059049\n",
      "epoch: 5 batch: 7 current batch loss: 0.19752848148345947 current lr: 0.00059049\n",
      "epoch: 5 batch: 8 current batch loss: 0.20085830986499786 current lr: 0.00059049\n",
      "epoch: 5 batch: 9 current batch loss: 0.17221367359161377 current lr: 0.00059049\n",
      "epoch: 5 batch: 10 current batch loss: 0.15567666292190552 current lr: 0.00059049\n",
      "epoch: 5 batch: 11 current batch loss: 0.16227291524410248 current lr: 0.00059049\n",
      "epoch: 5 batch: 12 current batch loss: 0.15846747159957886 current lr: 0.00059049\n",
      "epoch: 5 batch: 13 current batch loss: 0.16816842555999756 current lr: 0.00059049\n",
      "epoch: 5 batch: 14 current batch loss: 0.16757699847221375 current lr: 0.00059049\n",
      "epoch: 5 batch: 15 current batch loss: 0.1613049954175949 current lr: 0.00059049\n",
      "epoch: 5 batch: 16 current batch loss: 0.1828220784664154 current lr: 0.00059049\n",
      "epoch: 5 batch: 17 current batch loss: 0.18611657619476318 current lr: 0.00059049\n",
      "epoch: 5 batch: 18 current batch loss: 0.15050944685935974 current lr: 0.00059049\n",
      "epoch: 5 batch: 19 current batch loss: 0.1631021499633789 current lr: 0.00059049\n",
      "epoch: 5 batch: 20 current batch loss: 0.16718518733978271 current lr: 0.00059049\n",
      "epoch: 5 batch: 21 current batch loss: 0.17742154002189636 current lr: 0.00059049\n",
      "epoch: 5 batch: 22 current batch loss: 0.1542052924633026 current lr: 0.00059049\n",
      "epoch: 5 batch: 23 current batch loss: 0.16960416734218597 current lr: 0.00059049\n",
      "epoch: 5 batch: 24 current batch loss: 0.15159030258655548 current lr: 0.00059049\n",
      "epoch: 5 batch: 25 current batch loss: 0.1642795354127884 current lr: 0.00059049\n",
      "epoch: 5 batch: 26 current batch loss: 0.1460157036781311 current lr: 0.00059049\n",
      "epoch: 5 batch: 27 current batch loss: 0.15906447172164917 current lr: 0.00059049\n",
      "epoch: 5 batch: 28 current batch loss: 0.16119559109210968 current lr: 0.00059049\n",
      "epoch: 5 batch: 29 current batch loss: 0.16419808566570282 current lr: 0.00059049\n",
      "epoch: 6 batch: 0 current batch loss: 0.15524330735206604 current lr: 0.000531441\n",
      "epoch: 6 batch: 1 current batch loss: 0.13868418335914612 current lr: 0.000531441\n",
      "epoch: 6 batch: 2 current batch loss: 0.1465563327074051 current lr: 0.000531441\n",
      "epoch: 6 batch: 3 current batch loss: 0.15462124347686768 current lr: 0.000531441\n",
      "epoch: 6 batch: 4 current batch loss: 0.12449026107788086 current lr: 0.000531441\n",
      "epoch: 6 batch: 5 current batch loss: 0.1421516388654709 current lr: 0.000531441\n",
      "epoch: 6 batch: 6 current batch loss: 0.13969770073890686 current lr: 0.000531441\n",
      "epoch: 6 batch: 7 current batch loss: 0.13670317828655243 current lr: 0.000531441\n",
      "epoch: 6 batch: 8 current batch loss: 0.15151822566986084 current lr: 0.000531441\n",
      "epoch: 6 batch: 9 current batch loss: 0.17721502482891083 current lr: 0.000531441\n",
      "epoch: 6 batch: 10 current batch loss: 0.1374194622039795 current lr: 0.000531441\n",
      "epoch: 6 batch: 11 current batch loss: 0.12616737186908722 current lr: 0.000531441\n",
      "epoch: 6 batch: 12 current batch loss: 0.14614132046699524 current lr: 0.000531441\n",
      "epoch: 6 batch: 13 current batch loss: 0.1383081078529358 current lr: 0.000531441\n",
      "epoch: 6 batch: 14 current batch loss: 0.14565765857696533 current lr: 0.000531441\n",
      "epoch: 6 batch: 15 current batch loss: 0.14198464155197144 current lr: 0.000531441\n",
      "epoch: 6 batch: 16 current batch loss: 0.15320222079753876 current lr: 0.000531441\n",
      "epoch: 6 batch: 17 current batch loss: 0.14949028193950653 current lr: 0.000531441\n",
      "epoch: 6 batch: 18 current batch loss: 0.13694627583026886 current lr: 0.000531441\n",
      "epoch: 6 batch: 19 current batch loss: 0.16081586480140686 current lr: 0.000531441\n",
      "epoch: 6 batch: 20 current batch loss: 0.1389104127883911 current lr: 0.000531441\n",
      "epoch: 6 batch: 21 current batch loss: 0.163263201713562 current lr: 0.000531441\n",
      "epoch: 6 batch: 22 current batch loss: 0.12848851084709167 current lr: 0.000531441\n",
      "epoch: 6 batch: 23 current batch loss: 0.15905356407165527 current lr: 0.000531441\n",
      "epoch: 6 batch: 24 current batch loss: 0.14775779843330383 current lr: 0.000531441\n",
      "epoch: 6 batch: 25 current batch loss: 0.14878877997398376 current lr: 0.000531441\n",
      "epoch: 6 batch: 26 current batch loss: 0.1623593270778656 current lr: 0.000531441\n",
      "epoch: 6 batch: 27 current batch loss: 0.13938407599925995 current lr: 0.000531441\n",
      "epoch: 6 batch: 28 current batch loss: 0.1439828872680664 current lr: 0.000531441\n",
      "epoch: 6 batch: 29 current batch loss: 0.13966412842273712 current lr: 0.000531441\n",
      "epoch: 7 batch: 0 current batch loss: 0.13635294139385223 current lr: 0.0004782969\n",
      "epoch: 7 batch: 1 current batch loss: 0.12109027057886124 current lr: 0.0004782969\n",
      "epoch: 7 batch: 2 current batch loss: 0.140532448887825 current lr: 0.0004782969\n",
      "epoch: 7 batch: 3 current batch loss: 0.1274127960205078 current lr: 0.0004782969\n",
      "epoch: 7 batch: 4 current batch loss: 0.1523759663105011 current lr: 0.0004782969\n",
      "epoch: 7 batch: 5 current batch loss: 0.12383486330509186 current lr: 0.0004782969\n",
      "epoch: 7 batch: 6 current batch loss: 0.10736574232578278 current lr: 0.0004782969\n",
      "epoch: 7 batch: 7 current batch loss: 0.11709514260292053 current lr: 0.0004782969\n",
      "epoch: 7 batch: 8 current batch loss: 0.127617746591568 current lr: 0.0004782969\n",
      "epoch: 7 batch: 9 current batch loss: 0.12645702064037323 current lr: 0.0004782969\n",
      "epoch: 7 batch: 10 current batch loss: 0.1444474309682846 current lr: 0.0004782969\n",
      "epoch: 7 batch: 11 current batch loss: 0.13510601222515106 current lr: 0.0004782969\n",
      "epoch: 7 batch: 12 current batch loss: 0.13850079476833344 current lr: 0.0004782969\n",
      "epoch: 7 batch: 13 current batch loss: 0.12214786559343338 current lr: 0.0004782969\n",
      "epoch: 7 batch: 14 current batch loss: 0.10524111241102219 current lr: 0.0004782969\n",
      "epoch: 7 batch: 15 current batch loss: 0.12959547340869904 current lr: 0.0004782969\n",
      "epoch: 7 batch: 16 current batch loss: 0.1341860294342041 current lr: 0.0004782969\n",
      "epoch: 7 batch: 17 current batch loss: 0.12991659343242645 current lr: 0.0004782969\n",
      "epoch: 7 batch: 18 current batch loss: 0.13132624328136444 current lr: 0.0004782969\n",
      "epoch: 7 batch: 19 current batch loss: 0.1294005811214447 current lr: 0.0004782969\n",
      "epoch: 7 batch: 20 current batch loss: 0.14701592922210693 current lr: 0.0004782969\n",
      "epoch: 7 batch: 21 current batch loss: 0.14242307841777802 current lr: 0.0004782969\n",
      "epoch: 7 batch: 22 current batch loss: 0.10854847729206085 current lr: 0.0004782969\n",
      "epoch: 7 batch: 23 current batch loss: 0.11932138353586197 current lr: 0.0004782969\n",
      "epoch: 7 batch: 24 current batch loss: 0.10544142127037048 current lr: 0.0004782969\n",
      "epoch: 7 batch: 25 current batch loss: 0.125599205493927 current lr: 0.0004782969\n",
      "epoch: 7 batch: 26 current batch loss: 0.1337081640958786 current lr: 0.0004782969\n",
      "epoch: 7 batch: 27 current batch loss: 0.13046550750732422 current lr: 0.0004782969\n",
      "epoch: 7 batch: 28 current batch loss: 0.11837772279977798 current lr: 0.0004782969\n",
      "epoch: 7 batch: 29 current batch loss: 0.15354721248149872 current lr: 0.0004782969\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #5\n",
    "\n",
    "Well, it seems that we were able to get the learning rate much below 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) Some other schedulers you might want to experiment with: \n",
    "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
    "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #6\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9703\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9603\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5c6ca-1055-4d9f-b6e0-3e4601f3919d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
