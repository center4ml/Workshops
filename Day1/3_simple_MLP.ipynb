{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers \n",
    "\n",
    "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
    "\n",
    "Linear layers must be separated by nonlinear components (also called *activation functions*). \n",
    "\n",
    "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #1\n",
    "\n",
    "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
    "\n",
    "### Answers to task #1\n",
    "A nonlinear component in-between the linear layers is essential: \n",
    "1. without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. \n",
    "2. it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. MNIST stands for Modified National Institute of Standards and Technology database.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #2\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #3\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1306604762738429, 0.30810780385646264)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([1, 6, 3,  ..., 7, 9, 5])\n",
      "1 -th batch labels : tensor([2, 8, 6,  ..., 7, 2, 8])\n",
      "2 -th batch labels : tensor([1, 6, 4,  ..., 9, 5, 5])\n",
      "3 -th batch labels : tensor([8, 4, 8,  ..., 7, 3, 1])\n",
      "4 -th batch labels : tensor([1, 6, 4,  ..., 1, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your taks #4\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i==0:\n",
    "            print(i, \"-th batch inputs :\", batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
    "                                          #that output from a preceding layer serves as input to the next layer \n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3365862369537354\n",
      "epoch: 0 batch: 1 current batch loss: 2.4079718589782715\n",
      "epoch: 0 batch: 2 current batch loss: 2.3658342361450195\n",
      "epoch: 0 batch: 3 current batch loss: 2.3228752613067627\n",
      "epoch: 0 batch: 4 current batch loss: 2.278453826904297\n",
      "epoch: 0 batch: 5 current batch loss: 2.274646759033203\n",
      "epoch: 0 batch: 6 current batch loss: 2.270784854888916\n",
      "epoch: 0 batch: 7 current batch loss: 2.247797966003418\n",
      "epoch: 0 batch: 8 current batch loss: 2.209421396255493\n",
      "epoch: 0 batch: 9 current batch loss: 2.145573139190674\n",
      "epoch: 0 batch: 10 current batch loss: 2.080878496170044\n",
      "epoch: 0 batch: 11 current batch loss: 2.0131282806396484\n",
      "epoch: 0 batch: 12 current batch loss: 1.9361399412155151\n",
      "epoch: 0 batch: 13 current batch loss: 1.8556175231933594\n",
      "epoch: 0 batch: 14 current batch loss: 1.768169641494751\n",
      "epoch: 0 batch: 15 current batch loss: 1.6696536540985107\n",
      "epoch: 0 batch: 16 current batch loss: 1.5995286703109741\n",
      "epoch: 0 batch: 17 current batch loss: 1.5043600797653198\n",
      "epoch: 0 batch: 18 current batch loss: 1.4419023990631104\n",
      "epoch: 0 batch: 19 current batch loss: 1.3612477779388428\n",
      "epoch: 0 batch: 20 current batch loss: 1.3399089574813843\n",
      "epoch: 0 batch: 21 current batch loss: 1.2969154119491577\n",
      "epoch: 0 batch: 22 current batch loss: 1.2133389711380005\n",
      "epoch: 0 batch: 23 current batch loss: 1.1582611799240112\n",
      "epoch: 0 batch: 24 current batch loss: 1.1083930730819702\n",
      "epoch: 0 batch: 25 current batch loss: 1.063963532447815\n",
      "epoch: 0 batch: 26 current batch loss: 1.0570396184921265\n",
      "epoch: 0 batch: 27 current batch loss: 1.003433346748352\n",
      "epoch: 0 batch: 28 current batch loss: 0.9508371353149414\n",
      "epoch: 0 batch: 29 current batch loss: 0.949578046798706\n",
      "epoch: 1 batch: 0 current batch loss: 0.8618167638778687\n",
      "epoch: 1 batch: 1 current batch loss: 0.8502396941184998\n",
      "epoch: 1 batch: 2 current batch loss: 0.832732617855072\n",
      "epoch: 1 batch: 3 current batch loss: 0.7735754251480103\n",
      "epoch: 1 batch: 4 current batch loss: 0.7832766175270081\n",
      "epoch: 1 batch: 5 current batch loss: 0.7557021379470825\n",
      "epoch: 1 batch: 6 current batch loss: 0.7428678870201111\n",
      "epoch: 1 batch: 7 current batch loss: 0.7242282629013062\n",
      "epoch: 1 batch: 8 current batch loss: 0.7044292092323303\n",
      "epoch: 1 batch: 9 current batch loss: 0.6427560448646545\n",
      "epoch: 1 batch: 10 current batch loss: 0.6550149321556091\n",
      "epoch: 1 batch: 11 current batch loss: 0.6232375502586365\n",
      "epoch: 1 batch: 12 current batch loss: 0.5822619795799255\n",
      "epoch: 1 batch: 13 current batch loss: 0.5685968399047852\n",
      "epoch: 1 batch: 14 current batch loss: 0.5669847726821899\n",
      "epoch: 1 batch: 15 current batch loss: 0.5702755451202393\n",
      "epoch: 1 batch: 16 current batch loss: 0.5418381690979004\n",
      "epoch: 1 batch: 17 current batch loss: 0.4966864287853241\n",
      "epoch: 1 batch: 18 current batch loss: 0.5154799222946167\n",
      "epoch: 1 batch: 19 current batch loss: 0.5132567882537842\n",
      "epoch: 1 batch: 20 current batch loss: 0.5081956386566162\n",
      "epoch: 1 batch: 21 current batch loss: 0.468178927898407\n",
      "epoch: 1 batch: 22 current batch loss: 0.4815860390663147\n",
      "epoch: 1 batch: 23 current batch loss: 0.45874834060668945\n",
      "epoch: 1 batch: 24 current batch loss: 0.45977792143821716\n",
      "epoch: 1 batch: 25 current batch loss: 0.4271722137928009\n",
      "epoch: 1 batch: 26 current batch loss: 0.4169112741947174\n",
      "epoch: 1 batch: 27 current batch loss: 0.43697822093963623\n",
      "epoch: 1 batch: 28 current batch loss: 0.3855181932449341\n",
      "epoch: 1 batch: 29 current batch loss: 0.4640432298183441\n",
      "epoch: 2 batch: 0 current batch loss: 0.37744879722595215\n",
      "epoch: 2 batch: 1 current batch loss: 0.3738678991794586\n",
      "epoch: 2 batch: 2 current batch loss: 0.3721514940261841\n",
      "epoch: 2 batch: 3 current batch loss: 0.3453090488910675\n",
      "epoch: 2 batch: 4 current batch loss: 0.3609238266944885\n",
      "epoch: 2 batch: 5 current batch loss: 0.3681008517742157\n",
      "epoch: 2 batch: 6 current batch loss: 0.3613240420818329\n",
      "epoch: 2 batch: 7 current batch loss: 0.3657866418361664\n",
      "epoch: 2 batch: 8 current batch loss: 0.3669528067111969\n",
      "epoch: 2 batch: 9 current batch loss: 0.3379411995410919\n",
      "epoch: 2 batch: 10 current batch loss: 0.3377550542354584\n",
      "epoch: 2 batch: 11 current batch loss: 0.3184068500995636\n",
      "epoch: 2 batch: 12 current batch loss: 0.3440219759941101\n",
      "epoch: 2 batch: 13 current batch loss: 0.3414563834667206\n",
      "epoch: 2 batch: 14 current batch loss: 0.3315122723579407\n",
      "epoch: 2 batch: 15 current batch loss: 0.3284071683883667\n",
      "epoch: 2 batch: 16 current batch loss: 0.33759987354278564\n",
      "epoch: 2 batch: 17 current batch loss: 0.33212345838546753\n",
      "epoch: 2 batch: 18 current batch loss: 0.2934708297252655\n",
      "epoch: 2 batch: 19 current batch loss: 0.3184148967266083\n",
      "epoch: 2 batch: 20 current batch loss: 0.2932499051094055\n",
      "epoch: 2 batch: 21 current batch loss: 0.28985387086868286\n",
      "epoch: 2 batch: 22 current batch loss: 0.27496132254600525\n",
      "epoch: 2 batch: 23 current batch loss: 0.27987128496170044\n",
      "epoch: 2 batch: 24 current batch loss: 0.26912054419517517\n",
      "epoch: 2 batch: 25 current batch loss: 0.27244940400123596\n",
      "epoch: 2 batch: 26 current batch loss: 0.28263479471206665\n",
      "epoch: 2 batch: 27 current batch loss: 0.28761452436447144\n",
      "epoch: 2 batch: 28 current batch loss: 0.2487213909626007\n",
      "epoch: 2 batch: 29 current batch loss: 0.2851274609565735\n",
      "epoch: 3 batch: 0 current batch loss: 0.2582232654094696\n",
      "epoch: 3 batch: 1 current batch loss: 0.2539202570915222\n",
      "epoch: 3 batch: 2 current batch loss: 0.26403987407684326\n",
      "epoch: 3 batch: 3 current batch loss: 0.2641444504261017\n",
      "epoch: 3 batch: 4 current batch loss: 0.25981900095939636\n",
      "epoch: 3 batch: 5 current batch loss: 0.27054792642593384\n",
      "epoch: 3 batch: 6 current batch loss: 0.26136890053749084\n",
      "epoch: 3 batch: 7 current batch loss: 0.23553211987018585\n",
      "epoch: 3 batch: 8 current batch loss: 0.2481677234172821\n",
      "epoch: 3 batch: 9 current batch loss: 0.24795778095722198\n",
      "epoch: 3 batch: 10 current batch loss: 0.2520641088485718\n",
      "epoch: 3 batch: 11 current batch loss: 0.21015705168247223\n",
      "epoch: 3 batch: 12 current batch loss: 0.2478981912136078\n",
      "epoch: 3 batch: 13 current batch loss: 0.23798763751983643\n",
      "epoch: 3 batch: 14 current batch loss: 0.23785363137722015\n",
      "epoch: 3 batch: 15 current batch loss: 0.22273771464824677\n",
      "epoch: 3 batch: 16 current batch loss: 0.21473148465156555\n",
      "epoch: 3 batch: 17 current batch loss: 0.23888884484767914\n",
      "epoch: 3 batch: 18 current batch loss: 0.23292523622512817\n",
      "epoch: 3 batch: 19 current batch loss: 0.23131534457206726\n",
      "epoch: 3 batch: 20 current batch loss: 0.22327028214931488\n",
      "epoch: 3 batch: 21 current batch loss: 0.2189110666513443\n",
      "epoch: 3 batch: 22 current batch loss: 0.21722592413425446\n",
      "epoch: 3 batch: 23 current batch loss: 0.21893100440502167\n",
      "epoch: 3 batch: 24 current batch loss: 0.20368888974189758\n",
      "epoch: 3 batch: 25 current batch loss: 0.2202775478363037\n",
      "epoch: 3 batch: 26 current batch loss: 0.19696146249771118\n",
      "epoch: 3 batch: 27 current batch loss: 0.2089138627052307\n",
      "epoch: 3 batch: 28 current batch loss: 0.2146911323070526\n",
      "epoch: 3 batch: 29 current batch loss: 0.18035386502742767\n",
      "epoch: 4 batch: 0 current batch loss: 0.1795295774936676\n",
      "epoch: 4 batch: 1 current batch loss: 0.19969442486763\n",
      "epoch: 4 batch: 2 current batch loss: 0.16855080425739288\n",
      "epoch: 4 batch: 3 current batch loss: 0.19962629675865173\n",
      "epoch: 4 batch: 4 current batch loss: 0.18761588633060455\n",
      "epoch: 4 batch: 5 current batch loss: 0.2097180336713791\n",
      "epoch: 4 batch: 6 current batch loss: 0.19808116555213928\n",
      "epoch: 4 batch: 7 current batch loss: 0.18563352525234222\n",
      "epoch: 4 batch: 8 current batch loss: 0.190762460231781\n",
      "epoch: 4 batch: 9 current batch loss: 0.19855725765228271\n",
      "epoch: 4 batch: 10 current batch loss: 0.20083455741405487\n",
      "epoch: 4 batch: 11 current batch loss: 0.19297994673252106\n",
      "epoch: 4 batch: 12 current batch loss: 0.15362153947353363\n",
      "epoch: 4 batch: 13 current batch loss: 0.17074628174304962\n",
      "epoch: 4 batch: 14 current batch loss: 0.17718642950057983\n",
      "epoch: 4 batch: 15 current batch loss: 0.16440466046333313\n",
      "epoch: 4 batch: 16 current batch loss: 0.1943366676568985\n",
      "epoch: 4 batch: 17 current batch loss: 0.18665891885757446\n",
      "epoch: 4 batch: 18 current batch loss: 0.19342385232448578\n",
      "epoch: 4 batch: 19 current batch loss: 0.18659734725952148\n",
      "epoch: 4 batch: 20 current batch loss: 0.171159565448761\n",
      "epoch: 4 batch: 21 current batch loss: 0.1618323177099228\n",
      "epoch: 4 batch: 22 current batch loss: 0.15080632269382477\n",
      "epoch: 4 batch: 23 current batch loss: 0.17650996148586273\n",
      "epoch: 4 batch: 24 current batch loss: 0.17574334144592285\n",
      "epoch: 4 batch: 25 current batch loss: 0.1728747934103012\n",
      "epoch: 4 batch: 26 current batch loss: 0.15012314915657043\n",
      "epoch: 4 batch: 27 current batch loss: 0.1556302309036255\n",
      "epoch: 4 batch: 28 current batch loss: 0.1827889084815979\n",
      "epoch: 4 batch: 29 current batch loss: 0.12157264351844788\n",
      "epoch: 5 batch: 0 current batch loss: 0.14865687489509583\n",
      "epoch: 5 batch: 1 current batch loss: 0.13821552693843842\n",
      "epoch: 5 batch: 2 current batch loss: 0.1655314415693283\n",
      "epoch: 5 batch: 3 current batch loss: 0.1484866440296173\n",
      "epoch: 5 batch: 4 current batch loss: 0.14965198934078217\n",
      "epoch: 5 batch: 5 current batch loss: 0.15583185851573944\n",
      "epoch: 5 batch: 6 current batch loss: 0.15669314563274384\n",
      "epoch: 5 batch: 7 current batch loss: 0.15886056423187256\n",
      "epoch: 5 batch: 8 current batch loss: 0.17129886150360107\n",
      "epoch: 5 batch: 9 current batch loss: 0.14016687870025635\n",
      "epoch: 5 batch: 10 current batch loss: 0.14246071875095367\n",
      "epoch: 5 batch: 11 current batch loss: 0.1413404792547226\n",
      "epoch: 5 batch: 12 current batch loss: 0.14743725955486298\n",
      "epoch: 5 batch: 13 current batch loss: 0.13304656744003296\n",
      "epoch: 5 batch: 14 current batch loss: 0.1263253539800644\n",
      "epoch: 5 batch: 15 current batch loss: 0.12849865853786469\n",
      "epoch: 5 batch: 16 current batch loss: 0.1409948766231537\n",
      "epoch: 5 batch: 17 current batch loss: 0.13253375887870789\n",
      "epoch: 5 batch: 18 current batch loss: 0.1421518623828888\n",
      "epoch: 5 batch: 19 current batch loss: 0.15461064875125885\n",
      "epoch: 5 batch: 20 current batch loss: 0.1361408829689026\n",
      "epoch: 5 batch: 21 current batch loss: 0.13259172439575195\n",
      "epoch: 5 batch: 22 current batch loss: 0.13005097210407257\n",
      "epoch: 5 batch: 23 current batch loss: 0.12745016813278198\n",
      "epoch: 5 batch: 24 current batch loss: 0.16081994771957397\n",
      "epoch: 5 batch: 25 current batch loss: 0.13404244184494019\n",
      "epoch: 5 batch: 26 current batch loss: 0.1334894895553589\n",
      "epoch: 5 batch: 27 current batch loss: 0.13571244478225708\n",
      "epoch: 5 batch: 28 current batch loss: 0.1261238008737564\n",
      "epoch: 5 batch: 29 current batch loss: 0.09696490317583084\n",
      "epoch: 6 batch: 0 current batch loss: 0.1328720897436142\n",
      "epoch: 6 batch: 1 current batch loss: 0.10735928267240524\n",
      "epoch: 6 batch: 2 current batch loss: 0.12900935113430023\n",
      "epoch: 6 batch: 3 current batch loss: 0.12108784914016724\n",
      "epoch: 6 batch: 4 current batch loss: 0.11839772760868073\n",
      "epoch: 6 batch: 5 current batch loss: 0.13134483993053436\n",
      "epoch: 6 batch: 6 current batch loss: 0.11065560579299927\n",
      "epoch: 6 batch: 7 current batch loss: 0.11405652016401291\n",
      "epoch: 6 batch: 8 current batch loss: 0.11049484461545944\n",
      "epoch: 6 batch: 9 current batch loss: 0.11131452769041061\n",
      "epoch: 6 batch: 10 current batch loss: 0.11232165992259979\n",
      "epoch: 6 batch: 11 current batch loss: 0.11253859102725983\n",
      "epoch: 6 batch: 12 current batch loss: 0.10042070597410202\n",
      "epoch: 6 batch: 13 current batch loss: 0.10718207061290741\n",
      "epoch: 6 batch: 14 current batch loss: 0.1037481352686882\n",
      "epoch: 6 batch: 15 current batch loss: 0.1159435510635376\n",
      "epoch: 6 batch: 16 current batch loss: 0.10746867209672928\n",
      "epoch: 6 batch: 17 current batch loss: 0.10500282049179077\n",
      "epoch: 6 batch: 18 current batch loss: 0.11528950929641724\n",
      "epoch: 6 batch: 19 current batch loss: 0.11896920949220657\n",
      "epoch: 6 batch: 20 current batch loss: 0.1046617329120636\n",
      "epoch: 6 batch: 21 current batch loss: 0.11919866502285004\n",
      "epoch: 6 batch: 22 current batch loss: 0.10085797309875488\n",
      "epoch: 6 batch: 23 current batch loss: 0.12111736088991165\n",
      "epoch: 6 batch: 24 current batch loss: 0.12902918457984924\n",
      "epoch: 6 batch: 25 current batch loss: 0.1113269105553627\n",
      "epoch: 6 batch: 26 current batch loss: 0.11738668382167816\n",
      "epoch: 6 batch: 27 current batch loss: 0.10832281410694122\n",
      "epoch: 6 batch: 28 current batch loss: 0.11094588041305542\n",
      "epoch: 6 batch: 29 current batch loss: 0.16045266389846802\n",
      "epoch: 7 batch: 0 current batch loss: 0.10545392334461212\n",
      "epoch: 7 batch: 1 current batch loss: 0.11811409890651703\n",
      "epoch: 7 batch: 2 current batch loss: 0.09363505989313126\n",
      "epoch: 7 batch: 3 current batch loss: 0.10972139984369278\n",
      "epoch: 7 batch: 4 current batch loss: 0.08761734515428543\n",
      "epoch: 7 batch: 5 current batch loss: 0.09936545044183731\n",
      "epoch: 7 batch: 6 current batch loss: 0.1215473860502243\n",
      "epoch: 7 batch: 7 current batch loss: 0.1207737848162651\n",
      "epoch: 7 batch: 8 current batch loss: 0.08313875645399094\n",
      "epoch: 7 batch: 9 current batch loss: 0.10531539469957352\n",
      "epoch: 7 batch: 10 current batch loss: 0.09696844220161438\n",
      "epoch: 7 batch: 11 current batch loss: 0.10145245492458344\n",
      "epoch: 7 batch: 12 current batch loss: 0.09734532982110977\n",
      "epoch: 7 batch: 13 current batch loss: 0.09433528035879135\n",
      "epoch: 7 batch: 14 current batch loss: 0.10714695602655411\n",
      "epoch: 7 batch: 15 current batch loss: 0.08753592520952225\n",
      "epoch: 7 batch: 16 current batch loss: 0.08519727736711502\n",
      "epoch: 7 batch: 17 current batch loss: 0.0694175660610199\n",
      "epoch: 7 batch: 18 current batch loss: 0.10515622049570084\n",
      "epoch: 7 batch: 19 current batch loss: 0.09417570382356644\n",
      "epoch: 7 batch: 20 current batch loss: 0.09037352353334427\n",
      "epoch: 7 batch: 21 current batch loss: 0.08447392284870148\n",
      "epoch: 7 batch: 22 current batch loss: 0.08934733271598816\n",
      "epoch: 7 batch: 23 current batch loss: 0.10848645120859146\n",
      "epoch: 7 batch: 24 current batch loss: 0.09643176198005676\n",
      "epoch: 7 batch: 25 current batch loss: 0.10518815368413925\n",
      "epoch: 7 batch: 26 current batch loss: 0.09742755442857742\n",
      "epoch: 7 batch: 27 current batch loss: 0.08038242161273956\n",
      "epoch: 7 batch: 28 current batch loss: 0.07285307347774506\n",
      "epoch: 7 batch: 29 current batch loss: 0.1291101723909378\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
    "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #5\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
   "metadata": {},
   "source": [
    "### Answers to task #5\n",
    "\n",
    "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
    "\n",
    "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - the second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3394885063171387 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.4232640266418457 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.378471612930298 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.3111939430236816 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.2804787158966064 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.282994508743286 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.2755613327026367 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.2581899166107178 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.200209379196167 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.1527390480041504 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.1052186489105225 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.035961627960205 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 1.9708858728408813 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.9007701873779297 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.8411128520965576 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.7313768863677979 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.6659544706344604 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.5773459672927856 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.5063214302062988 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.4434049129486084 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.3716907501220703 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.3238270282745361 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.2595363855361938 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.1763252019882202 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.1336745023727417 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.053154706954956 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 1.034383773803711 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 0.9946162700653076 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 0.9553250074386597 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.9027447700500488 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.885441243648529 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.8490056991577148 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.7984921336174011 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.7943091988563538 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.7808595895767212 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.7314167618751526 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.7087092995643616 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.6863040328025818 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.675807535648346 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.6470564603805542 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.606590986251831 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 11 current batch loss: 0.6270073056221008 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 12 current batch loss: 0.5851461291313171 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 13 current batch loss: 0.5789533257484436 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 14 current batch loss: 0.582665205001831 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 15 current batch loss: 0.520540177822113 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 16 current batch loss: 0.5550079345703125 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 17 current batch loss: 0.533686101436615 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 18 current batch loss: 0.507209837436676 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 19 current batch loss: 0.4901050627231598 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 20 current batch loss: 0.46988317370414734 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 21 current batch loss: 0.49102744460105896 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 22 current batch loss: 0.46432042121887207 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 23 current batch loss: 0.4327777326107025 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 24 current batch loss: 0.4410178065299988 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 25 current batch loss: 0.44470253586769104 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 26 current batch loss: 0.41464006900787354 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 27 current batch loss: 0.4308494031429291 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 28 current batch loss: 0.436056524515152 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 29 current batch loss: 0.4083678126335144 current lr: 0.0009000000000000001\n",
      "epoch: 2 batch: 0 current batch loss: 0.4148430824279785 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 1 current batch loss: 0.39043915271759033 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 2 current batch loss: 0.4188896417617798 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 3 current batch loss: 0.3672229051589966 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 4 current batch loss: 0.3948206305503845 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 5 current batch loss: 0.36038756370544434 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 6 current batch loss: 0.37734299898147583 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 7 current batch loss: 0.36851775646209717 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 8 current batch loss: 0.36162441968917847 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 9 current batch loss: 0.34837067127227783 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 10 current batch loss: 0.32619166374206543 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 11 current batch loss: 0.3228966295719147 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 12 current batch loss: 0.33341965079307556 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 13 current batch loss: 0.37076011300086975 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 14 current batch loss: 0.34806355834007263 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 15 current batch loss: 0.3663783371448517 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 16 current batch loss: 0.32391491532325745 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 17 current batch loss: 0.33230042457580566 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 18 current batch loss: 0.34060344099998474 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 19 current batch loss: 0.3464067578315735 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 20 current batch loss: 0.31229785084724426 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 21 current batch loss: 0.316131055355072 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 22 current batch loss: 0.2969714105129242 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 23 current batch loss: 0.31131601333618164 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 24 current batch loss: 0.29301953315734863 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 25 current batch loss: 0.2776346504688263 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 26 current batch loss: 0.317528635263443 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 27 current batch loss: 0.28838375210762024 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 28 current batch loss: 0.29022157192230225 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 29 current batch loss: 0.3046780228614807 current lr: 0.0008100000000000001\n",
      "epoch: 3 batch: 0 current batch loss: 0.2580612003803253 current lr: 0.000729\n",
      "epoch: 3 batch: 1 current batch loss: 0.2904788553714752 current lr: 0.000729\n",
      "epoch: 3 batch: 2 current batch loss: 0.2549755573272705 current lr: 0.000729\n",
      "epoch: 3 batch: 3 current batch loss: 0.2784718871116638 current lr: 0.000729\n",
      "epoch: 3 batch: 4 current batch loss: 0.2670484185218811 current lr: 0.000729\n",
      "epoch: 3 batch: 5 current batch loss: 0.25594884157180786 current lr: 0.000729\n",
      "epoch: 3 batch: 6 current batch loss: 0.30834653973579407 current lr: 0.000729\n",
      "epoch: 3 batch: 7 current batch loss: 0.2692779302597046 current lr: 0.000729\n",
      "epoch: 3 batch: 8 current batch loss: 0.2774624526500702 current lr: 0.000729\n",
      "epoch: 3 batch: 9 current batch loss: 0.27784019708633423 current lr: 0.000729\n",
      "epoch: 3 batch: 10 current batch loss: 0.24640744924545288 current lr: 0.000729\n",
      "epoch: 3 batch: 11 current batch loss: 0.23491376638412476 current lr: 0.000729\n",
      "epoch: 3 batch: 12 current batch loss: 0.24218401312828064 current lr: 0.000729\n",
      "epoch: 3 batch: 13 current batch loss: 0.26209473609924316 current lr: 0.000729\n",
      "epoch: 3 batch: 14 current batch loss: 0.2789885401725769 current lr: 0.000729\n",
      "epoch: 3 batch: 15 current batch loss: 0.2612031400203705 current lr: 0.000729\n",
      "epoch: 3 batch: 16 current batch loss: 0.2475031167268753 current lr: 0.000729\n",
      "epoch: 3 batch: 17 current batch loss: 0.25674402713775635 current lr: 0.000729\n",
      "epoch: 3 batch: 18 current batch loss: 0.22734218835830688 current lr: 0.000729\n",
      "epoch: 3 batch: 19 current batch loss: 0.263826847076416 current lr: 0.000729\n",
      "epoch: 3 batch: 20 current batch loss: 0.2637021243572235 current lr: 0.000729\n",
      "epoch: 3 batch: 21 current batch loss: 0.22810843586921692 current lr: 0.000729\n",
      "epoch: 3 batch: 22 current batch loss: 0.2524872124195099 current lr: 0.000729\n",
      "epoch: 3 batch: 23 current batch loss: 0.22217044234275818 current lr: 0.000729\n",
      "epoch: 3 batch: 24 current batch loss: 0.22675253450870514 current lr: 0.000729\n",
      "epoch: 3 batch: 25 current batch loss: 0.2278747856616974 current lr: 0.000729\n",
      "epoch: 3 batch: 26 current batch loss: 0.2510239779949188 current lr: 0.000729\n",
      "epoch: 3 batch: 27 current batch loss: 0.21379530429840088 current lr: 0.000729\n",
      "epoch: 3 batch: 28 current batch loss: 0.23601959645748138 current lr: 0.000729\n",
      "epoch: 3 batch: 29 current batch loss: 0.26707473397254944 current lr: 0.000729\n",
      "epoch: 4 batch: 0 current batch loss: 0.20570248365402222 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 1 current batch loss: 0.23902805149555206 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 2 current batch loss: 0.22662052512168884 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 3 current batch loss: 0.21904616057872772 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 4 current batch loss: 0.20618581771850586 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 5 current batch loss: 0.21037344634532928 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 6 current batch loss: 0.21437422931194305 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 7 current batch loss: 0.21924081444740295 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 8 current batch loss: 0.21106739342212677 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 9 current batch loss: 0.19753721356391907 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 10 current batch loss: 0.18678796291351318 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 11 current batch loss: 0.20482231676578522 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 12 current batch loss: 0.22308756411075592 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 13 current batch loss: 0.2056833654642105 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 14 current batch loss: 0.21132157742977142 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 15 current batch loss: 0.19264639914035797 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 16 current batch loss: 0.1991303712129593 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 17 current batch loss: 0.21294139325618744 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 18 current batch loss: 0.2029213011264801 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 19 current batch loss: 0.19209201633930206 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 20 current batch loss: 0.18961387872695923 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 21 current batch loss: 0.21547910571098328 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 22 current batch loss: 0.18936750292778015 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 23 current batch loss: 0.19980977475643158 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 24 current batch loss: 0.20922960340976715 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 25 current batch loss: 0.2059701532125473 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 26 current batch loss: 0.20167149603366852 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 27 current batch loss: 0.18718752264976501 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 28 current batch loss: 0.18306049704551697 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 29 current batch loss: 0.1461968719959259 current lr: 0.0006561000000000001\n",
      "epoch: 5 batch: 0 current batch loss: 0.17909666895866394 current lr: 0.00059049\n",
      "epoch: 5 batch: 1 current batch loss: 0.1904827207326889 current lr: 0.00059049\n",
      "epoch: 5 batch: 2 current batch loss: 0.1685023158788681 current lr: 0.00059049\n",
      "epoch: 5 batch: 3 current batch loss: 0.18300682306289673 current lr: 0.00059049\n",
      "epoch: 5 batch: 4 current batch loss: 0.1704958826303482 current lr: 0.00059049\n",
      "epoch: 5 batch: 5 current batch loss: 0.200779527425766 current lr: 0.00059049\n",
      "epoch: 5 batch: 6 current batch loss: 0.15343810617923737 current lr: 0.00059049\n",
      "epoch: 5 batch: 7 current batch loss: 0.17888325452804565 current lr: 0.00059049\n",
      "epoch: 5 batch: 8 current batch loss: 0.19654765725135803 current lr: 0.00059049\n",
      "epoch: 5 batch: 9 current batch loss: 0.18235130608081818 current lr: 0.00059049\n",
      "epoch: 5 batch: 10 current batch loss: 0.1589038223028183 current lr: 0.00059049\n",
      "epoch: 5 batch: 11 current batch loss: 0.1703091263771057 current lr: 0.00059049\n",
      "epoch: 5 batch: 12 current batch loss: 0.1530180126428604 current lr: 0.00059049\n",
      "epoch: 5 batch: 13 current batch loss: 0.1603461652994156 current lr: 0.00059049\n",
      "epoch: 5 batch: 14 current batch loss: 0.16096925735473633 current lr: 0.00059049\n",
      "epoch: 5 batch: 15 current batch loss: 0.18503688275814056 current lr: 0.00059049\n",
      "epoch: 5 batch: 16 current batch loss: 0.18945400416851044 current lr: 0.00059049\n",
      "epoch: 5 batch: 17 current batch loss: 0.18494190275669098 current lr: 0.00059049\n",
      "epoch: 5 batch: 18 current batch loss: 0.15692846477031708 current lr: 0.00059049\n",
      "epoch: 5 batch: 19 current batch loss: 0.1788572072982788 current lr: 0.00059049\n",
      "epoch: 5 batch: 20 current batch loss: 0.1537133902311325 current lr: 0.00059049\n",
      "epoch: 5 batch: 21 current batch loss: 0.14392980933189392 current lr: 0.00059049\n",
      "epoch: 5 batch: 22 current batch loss: 0.16304998099803925 current lr: 0.00059049\n",
      "epoch: 5 batch: 23 current batch loss: 0.1577494740486145 current lr: 0.00059049\n",
      "epoch: 5 batch: 24 current batch loss: 0.16128863394260406 current lr: 0.00059049\n",
      "epoch: 5 batch: 25 current batch loss: 0.18624214828014374 current lr: 0.00059049\n",
      "epoch: 5 batch: 26 current batch loss: 0.1661340743303299 current lr: 0.00059049\n",
      "epoch: 5 batch: 27 current batch loss: 0.13445648550987244 current lr: 0.00059049\n",
      "epoch: 5 batch: 28 current batch loss: 0.15118901431560516 current lr: 0.00059049\n",
      "epoch: 5 batch: 29 current batch loss: 0.1263662725687027 current lr: 0.00059049\n",
      "epoch: 6 batch: 0 current batch loss: 0.1477816104888916 current lr: 0.000531441\n",
      "epoch: 6 batch: 1 current batch loss: 0.16805653274059296 current lr: 0.000531441\n",
      "epoch: 6 batch: 2 current batch loss: 0.15030893683433533 current lr: 0.000531441\n",
      "epoch: 6 batch: 3 current batch loss: 0.1464548259973526 current lr: 0.000531441\n",
      "epoch: 6 batch: 4 current batch loss: 0.15792542695999146 current lr: 0.000531441\n",
      "epoch: 6 batch: 5 current batch loss: 0.16885198652744293 current lr: 0.000531441\n",
      "epoch: 6 batch: 6 current batch loss: 0.162168487906456 current lr: 0.000531441\n",
      "epoch: 6 batch: 7 current batch loss: 0.1490255743265152 current lr: 0.000531441\n",
      "epoch: 6 batch: 8 current batch loss: 0.14742682874202728 current lr: 0.000531441\n",
      "epoch: 6 batch: 9 current batch loss: 0.15538547933101654 current lr: 0.000531441\n",
      "epoch: 6 batch: 10 current batch loss: 0.13792791962623596 current lr: 0.000531441\n",
      "epoch: 6 batch: 11 current batch loss: 0.14796876907348633 current lr: 0.000531441\n",
      "epoch: 6 batch: 12 current batch loss: 0.12384092062711716 current lr: 0.000531441\n",
      "epoch: 6 batch: 13 current batch loss: 0.1292782574892044 current lr: 0.000531441\n",
      "epoch: 6 batch: 14 current batch loss: 0.14542870223522186 current lr: 0.000531441\n",
      "epoch: 6 batch: 15 current batch loss: 0.13870269060134888 current lr: 0.000531441\n",
      "epoch: 6 batch: 16 current batch loss: 0.13234665989875793 current lr: 0.000531441\n",
      "epoch: 6 batch: 17 current batch loss: 0.1256670504808426 current lr: 0.000531441\n",
      "epoch: 6 batch: 18 current batch loss: 0.14233916997909546 current lr: 0.000531441\n",
      "epoch: 6 batch: 19 current batch loss: 0.14942115545272827 current lr: 0.000531441\n",
      "epoch: 6 batch: 20 current batch loss: 0.15776345133781433 current lr: 0.000531441\n",
      "epoch: 6 batch: 21 current batch loss: 0.1287599354982376 current lr: 0.000531441\n",
      "epoch: 6 batch: 22 current batch loss: 0.14886295795440674 current lr: 0.000531441\n",
      "epoch: 6 batch: 23 current batch loss: 0.13999758660793304 current lr: 0.000531441\n",
      "epoch: 6 batch: 24 current batch loss: 0.1610838919878006 current lr: 0.000531441\n",
      "epoch: 6 batch: 25 current batch loss: 0.1379978060722351 current lr: 0.000531441\n",
      "epoch: 6 batch: 26 current batch loss: 0.13903969526290894 current lr: 0.000531441\n",
      "epoch: 6 batch: 27 current batch loss: 0.13489407300949097 current lr: 0.000531441\n",
      "epoch: 6 batch: 28 current batch loss: 0.15184076130390167 current lr: 0.000531441\n",
      "epoch: 6 batch: 29 current batch loss: 0.13880635797977448 current lr: 0.000531441\n",
      "epoch: 7 batch: 0 current batch loss: 0.1425783932209015 current lr: 0.0004782969\n",
      "epoch: 7 batch: 1 current batch loss: 0.13623438775539398 current lr: 0.0004782969\n",
      "epoch: 7 batch: 2 current batch loss: 0.12959159910678864 current lr: 0.0004782969\n",
      "epoch: 7 batch: 3 current batch loss: 0.14103305339813232 current lr: 0.0004782969\n",
      "epoch: 7 batch: 4 current batch loss: 0.13400894403457642 current lr: 0.0004782969\n",
      "epoch: 7 batch: 5 current batch loss: 0.1383882761001587 current lr: 0.0004782969\n",
      "epoch: 7 batch: 6 current batch loss: 0.12287604808807373 current lr: 0.0004782969\n",
      "epoch: 7 batch: 7 current batch loss: 0.1259395182132721 current lr: 0.0004782969\n",
      "epoch: 7 batch: 8 current batch loss: 0.12761712074279785 current lr: 0.0004782969\n",
      "epoch: 7 batch: 9 current batch loss: 0.1366197168827057 current lr: 0.0004782969\n",
      "epoch: 7 batch: 10 current batch loss: 0.1365940272808075 current lr: 0.0004782969\n",
      "epoch: 7 batch: 11 current batch loss: 0.10848204791545868 current lr: 0.0004782969\n",
      "epoch: 7 batch: 12 current batch loss: 0.12920734286308289 current lr: 0.0004782969\n",
      "epoch: 7 batch: 13 current batch loss: 0.14282704889774323 current lr: 0.0004782969\n",
      "epoch: 7 batch: 14 current batch loss: 0.13027213513851166 current lr: 0.0004782969\n",
      "epoch: 7 batch: 15 current batch loss: 0.12627936899662018 current lr: 0.0004782969\n",
      "epoch: 7 batch: 16 current batch loss: 0.11482555419206619 current lr: 0.0004782969\n",
      "epoch: 7 batch: 17 current batch loss: 0.12195777893066406 current lr: 0.0004782969\n",
      "epoch: 7 batch: 18 current batch loss: 0.1398056447505951 current lr: 0.0004782969\n",
      "epoch: 7 batch: 19 current batch loss: 0.1398973912000656 current lr: 0.0004782969\n",
      "epoch: 7 batch: 20 current batch loss: 0.1306394636631012 current lr: 0.0004782969\n",
      "epoch: 7 batch: 21 current batch loss: 0.11951058357954025 current lr: 0.0004782969\n",
      "epoch: 7 batch: 22 current batch loss: 0.12982496619224548 current lr: 0.0004782969\n",
      "epoch: 7 batch: 23 current batch loss: 0.10462385416030884 current lr: 0.0004782969\n",
      "epoch: 7 batch: 24 current batch loss: 0.13085316121578217 current lr: 0.0004782969\n",
      "epoch: 7 batch: 25 current batch loss: 0.11388925462961197 current lr: 0.0004782969\n",
      "epoch: 7 batch: 26 current batch loss: 0.11989292502403259 current lr: 0.0004782969\n",
      "epoch: 7 batch: 27 current batch loss: 0.1093522235751152 current lr: 0.0004782969\n",
      "epoch: 7 batch: 28 current batch loss: 0.10469715297222137 current lr: 0.0004782969\n",
      "epoch: 7 batch: 29 current batch loss: 0.12041673809289932 current lr: 0.0004782969\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #6\n",
    "\n",
    "Well, it seems that we were able to get the learning rate much below 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) Some other schedulers you might want to experiment with: \n",
    "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
    "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #7\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9675\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9624\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5c6ca-1055-4d9f-b6e0-3e4601f3919d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
