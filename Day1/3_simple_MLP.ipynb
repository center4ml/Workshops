{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers \n",
    "\n",
    "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
    "\n",
    "Linear layers must be separated by nonlinear components (also called *activation functions*). \n",
    "\n",
    "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #1\n",
    "\n",
    "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. MNIST stands for Modified National Institute of Standards and Technology database.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snowakowski/venv/jupyter/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOZ0lEQVR4nO3dbYxc5XnG8euKbezamMQbB9chLjjgFAg0Jl0ZEBZQoVCCKgGqArGiyKG0ThOchNaVoLQqtKKVWyVElFIkU1xMxUsgAeEPNAm1ECRqcFlcY2wIb8Y0NmaNWYENIX5Z3/2w42iBnWeXmTMv3vv/k1Yzc+45c24NXD5nznNmHkeEAIx/H+p0AwDag7ADSRB2IAnCDiRB2IEkJrZzY4d5ckzRtHZuEkjlV3pbe2OPR6o1FXbb50m6QdIESf8WEctLz5+iaTrV5zSzSQAFa2NN3VrDh/G2J0i6SdLnJZ0oaZHtExt9PQCt1cxn9gWSXoiIzRGxV9Ldki6opi0AVWsm7EdJ+sWwx1try97F9hLbfbb79mlPE5sD0IyWn42PiBUR0RsRvZM0udWbA1BHM2HfJmnOsMefqC0D0IWaCfvjkubZnmv7MElflLS6mrYAVK3hobeI2G97qaQfaWjobWVEbKqsMwCVamqcPSIelPRgRb0AaCEulwWSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJpmZxRffzxPJ/4gkfm9nS7T/7F8fUrQ1OPVBc9+hjdxTrU7/uYv3V6w+rW1vX+73iujsH3y7WT713WbF+3J8/Vqx3QlNht71F0m5Jg5L2R0RvFU0BqF4Ve/bfi4idFbwOgBbiMzuQRLNhD0k/tv2E7SUjPcH2Ett9tvv2aU+TmwPQqGYP4xdGxDbbR0p6yPbPI+LR4U+IiBWSVkjSEe6JJrcHoEFN7dkjYlvtdoek+yUtqKIpANVrOOy2p9mefvC+pHMlbayqMQDVauYwfpak+20ffJ07I+KHlXQ1zkw4YV6xHpMnFeuvnPWRYv2d0+qPCfd8uDxe/JPPlMebO+k/fzm9WP/HfzmvWF978p11ay/te6e47vL+zxXrH//JofeJtOGwR8RmSZ+psBcALcTQG5AEYQeSIOxAEoQdSIKwA0nwFdcKDJ792WL9+ttuKtY/Nan+VzHHs30xWKz/zY1fKdYnvl0e/jr93qV1a9O37S+uO3lneWhuat/aYr0bsWcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ6/A5GdfKdaf+NWcYv1Tk/qrbKdSy7afVqxvfqv8U9S3Hfv9urU3D5THyWf9838X66106H2BdXTs2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCUe0b0TxCPfEqT6nbdvrFgOXnl6s7zqv/HPPEzYcXqw/+fUbP3BPB12383eK9cfPKo+jD77xZrEep9f/AeIt3yyuqrmLniw/Ae+zNtZoVwyMOJc1e3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9i4wYeZHi/XB1weK9ZfurD9WvunMlcV1F/zDN4r1I2/q3HfK8cE1Nc5ue6XtHbY3DlvWY/sh28/XbmdU2TCA6o3lMP42Se+d9f4qSWsiYp6kNbXHALrYqGGPiEclvfc48gJJq2r3V0m6sNq2AFSt0d+gmxUR22v3X5U0q94TbS+RtESSpmhqg5sD0Kymz8bH0Bm+umf5ImJFRPRGRO8kTW52cwAa1GjY+23PlqTa7Y7qWgLQCo2GfbWkxbX7iyU9UE07AFpl1M/stu+SdLakmba3SrpG0nJJ99i+TNLLki5uZZPj3eDO15taf9+uxud3//SXni7WX7t5QvkFDpTnWEf3GDXsEbGoTomrY4BDCJfLAkkQdiAJwg4kQdiBJAg7kARTNo8DJ1z5XN3apSeXB03+/eg1xfpZX7i8WJ/+vceKdXQP9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7ONAadrk1792QnHd/1v9TrF+1XW3F+t/efFFxXr874fr1ub8/c+K66qNP3OeAXt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCKZuTG/ij04v1O675drE+d+KUhrf96duXFuvzbtlerO/fvKXhbY9XTU3ZDGB8IOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnR1GcMb9YP2L51mL9rk/+qOFtH//wHxfrv/239b/HL0mDz29ueNuHqqbG2W2vtL3D9sZhy661vc32+trf+VU2DKB6YzmMv03SeSMs/25EzK/9PVhtWwCqNmrYI+JRSQNt6AVACzVzgm6p7Q21w/wZ9Z5ke4ntPtt9+7Snic0BaEajYb9Z0rGS5kvaLuk79Z4YESsiojcieidpcoObA9CshsIeEf0RMRgRByTdImlBtW0BqFpDYbc9e9jDiyRtrPdcAN1h1HF223dJOlvSTEn9kq6pPZ4vKSRtkfTViCh/+ViMs49HE2YdWay/cslxdWtrr7yhuO6HRtkXfemlc4v1Nxe+XqyPR6Vx9lEniYiIRSMsvrXprgC0FZfLAkkQdiAJwg4kQdiBJAg7kARfcUXH3LO1PGXzVB9WrP8y9hbrf/CNK+q/9v1ri+seqvgpaQCEHciCsANJEHYgCcIOJEHYgSQIO5DEqN96Q24HFs4v1l/8QnnK5pPmb6lbG20cfTQ3DpxSrE99oK+p1x9v2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs49z7j2pWH/um+Wx7lvOWFWsnzml/J3yZuyJfcX6YwNzyy9wYNRfN0+FPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+yFg4tyji/UXL/143dq1l9xdXPcPD9/ZUE9VuLq/t1h/5IbTivUZq8q/O493G3XPbnuO7YdtP217k+1v1Zb32H7I9vO12xmtbxdAo8ZyGL9f0rKIOFHSaZIut32ipKskrYmIeZLW1B4D6FKjhj0itkfEutr93ZKekXSUpAskHbyWcpWkC1vUI4AKfKDP7LaPkXSKpLWSZkXEwYuPX5U0q846SyQtkaQpmtpwowCaM+az8bYPl/QDSVdExK7htRiaHXLEGSIjYkVE9EZE7yRNbqpZAI0bU9htT9JQ0O+IiPtqi/ttz67VZ0va0ZoWAVRh1MN425Z0q6RnIuL6YaXVkhZLWl67faAlHY4DE4/5rWL9zd+dXaxf8nc/LNb/9CP3FeuttGx7eXjsZ/9af3it57b/Ka474wBDa1Uay2f2MyR9WdJTttfXll2toZDfY/sySS9LurglHQKoxKhhj4ifShpxcndJ51TbDoBW4XJZIAnCDiRB2IEkCDuQBGEHkuArrmM0cfZv1q0NrJxWXPdrcx8p1hdN72+opyos3bawWF938/xifeb3NxbrPbsZK+8W7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+x7f7/8s8V7/2ygWL/6uAfr1s79jbcb6qkq/YPv1K2duXpZcd3j//rnxXrPG+Vx8gPFKroJe3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSCLNOPuWC8v/rj138r0t2/ZNbxxbrN/wyLnFugfr/bjvkOOve6lubV7/2uK6g8UqxhP27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQhCOi/AR7jqTbJc2SFJJWRMQNtq+V9CeSXqs99eqIqP+lb0lHuCdONRO/Aq2yNtZoVwyMeGHGWC6q2S9pWUSssz1d0hO2H6rVvhsR366qUQCtM5b52bdL2l67v9v2M5KOanVjAKr1gT6z2z5G0imSDl6DudT2Btsrbc+os84S2322+/ZpT3PdAmjYmMNu+3BJP5B0RUTsknSzpGMlzdfQnv87I60XESsiojcieidpcvMdA2jImMJue5KGgn5HRNwnSRHRHxGDEXFA0i2SFrSuTQDNGjXsti3pVknPRMT1w5bPHva0iySVp/ME0FFjORt/hqQvS3rK9vrasqslLbI9X0PDcVskfbUF/QGoyFjOxv9U0kjjdsUxdQDdhSvogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSYz6U9KVbsx+TdLLwxbNlLSzbQ18MN3aW7f2JdFbo6rs7eiI+NhIhbaG/X0bt/siordjDRR0a2/d2pdEb41qV28cxgNJEHYgiU6HfUWHt1/Srb11a18SvTWqLb119DM7gPbp9J4dQJsQdiCJjoTd9nm2n7X9gu2rOtFDPba32H7K9nrbfR3uZaXtHbY3DlvWY/sh28/XbkecY69DvV1re1vtvVtv+/wO9TbH9sO2n7a9yfa3ass7+t4V+mrL+9b2z+y2J0h6TtLnJG2V9LikRRHxdFsbqcP2Fkm9EdHxCzBsnynpLUm3R8RJtWX/JGkgIpbX/qGcERFXdklv10p6q9PTeNdmK5o9fJpxSRdK+oo6+N4V+rpYbXjfOrFnXyDphYjYHBF7Jd0t6YIO9NH1IuJRSQPvWXyBpFW1+6s09D9L29XprStExPaIWFe7v1vSwWnGO/reFfpqi06E/ShJvxj2eKu6a773kPRj20/YXtLpZkYwKyK21+6/KmlWJ5sZwajTeLfTe6YZ75r3rpHpz5vFCbr3WxgRn5X0eUmX1w5Xu1IMfQbrprHTMU3j3S4jTDP+a5187xqd/rxZnQj7Nklzhj3+RG1ZV4iIbbXbHZLuV/dNRd1/cAbd2u2ODvfza900jfdI04yrC967Tk5/3omwPy5pnu25tg+T9EVJqzvQx/vYnlY7cSLb0ySdq+6binq1pMW1+4slPdDBXt6lW6bxrjfNuDr83nV8+vOIaPufpPM1dEb+RUl/1Yke6vT1SUlP1v42dbo3SXdp6LBun4bObVwm6aOS1kh6XtJ/Serpot7+Q9JTkjZoKFizO9TbQg0dom+QtL72d36n37tCX21537hcFkiCE3RAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMT/A65XcTMQuIbWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #2\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #3\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5df57-2b6c-44d2-a521-375bd4375f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([8, 8, 8,  ..., 7, 2, 8])\n",
      "1 -th batch labels : tensor([8, 2, 1,  ..., 2, 0, 8])\n",
      "2 -th batch labels : tensor([9, 8, 1,  ..., 1, 3, 5])\n",
      "3 -th batch labels : tensor([9, 7, 4,  ..., 7, 2, 5])\n",
      "4 -th batch labels : tensor([8, 4, 3,  ..., 9, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your taks #4\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
    "                                          #that output from a preceding layer serves as input to the next layer \n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3556504249572754\n",
      "epoch: 0 batch: 1 current batch loss: 2.401492118835449\n",
      "epoch: 0 batch: 2 current batch loss: 2.3903815746307373\n",
      "epoch: 0 batch: 3 current batch loss: 2.333325147628784\n",
      "epoch: 0 batch: 4 current batch loss: 2.313931465148926\n",
      "epoch: 0 batch: 5 current batch loss: 2.271759510040283\n",
      "epoch: 0 batch: 6 current batch loss: 2.276751756668091\n",
      "epoch: 0 batch: 7 current batch loss: 2.251744508743286\n",
      "epoch: 0 batch: 8 current batch loss: 2.2500834465026855\n",
      "epoch: 0 batch: 9 current batch loss: 2.2141973972320557\n",
      "epoch: 0 batch: 10 current batch loss: 2.1578187942504883\n",
      "epoch: 0 batch: 11 current batch loss: 2.0803072452545166\n",
      "epoch: 0 batch: 12 current batch loss: 2.024296283721924\n",
      "epoch: 0 batch: 13 current batch loss: 1.9556198120117188\n",
      "epoch: 0 batch: 14 current batch loss: 1.9013807773590088\n",
      "epoch: 0 batch: 15 current batch loss: 1.8211557865142822\n",
      "epoch: 0 batch: 16 current batch loss: 1.7670648097991943\n",
      "epoch: 0 batch: 17 current batch loss: 1.6568189859390259\n",
      "epoch: 0 batch: 18 current batch loss: 1.5830837488174438\n",
      "epoch: 0 batch: 19 current batch loss: 1.5221960544586182\n",
      "epoch: 0 batch: 20 current batch loss: 1.447272777557373\n",
      "epoch: 0 batch: 21 current batch loss: 1.4137617349624634\n",
      "epoch: 0 batch: 22 current batch loss: 1.3707224130630493\n",
      "epoch: 0 batch: 23 current batch loss: 1.2912354469299316\n",
      "epoch: 0 batch: 24 current batch loss: 1.2654396295547485\n",
      "epoch: 0 batch: 25 current batch loss: 1.196075677871704\n",
      "epoch: 0 batch: 26 current batch loss: 1.1417646408081055\n",
      "epoch: 0 batch: 27 current batch loss: 1.1149375438690186\n",
      "epoch: 0 batch: 28 current batch loss: 1.0730563402175903\n",
      "epoch: 0 batch: 29 current batch loss: 1.0162785053253174\n",
      "epoch: 1 batch: 0 current batch loss: 1.001828670501709\n",
      "epoch: 1 batch: 1 current batch loss: 0.979894757270813\n",
      "epoch: 1 batch: 2 current batch loss: 0.9444451332092285\n",
      "epoch: 1 batch: 3 current batch loss: 0.9289211630821228\n",
      "epoch: 1 batch: 4 current batch loss: 0.8962873816490173\n",
      "epoch: 1 batch: 5 current batch loss: 0.8421803712844849\n",
      "epoch: 1 batch: 6 current batch loss: 0.842253565788269\n",
      "epoch: 1 batch: 7 current batch loss: 0.8048539161682129\n",
      "epoch: 1 batch: 8 current batch loss: 0.7503359317779541\n",
      "epoch: 1 batch: 9 current batch loss: 0.740411639213562\n",
      "epoch: 1 batch: 10 current batch loss: 0.7024797797203064\n",
      "epoch: 1 batch: 11 current batch loss: 0.6736392378807068\n",
      "epoch: 1 batch: 12 current batch loss: 0.679101288318634\n",
      "epoch: 1 batch: 13 current batch loss: 0.6562315821647644\n",
      "epoch: 1 batch: 14 current batch loss: 0.6682080030441284\n",
      "epoch: 1 batch: 15 current batch loss: 0.6255563497543335\n",
      "epoch: 1 batch: 16 current batch loss: 0.5877550840377808\n",
      "epoch: 1 batch: 17 current batch loss: 0.5917223691940308\n",
      "epoch: 1 batch: 18 current batch loss: 0.5627952218055725\n",
      "epoch: 1 batch: 19 current batch loss: 0.5304036736488342\n",
      "epoch: 1 batch: 20 current batch loss: 0.5412710905075073\n",
      "epoch: 1 batch: 21 current batch loss: 0.5304055213928223\n",
      "epoch: 1 batch: 22 current batch loss: 0.5314541459083557\n",
      "epoch: 1 batch: 23 current batch loss: 0.5326627492904663\n",
      "epoch: 1 batch: 24 current batch loss: 0.4737878143787384\n",
      "epoch: 1 batch: 25 current batch loss: 0.5136838555335999\n",
      "epoch: 1 batch: 26 current batch loss: 0.4599023163318634\n",
      "epoch: 1 batch: 27 current batch loss: 0.4547992944717407\n",
      "epoch: 1 batch: 28 current batch loss: 0.483269602060318\n",
      "epoch: 1 batch: 29 current batch loss: 0.4076443910598755\n",
      "epoch: 2 batch: 0 current batch loss: 0.42998209595680237\n",
      "epoch: 2 batch: 1 current batch loss: 0.42908844351768494\n",
      "epoch: 2 batch: 2 current batch loss: 0.42023709416389465\n",
      "epoch: 2 batch: 3 current batch loss: 0.42091605067253113\n",
      "epoch: 2 batch: 4 current batch loss: 0.3884790539741516\n",
      "epoch: 2 batch: 5 current batch loss: 0.4250272512435913\n",
      "epoch: 2 batch: 6 current batch loss: 0.3822465240955353\n",
      "epoch: 2 batch: 7 current batch loss: 0.40330711007118225\n",
      "epoch: 2 batch: 8 current batch loss: 0.35694798827171326\n",
      "epoch: 2 batch: 9 current batch loss: 0.3621796667575836\n",
      "epoch: 2 batch: 10 current batch loss: 0.3588106036186218\n",
      "epoch: 2 batch: 11 current batch loss: 0.3263847827911377\n",
      "epoch: 2 batch: 12 current batch loss: 0.34546881914138794\n",
      "epoch: 2 batch: 13 current batch loss: 0.3710641860961914\n",
      "epoch: 2 batch: 14 current batch loss: 0.32605886459350586\n",
      "epoch: 2 batch: 15 current batch loss: 0.36644890904426575\n",
      "epoch: 2 batch: 16 current batch loss: 0.32353612780570984\n",
      "epoch: 2 batch: 17 current batch loss: 0.3158664405345917\n",
      "epoch: 2 batch: 18 current batch loss: 0.3172462284564972\n",
      "epoch: 2 batch: 19 current batch loss: 0.2899107038974762\n",
      "epoch: 2 batch: 20 current batch loss: 0.3214534819126129\n",
      "epoch: 2 batch: 21 current batch loss: 0.3004949390888214\n",
      "epoch: 2 batch: 22 current batch loss: 0.32184094190597534\n",
      "epoch: 2 batch: 23 current batch loss: 0.2962300479412079\n",
      "epoch: 2 batch: 24 current batch loss: 0.258786141872406\n",
      "epoch: 2 batch: 25 current batch loss: 0.27217182517051697\n",
      "epoch: 2 batch: 26 current batch loss: 0.2535984218120575\n",
      "epoch: 2 batch: 27 current batch loss: 0.3062455952167511\n",
      "epoch: 2 batch: 28 current batch loss: 0.29092657566070557\n",
      "epoch: 2 batch: 29 current batch loss: 0.2759588360786438\n",
      "epoch: 3 batch: 0 current batch loss: 0.30830514430999756\n",
      "epoch: 3 batch: 1 current batch loss: 0.28926151990890503\n",
      "epoch: 3 batch: 2 current batch loss: 0.2671867609024048\n",
      "epoch: 3 batch: 3 current batch loss: 0.26473069190979004\n",
      "epoch: 3 batch: 4 current batch loss: 0.24618098139762878\n",
      "epoch: 3 batch: 5 current batch loss: 0.2660646438598633\n",
      "epoch: 3 batch: 6 current batch loss: 0.2500801980495453\n",
      "epoch: 3 batch: 7 current batch loss: 0.2589128017425537\n",
      "epoch: 3 batch: 8 current batch loss: 0.26591914892196655\n",
      "epoch: 3 batch: 9 current batch loss: 0.23402220010757446\n",
      "epoch: 3 batch: 10 current batch loss: 0.2324192374944687\n",
      "epoch: 3 batch: 11 current batch loss: 0.27549269795417786\n",
      "epoch: 3 batch: 12 current batch loss: 0.2614811062812805\n",
      "epoch: 3 batch: 13 current batch loss: 0.2523042559623718\n",
      "epoch: 3 batch: 14 current batch loss: 0.21299465000629425\n",
      "epoch: 3 batch: 15 current batch loss: 0.2373989075422287\n",
      "epoch: 3 batch: 16 current batch loss: 0.24432621896266937\n",
      "epoch: 3 batch: 17 current batch loss: 0.22549055516719818\n",
      "epoch: 3 batch: 18 current batch loss: 0.26297688484191895\n",
      "epoch: 3 batch: 19 current batch loss: 0.2251339554786682\n",
      "epoch: 3 batch: 20 current batch loss: 0.2315717190504074\n",
      "epoch: 3 batch: 21 current batch loss: 0.24452543258666992\n",
      "epoch: 3 batch: 22 current batch loss: 0.20628513395786285\n",
      "epoch: 3 batch: 23 current batch loss: 0.20340824127197266\n",
      "epoch: 3 batch: 24 current batch loss: 0.21850568056106567\n",
      "epoch: 3 batch: 25 current batch loss: 0.22531092166900635\n",
      "epoch: 3 batch: 26 current batch loss: 0.21074944734573364\n",
      "epoch: 3 batch: 27 current batch loss: 0.2125251144170761\n",
      "epoch: 3 batch: 28 current batch loss: 0.21309694647789001\n",
      "epoch: 3 batch: 29 current batch loss: 0.2178056389093399\n",
      "epoch: 4 batch: 0 current batch loss: 0.23188824951648712\n",
      "epoch: 4 batch: 1 current batch loss: 0.23246368765830994\n",
      "epoch: 4 batch: 2 current batch loss: 0.2097458690404892\n",
      "epoch: 4 batch: 3 current batch loss: 0.20013727247714996\n",
      "epoch: 4 batch: 4 current batch loss: 0.17608334124088287\n",
      "epoch: 4 batch: 5 current batch loss: 0.1963302344083786\n",
      "epoch: 4 batch: 6 current batch loss: 0.19335703551769257\n",
      "epoch: 4 batch: 7 current batch loss: 0.18777678906917572\n",
      "epoch: 4 batch: 8 current batch loss: 0.18121132254600525\n",
      "epoch: 4 batch: 9 current batch loss: 0.18800346553325653\n",
      "epoch: 4 batch: 10 current batch loss: 0.18566136062145233\n",
      "epoch: 4 batch: 11 current batch loss: 0.17544381320476532\n",
      "epoch: 4 batch: 12 current batch loss: 0.19277384877204895\n",
      "epoch: 4 batch: 13 current batch loss: 0.1625373512506485\n",
      "epoch: 4 batch: 14 current batch loss: 0.18819700181484222\n",
      "epoch: 4 batch: 15 current batch loss: 0.2003103345632553\n",
      "epoch: 4 batch: 16 current batch loss: 0.17586028575897217\n",
      "epoch: 4 batch: 17 current batch loss: 0.18895018100738525\n",
      "epoch: 4 batch: 18 current batch loss: 0.1790240854024887\n",
      "epoch: 4 batch: 19 current batch loss: 0.18299968540668488\n",
      "epoch: 4 batch: 20 current batch loss: 0.18653763830661774\n",
      "epoch: 4 batch: 21 current batch loss: 0.18984268605709076\n",
      "epoch: 4 batch: 22 current batch loss: 0.16747213900089264\n",
      "epoch: 4 batch: 23 current batch loss: 0.1671341061592102\n",
      "epoch: 4 batch: 24 current batch loss: 0.17954842746257782\n",
      "epoch: 4 batch: 25 current batch loss: 0.17058640718460083\n",
      "epoch: 4 batch: 26 current batch loss: 0.15809142589569092\n",
      "epoch: 4 batch: 27 current batch loss: 0.179133340716362\n",
      "epoch: 4 batch: 28 current batch loss: 0.17207206785678864\n",
      "epoch: 4 batch: 29 current batch loss: 0.18429839611053467\n",
      "epoch: 5 batch: 0 current batch loss: 0.1623850166797638\n",
      "epoch: 5 batch: 1 current batch loss: 0.15544602274894714\n",
      "epoch: 5 batch: 2 current batch loss: 0.1756809502840042\n",
      "epoch: 5 batch: 3 current batch loss: 0.14768312871456146\n",
      "epoch: 5 batch: 4 current batch loss: 0.1555851399898529\n",
      "epoch: 5 batch: 5 current batch loss: 0.1805010437965393\n",
      "epoch: 5 batch: 6 current batch loss: 0.140019953250885\n",
      "epoch: 5 batch: 7 current batch loss: 0.16618874669075012\n",
      "epoch: 5 batch: 8 current batch loss: 0.16194458305835724\n",
      "epoch: 5 batch: 9 current batch loss: 0.15219815075397491\n",
      "epoch: 5 batch: 10 current batch loss: 0.16295568645000458\n",
      "epoch: 5 batch: 11 current batch loss: 0.1464301198720932\n",
      "epoch: 5 batch: 12 current batch loss: 0.1485193371772766\n",
      "epoch: 5 batch: 13 current batch loss: 0.14798620343208313\n",
      "epoch: 5 batch: 14 current batch loss: 0.1498268097639084\n",
      "epoch: 5 batch: 15 current batch loss: 0.13507509231567383\n",
      "epoch: 5 batch: 16 current batch loss: 0.15688586235046387\n",
      "epoch: 5 batch: 17 current batch loss: 0.14610159397125244\n",
      "epoch: 5 batch: 18 current batch loss: 0.14046715199947357\n",
      "epoch: 5 batch: 19 current batch loss: 0.143576979637146\n",
      "epoch: 5 batch: 20 current batch loss: 0.13626664876937866\n",
      "epoch: 5 batch: 21 current batch loss: 0.1421680897474289\n",
      "epoch: 5 batch: 22 current batch loss: 0.13711132109165192\n",
      "epoch: 5 batch: 23 current batch loss: 0.14708346128463745\n",
      "epoch: 5 batch: 24 current batch loss: 0.1237466037273407\n",
      "epoch: 5 batch: 25 current batch loss: 0.14711250364780426\n",
      "epoch: 5 batch: 26 current batch loss: 0.1330319046974182\n",
      "epoch: 5 batch: 27 current batch loss: 0.14445152878761292\n",
      "epoch: 5 batch: 28 current batch loss: 0.1272309571504593\n",
      "epoch: 5 batch: 29 current batch loss: 0.13618937134742737\n",
      "epoch: 6 batch: 0 current batch loss: 0.12901966273784637\n",
      "epoch: 6 batch: 1 current batch loss: 0.11041178554296494\n",
      "epoch: 6 batch: 2 current batch loss: 0.11581530421972275\n",
      "epoch: 6 batch: 3 current batch loss: 0.13508445024490356\n",
      "epoch: 6 batch: 4 current batch loss: 0.13308115303516388\n",
      "epoch: 6 batch: 5 current batch loss: 0.12211049348115921\n",
      "epoch: 6 batch: 6 current batch loss: 0.10930298268795013\n",
      "epoch: 6 batch: 7 current batch loss: 0.12814921140670776\n",
      "epoch: 6 batch: 8 current batch loss: 0.12731753289699554\n",
      "epoch: 6 batch: 9 current batch loss: 0.12034879624843597\n",
      "epoch: 6 batch: 10 current batch loss: 0.1351509988307953\n",
      "epoch: 6 batch: 11 current batch loss: 0.1224701851606369\n",
      "epoch: 6 batch: 12 current batch loss: 0.11977551132440567\n",
      "epoch: 6 batch: 13 current batch loss: 0.10524484515190125\n",
      "epoch: 6 batch: 14 current batch loss: 0.11607314646244049\n",
      "epoch: 6 batch: 15 current batch loss: 0.11791165918111801\n",
      "epoch: 6 batch: 16 current batch loss: 0.10422365367412567\n",
      "epoch: 6 batch: 17 current batch loss: 0.12057390809059143\n",
      "epoch: 6 batch: 18 current batch loss: 0.11756423860788345\n",
      "epoch: 6 batch: 19 current batch loss: 0.10204403102397919\n",
      "epoch: 6 batch: 20 current batch loss: 0.1060810387134552\n",
      "epoch: 6 batch: 21 current batch loss: 0.11205708980560303\n",
      "epoch: 6 batch: 22 current batch loss: 0.10074823349714279\n",
      "epoch: 6 batch: 23 current batch loss: 0.11269809305667877\n",
      "epoch: 6 batch: 24 current batch loss: 0.12021639198064804\n",
      "epoch: 6 batch: 25 current batch loss: 0.10722728073596954\n",
      "epoch: 6 batch: 26 current batch loss: 0.11262942105531693\n",
      "epoch: 6 batch: 27 current batch loss: 0.13003544509410858\n",
      "epoch: 6 batch: 28 current batch loss: 0.12858429551124573\n",
      "epoch: 6 batch: 29 current batch loss: 0.10110300034284592\n",
      "epoch: 7 batch: 0 current batch loss: 0.08270963281393051\n",
      "epoch: 7 batch: 1 current batch loss: 0.1278919130563736\n",
      "epoch: 7 batch: 2 current batch loss: 0.10593989491462708\n",
      "epoch: 7 batch: 3 current batch loss: 0.10640984773635864\n",
      "epoch: 7 batch: 4 current batch loss: 0.11818825453519821\n",
      "epoch: 7 batch: 5 current batch loss: 0.1020946055650711\n",
      "epoch: 7 batch: 6 current batch loss: 0.09620209783315659\n",
      "epoch: 7 batch: 7 current batch loss: 0.10645490139722824\n",
      "epoch: 7 batch: 8 current batch loss: 0.11915335804224014\n",
      "epoch: 7 batch: 9 current batch loss: 0.09905432164669037\n",
      "epoch: 7 batch: 10 current batch loss: 0.10538189113140106\n",
      "epoch: 7 batch: 11 current batch loss: 0.09084781259298325\n",
      "epoch: 7 batch: 12 current batch loss: 0.10881398618221283\n",
      "epoch: 7 batch: 13 current batch loss: 0.09577049314975739\n",
      "epoch: 7 batch: 14 current batch loss: 0.0749594122171402\n",
      "epoch: 7 batch: 15 current batch loss: 0.105311319231987\n",
      "epoch: 7 batch: 16 current batch loss: 0.11154430359601974\n",
      "epoch: 7 batch: 17 current batch loss: 0.08665423095226288\n",
      "epoch: 7 batch: 18 current batch loss: 0.09688810259103775\n",
      "epoch: 7 batch: 19 current batch loss: 0.1010204628109932\n",
      "epoch: 7 batch: 20 current batch loss: 0.0952596515417099\n",
      "epoch: 7 batch: 21 current batch loss: 0.08240930736064911\n",
      "epoch: 7 batch: 22 current batch loss: 0.10499953478574753\n",
      "epoch: 7 batch: 23 current batch loss: 0.10346505790948868\n",
      "epoch: 7 batch: 24 current batch loss: 0.09865878522396088\n",
      "epoch: 7 batch: 25 current batch loss: 0.1089470237493515\n",
      "epoch: 7 batch: 26 current batch loss: 0.11172010004520416\n",
      "epoch: 7 batch: 27 current batch loss: 0.09578210860490799\n",
      "epoch: 7 batch: 28 current batch loss: 0.11437489092350006\n",
      "epoch: 7 batch: 29 current batch loss: 0.11331178992986679\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
    "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #5\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - the second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3517255783081055 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.4011850357055664 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.3541176319122314 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.3107879161834717 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.288468599319458 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.289538621902466 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.2742080688476562 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.2688443660736084 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.236281394958496 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.197779417037964 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.1468143463134766 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.08884596824646 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 2.0286450386047363 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.9442933797836304 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.8606773614883423 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.771222710609436 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.6466139554977417 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.565658688545227 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.480183720588684 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.413981556892395 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.346901297569275 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.2957264184951782 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.2086055278778076 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.1716095209121704 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.0951200723648071 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.0873953104019165 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 1.0480135679244995 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 1.013705849647522 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 0.9451425075531006 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.9425120949745178 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.8740072846412659 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.8541630506515503 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.8162854909896851 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.7983854413032532 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.7945323586463928 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.7419249415397644 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.7521490454673767 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.7023018598556519 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.7043824791908264 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.656472384929657 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.6672511100769043 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 11 current batch loss: 0.6302699446678162 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 12 current batch loss: 0.6191329956054688 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 13 current batch loss: 0.5966921448707581 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 14 current batch loss: 0.5974599719047546 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 15 current batch loss: 0.5704531073570251 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 16 current batch loss: 0.5685278177261353 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 17 current batch loss: 0.5303357839584351 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 18 current batch loss: 0.5124316215515137 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 19 current batch loss: 0.5061802268028259 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 20 current batch loss: 0.4979959726333618 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 21 current batch loss: 0.4852164089679718 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 22 current batch loss: 0.4699719250202179 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 23 current batch loss: 0.46838846802711487 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 24 current batch loss: 0.4428836703300476 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 25 current batch loss: 0.4376734793186188 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 26 current batch loss: 0.44858959317207336 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 27 current batch loss: 0.42198437452316284 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 28 current batch loss: 0.3899543285369873 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 29 current batch loss: 0.41879335045814514 current lr: 0.0009000000000000001\n",
      "epoch: 2 batch: 0 current batch loss: 0.41736772656440735 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 1 current batch loss: 0.4306168854236603 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 2 current batch loss: 0.3774612247943878 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 3 current batch loss: 0.38944026827812195 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 4 current batch loss: 0.36605364084243774 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 5 current batch loss: 0.3538955748081207 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 6 current batch loss: 0.40036138892173767 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 7 current batch loss: 0.3421880900859833 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 8 current batch loss: 0.38481149077415466 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 9 current batch loss: 0.3715256452560425 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 10 current batch loss: 0.33244451880455017 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 11 current batch loss: 0.3453162908554077 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 12 current batch loss: 0.3482992351055145 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 13 current batch loss: 0.35520321130752563 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 14 current batch loss: 0.3371233642101288 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 15 current batch loss: 0.3315546214580536 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 16 current batch loss: 0.3450182378292084 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 17 current batch loss: 0.3192235827445984 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 18 current batch loss: 0.31345441937446594 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 19 current batch loss: 0.3213820159435272 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 20 current batch loss: 0.3354545533657074 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 21 current batch loss: 0.3010406494140625 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 22 current batch loss: 0.3053359389305115 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 23 current batch loss: 0.31537944078445435 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 24 current batch loss: 0.29721730947494507 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 25 current batch loss: 0.2956177294254303 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 26 current batch loss: 0.28831371665000916 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 27 current batch loss: 0.28984227776527405 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 28 current batch loss: 0.33011433482170105 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 29 current batch loss: 0.3133005201816559 current lr: 0.0008100000000000001\n",
      "epoch: 3 batch: 0 current batch loss: 0.28899937868118286 current lr: 0.000729\n",
      "epoch: 3 batch: 1 current batch loss: 0.28078988194465637 current lr: 0.000729\n",
      "epoch: 3 batch: 2 current batch loss: 0.28704193234443665 current lr: 0.000729\n",
      "epoch: 3 batch: 3 current batch loss: 0.271968811750412 current lr: 0.000729\n",
      "epoch: 3 batch: 4 current batch loss: 0.2714008390903473 current lr: 0.000729\n",
      "epoch: 3 batch: 5 current batch loss: 0.24807941913604736 current lr: 0.000729\n",
      "epoch: 3 batch: 6 current batch loss: 0.26700159907341003 current lr: 0.000729\n",
      "epoch: 3 batch: 7 current batch loss: 0.26851189136505127 current lr: 0.000729\n",
      "epoch: 3 batch: 8 current batch loss: 0.2545730471611023 current lr: 0.000729\n",
      "epoch: 3 batch: 9 current batch loss: 0.2660442292690277 current lr: 0.000729\n",
      "epoch: 3 batch: 10 current batch loss: 0.25737959146499634 current lr: 0.000729\n",
      "epoch: 3 batch: 11 current batch loss: 0.25297316908836365 current lr: 0.000729\n",
      "epoch: 3 batch: 12 current batch loss: 0.2543177008628845 current lr: 0.000729\n",
      "epoch: 3 batch: 13 current batch loss: 0.2514372169971466 current lr: 0.000729\n",
      "epoch: 3 batch: 14 current batch loss: 0.27189183235168457 current lr: 0.000729\n",
      "epoch: 3 batch: 15 current batch loss: 0.23547108471393585 current lr: 0.000729\n",
      "epoch: 3 batch: 16 current batch loss: 0.27225232124328613 current lr: 0.000729\n",
      "epoch: 3 batch: 17 current batch loss: 0.23646315932273865 current lr: 0.000729\n",
      "epoch: 3 batch: 18 current batch loss: 0.24916887283325195 current lr: 0.000729\n",
      "epoch: 3 batch: 19 current batch loss: 0.25405219197273254 current lr: 0.000729\n",
      "epoch: 3 batch: 20 current batch loss: 0.24825593829154968 current lr: 0.000729\n",
      "epoch: 3 batch: 21 current batch loss: 0.24948319792747498 current lr: 0.000729\n",
      "epoch: 3 batch: 22 current batch loss: 0.25375574827194214 current lr: 0.000729\n",
      "epoch: 3 batch: 23 current batch loss: 0.24959391355514526 current lr: 0.000729\n",
      "epoch: 3 batch: 24 current batch loss: 0.22746048867702484 current lr: 0.000729\n",
      "epoch: 3 batch: 25 current batch loss: 0.24770237505435944 current lr: 0.000729\n",
      "epoch: 3 batch: 26 current batch loss: 0.2279711663722992 current lr: 0.000729\n",
      "epoch: 3 batch: 27 current batch loss: 0.22897416353225708 current lr: 0.000729\n",
      "epoch: 3 batch: 28 current batch loss: 0.2313411980867386 current lr: 0.000729\n",
      "epoch: 3 batch: 29 current batch loss: 0.22870512306690216 current lr: 0.000729\n",
      "epoch: 4 batch: 0 current batch loss: 0.2137954831123352 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 1 current batch loss: 0.22073252499103546 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 2 current batch loss: 0.21061211824417114 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 3 current batch loss: 0.19561044871807098 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 4 current batch loss: 0.22697900235652924 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 5 current batch loss: 0.20192363858222961 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 6 current batch loss: 0.19725389778614044 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 7 current batch loss: 0.22710780799388885 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 8 current batch loss: 0.2124851495027542 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 9 current batch loss: 0.20762953162193298 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 10 current batch loss: 0.19689415395259857 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 11 current batch loss: 0.22096386551856995 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 12 current batch loss: 0.22338669002056122 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 13 current batch loss: 0.21983686089515686 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 14 current batch loss: 0.18590691685676575 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 15 current batch loss: 0.19358386099338531 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 16 current batch loss: 0.19797790050506592 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 17 current batch loss: 0.21913164854049683 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 18 current batch loss: 0.18693096935749054 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 19 current batch loss: 0.19175513088703156 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 20 current batch loss: 0.21598225831985474 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 21 current batch loss: 0.17750023305416107 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 22 current batch loss: 0.19167307019233704 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 23 current batch loss: 0.18646785616874695 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 24 current batch loss: 0.19395805895328522 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 25 current batch loss: 0.20859593152999878 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 26 current batch loss: 0.18906034529209137 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 27 current batch loss: 0.20378363132476807 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 28 current batch loss: 0.19731345772743225 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 29 current batch loss: 0.21638792753219604 current lr: 0.0006561000000000001\n",
      "epoch: 5 batch: 0 current batch loss: 0.1803962141275406 current lr: 0.00059049\n",
      "epoch: 5 batch: 1 current batch loss: 0.18969924747943878 current lr: 0.00059049\n",
      "epoch: 5 batch: 2 current batch loss: 0.17561137676239014 current lr: 0.00059049\n",
      "epoch: 5 batch: 3 current batch loss: 0.17640283703804016 current lr: 0.00059049\n",
      "epoch: 5 batch: 4 current batch loss: 0.1772359162569046 current lr: 0.00059049\n",
      "epoch: 5 batch: 5 current batch loss: 0.17988301813602448 current lr: 0.00059049\n",
      "epoch: 5 batch: 6 current batch loss: 0.18397419154644012 current lr: 0.00059049\n",
      "epoch: 5 batch: 7 current batch loss: 0.1717018038034439 current lr: 0.00059049\n",
      "epoch: 5 batch: 8 current batch loss: 0.1908683329820633 current lr: 0.00059049\n",
      "epoch: 5 batch: 9 current batch loss: 0.18585336208343506 current lr: 0.00059049\n",
      "epoch: 5 batch: 10 current batch loss: 0.17271727323532104 current lr: 0.00059049\n",
      "epoch: 5 batch: 11 current batch loss: 0.1807340681552887 current lr: 0.00059049\n",
      "epoch: 5 batch: 12 current batch loss: 0.16618269681930542 current lr: 0.00059049\n",
      "epoch: 5 batch: 13 current batch loss: 0.14722873270511627 current lr: 0.00059049\n",
      "epoch: 5 batch: 14 current batch loss: 0.17907248437404633 current lr: 0.00059049\n",
      "epoch: 5 batch: 15 current batch loss: 0.15640927851200104 current lr: 0.00059049\n",
      "epoch: 5 batch: 16 current batch loss: 0.16570861637592316 current lr: 0.00059049\n",
      "epoch: 5 batch: 17 current batch loss: 0.1535814106464386 current lr: 0.00059049\n",
      "epoch: 5 batch: 18 current batch loss: 0.1912616491317749 current lr: 0.00059049\n",
      "epoch: 5 batch: 19 current batch loss: 0.15535524487495422 current lr: 0.00059049\n",
      "epoch: 5 batch: 20 current batch loss: 0.17846588790416718 current lr: 0.00059049\n",
      "epoch: 5 batch: 21 current batch loss: 0.15586987137794495 current lr: 0.00059049\n",
      "epoch: 5 batch: 22 current batch loss: 0.16528518497943878 current lr: 0.00059049\n",
      "epoch: 5 batch: 23 current batch loss: 0.16935063898563385 current lr: 0.00059049\n",
      "epoch: 5 batch: 24 current batch loss: 0.1529950499534607 current lr: 0.00059049\n",
      "epoch: 5 batch: 25 current batch loss: 0.15312319993972778 current lr: 0.00059049\n",
      "epoch: 5 batch: 26 current batch loss: 0.16036023199558258 current lr: 0.00059049\n",
      "epoch: 5 batch: 27 current batch loss: 0.1492445468902588 current lr: 0.00059049\n",
      "epoch: 5 batch: 28 current batch loss: 0.15414218604564667 current lr: 0.00059049\n",
      "epoch: 5 batch: 29 current batch loss: 0.170243039727211 current lr: 0.00059049\n",
      "epoch: 6 batch: 0 current batch loss: 0.15441709756851196 current lr: 0.000531441\n",
      "epoch: 6 batch: 1 current batch loss: 0.15182746946811676 current lr: 0.000531441\n",
      "epoch: 6 batch: 2 current batch loss: 0.1486670970916748 current lr: 0.000531441\n",
      "epoch: 6 batch: 3 current batch loss: 0.17074552178382874 current lr: 0.000531441\n",
      "epoch: 6 batch: 4 current batch loss: 0.14329083263874054 current lr: 0.000531441\n",
      "epoch: 6 batch: 5 current batch loss: 0.16345614194869995 current lr: 0.000531441\n",
      "epoch: 6 batch: 6 current batch loss: 0.15140961110591888 current lr: 0.000531441\n",
      "epoch: 6 batch: 7 current batch loss: 0.1495649218559265 current lr: 0.000531441\n",
      "epoch: 6 batch: 8 current batch loss: 0.15352508425712585 current lr: 0.000531441\n",
      "epoch: 6 batch: 9 current batch loss: 0.13189126551151276 current lr: 0.000531441\n",
      "epoch: 6 batch: 10 current batch loss: 0.17033123970031738 current lr: 0.000531441\n",
      "epoch: 6 batch: 11 current batch loss: 0.12093058228492737 current lr: 0.000531441\n",
      "epoch: 6 batch: 12 current batch loss: 0.1648888885974884 current lr: 0.000531441\n",
      "epoch: 6 batch: 13 current batch loss: 0.14644016325473785 current lr: 0.000531441\n",
      "epoch: 6 batch: 14 current batch loss: 0.14902137219905853 current lr: 0.000531441\n",
      "epoch: 6 batch: 15 current batch loss: 0.13831403851509094 current lr: 0.000531441\n",
      "epoch: 6 batch: 16 current batch loss: 0.15398520231246948 current lr: 0.000531441\n",
      "epoch: 6 batch: 17 current batch loss: 0.13611142337322235 current lr: 0.000531441\n",
      "epoch: 6 batch: 18 current batch loss: 0.16232866048812866 current lr: 0.000531441\n",
      "epoch: 6 batch: 19 current batch loss: 0.16816028952598572 current lr: 0.000531441\n",
      "epoch: 6 batch: 20 current batch loss: 0.13249869644641876 current lr: 0.000531441\n",
      "epoch: 6 batch: 21 current batch loss: 0.1443859487771988 current lr: 0.000531441\n",
      "epoch: 6 batch: 22 current batch loss: 0.14769744873046875 current lr: 0.000531441\n",
      "epoch: 6 batch: 23 current batch loss: 0.1328718364238739 current lr: 0.000531441\n",
      "epoch: 6 batch: 24 current batch loss: 0.13490910828113556 current lr: 0.000531441\n",
      "epoch: 6 batch: 25 current batch loss: 0.12345181405544281 current lr: 0.000531441\n",
      "epoch: 6 batch: 26 current batch loss: 0.1365024894475937 current lr: 0.000531441\n",
      "epoch: 6 batch: 27 current batch loss: 0.12499392777681351 current lr: 0.000531441\n",
      "epoch: 6 batch: 28 current batch loss: 0.11491883546113968 current lr: 0.000531441\n",
      "epoch: 6 batch: 29 current batch loss: 0.1445888727903366 current lr: 0.000531441\n",
      "epoch: 7 batch: 0 current batch loss: 0.1392224282026291 current lr: 0.0004782969\n",
      "epoch: 7 batch: 1 current batch loss: 0.11263605952262878 current lr: 0.0004782969\n",
      "epoch: 7 batch: 2 current batch loss: 0.11952945590019226 current lr: 0.0004782969\n",
      "epoch: 7 batch: 3 current batch loss: 0.1470937430858612 current lr: 0.0004782969\n",
      "epoch: 7 batch: 4 current batch loss: 0.12695355713367462 current lr: 0.0004782969\n",
      "epoch: 7 batch: 5 current batch loss: 0.12625275552272797 current lr: 0.0004782969\n",
      "epoch: 7 batch: 6 current batch loss: 0.13486172258853912 current lr: 0.0004782969\n",
      "epoch: 7 batch: 7 current batch loss: 0.1301969289779663 current lr: 0.0004782969\n",
      "epoch: 7 batch: 8 current batch loss: 0.12877579033374786 current lr: 0.0004782969\n",
      "epoch: 7 batch: 9 current batch loss: 0.13220079243183136 current lr: 0.0004782969\n",
      "epoch: 7 batch: 10 current batch loss: 0.13610579073429108 current lr: 0.0004782969\n",
      "epoch: 7 batch: 11 current batch loss: 0.1438441127538681 current lr: 0.0004782969\n",
      "epoch: 7 batch: 12 current batch loss: 0.12857894599437714 current lr: 0.0004782969\n",
      "epoch: 7 batch: 13 current batch loss: 0.12550956010818481 current lr: 0.0004782969\n",
      "epoch: 7 batch: 14 current batch loss: 0.13040605187416077 current lr: 0.0004782969\n",
      "epoch: 7 batch: 15 current batch loss: 0.14753252267837524 current lr: 0.0004782969\n",
      "epoch: 7 batch: 16 current batch loss: 0.14064179360866547 current lr: 0.0004782969\n",
      "epoch: 7 batch: 17 current batch loss: 0.1133168414235115 current lr: 0.0004782969\n",
      "epoch: 7 batch: 18 current batch loss: 0.13241709768772125 current lr: 0.0004782969\n",
      "epoch: 7 batch: 19 current batch loss: 0.1352769285440445 current lr: 0.0004782969\n",
      "epoch: 7 batch: 20 current batch loss: 0.11527939885854721 current lr: 0.0004782969\n",
      "epoch: 7 batch: 21 current batch loss: 0.09190830588340759 current lr: 0.0004782969\n",
      "epoch: 7 batch: 22 current batch loss: 0.12910579144954681 current lr: 0.0004782969\n",
      "epoch: 7 batch: 23 current batch loss: 0.12125824391841888 current lr: 0.0004782969\n",
      "epoch: 7 batch: 24 current batch loss: 0.12552513182163239 current lr: 0.0004782969\n",
      "epoch: 7 batch: 25 current batch loss: 0.1318633109331131 current lr: 0.0004782969\n",
      "epoch: 7 batch: 26 current batch loss: 0.12070339918136597 current lr: 0.0004782969\n",
      "epoch: 7 batch: 27 current batch loss: 0.12807036936283112 current lr: 0.0004782969\n",
      "epoch: 7 batch: 28 current batch loss: 0.12793396413326263 current lr: 0.0004782969\n",
      "epoch: 7 batch: 29 current batch loss: 0.14301760494709015 current lr: 0.0004782969\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #6\n",
    "\n",
    "Well, it seems that we were able to get the learning rate much below 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) Some other schedulers you might want to experiment with: \n",
    "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
    "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #7\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9672\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9611\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603b03e-a7da-4b3e-a2eb-5e8b87324fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
