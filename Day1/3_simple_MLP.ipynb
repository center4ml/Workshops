{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers separated by nonlinear components (also called *activation functions*). A nonlinear component in-between the linear layers is essential: without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. Also, please note that it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
    "\n",
    "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. \n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #1\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #2\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5df57-2b6c-44d2-a521-375bd4375f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([9, 0, 4,  ..., 6, 8, 8])\n",
      "1 -th batch labels : tensor([1, 8, 2,  ..., 1, 0, 5])\n",
      "2 -th batch labels : tensor([7, 9, 2,  ..., 0, 2, 0])\n",
      "3 -th batch labels : tensor([9, 4, 8,  ..., 4, 4, 3])\n",
      "4 -th batch labels : tensor([6, 4, 3,  ..., 0, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your taks #3\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
    "                                          #that output from a preceding layer serves as input to the next layer \n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.356858015060425\n",
      "epoch: 0 batch: 1 current batch loss: 2.3641488552093506\n",
      "epoch: 0 batch: 2 current batch loss: 2.382612943649292\n",
      "epoch: 0 batch: 3 current batch loss: 2.3496406078338623\n",
      "epoch: 0 batch: 4 current batch loss: 2.3037946224212646\n",
      "epoch: 0 batch: 5 current batch loss: 2.2673795223236084\n",
      "epoch: 0 batch: 6 current batch loss: 2.261713743209839\n",
      "epoch: 0 batch: 7 current batch loss: 2.263072967529297\n",
      "epoch: 0 batch: 8 current batch loss: 2.2312793731689453\n",
      "epoch: 0 batch: 9 current batch loss: 2.1805598735809326\n",
      "epoch: 0 batch: 10 current batch loss: 2.1311519145965576\n",
      "epoch: 0 batch: 11 current batch loss: 2.0649499893188477\n",
      "epoch: 0 batch: 12 current batch loss: 1.9954478740692139\n",
      "epoch: 0 batch: 13 current batch loss: 1.892810344696045\n",
      "epoch: 0 batch: 14 current batch loss: 1.802590012550354\n",
      "epoch: 0 batch: 15 current batch loss: 1.718424677848816\n",
      "epoch: 0 batch: 16 current batch loss: 1.6120604276657104\n",
      "epoch: 0 batch: 17 current batch loss: 1.531752109527588\n",
      "epoch: 0 batch: 18 current batch loss: 1.4593729972839355\n",
      "epoch: 0 batch: 19 current batch loss: 1.3938204050064087\n",
      "epoch: 0 batch: 20 current batch loss: 1.3168666362762451\n",
      "epoch: 0 batch: 21 current batch loss: 1.2630341053009033\n",
      "epoch: 0 batch: 22 current batch loss: 1.2053451538085938\n",
      "epoch: 0 batch: 23 current batch loss: 1.1629234552383423\n",
      "epoch: 0 batch: 24 current batch loss: 1.07353937625885\n",
      "epoch: 0 batch: 25 current batch loss: 1.0795104503631592\n",
      "epoch: 0 batch: 26 current batch loss: 0.9929932355880737\n",
      "epoch: 0 batch: 27 current batch loss: 0.9444851875305176\n",
      "epoch: 0 batch: 28 current batch loss: 0.9400392770767212\n",
      "epoch: 0 batch: 29 current batch loss: 0.8879868388175964\n",
      "epoch: 1 batch: 0 current batch loss: 0.8514360785484314\n",
      "epoch: 1 batch: 1 current batch loss: 0.8416159749031067\n",
      "epoch: 1 batch: 2 current batch loss: 0.7819403409957886\n",
      "epoch: 1 batch: 3 current batch loss: 0.7666419148445129\n",
      "epoch: 1 batch: 4 current batch loss: 0.7304478883743286\n",
      "epoch: 1 batch: 5 current batch loss: 0.6962648034095764\n",
      "epoch: 1 batch: 6 current batch loss: 0.6660832762718201\n",
      "epoch: 1 batch: 7 current batch loss: 0.6630988121032715\n",
      "epoch: 1 batch: 8 current batch loss: 0.6226688623428345\n",
      "epoch: 1 batch: 9 current batch loss: 0.6136561036109924\n",
      "epoch: 1 batch: 10 current batch loss: 0.5894769430160522\n",
      "epoch: 1 batch: 11 current batch loss: 0.5756450295448303\n",
      "epoch: 1 batch: 12 current batch loss: 0.5843533277511597\n",
      "epoch: 1 batch: 13 current batch loss: 0.5552412271499634\n",
      "epoch: 1 batch: 14 current batch loss: 0.5370509624481201\n",
      "epoch: 1 batch: 15 current batch loss: 0.47885188460350037\n",
      "epoch: 1 batch: 16 current batch loss: 0.49564477801322937\n",
      "epoch: 1 batch: 17 current batch loss: 0.4783461093902588\n",
      "epoch: 1 batch: 18 current batch loss: 0.4837203919887543\n",
      "epoch: 1 batch: 19 current batch loss: 0.46176794171333313\n",
      "epoch: 1 batch: 20 current batch loss: 0.4333115816116333\n",
      "epoch: 1 batch: 21 current batch loss: 0.42002490162849426\n",
      "epoch: 1 batch: 22 current batch loss: 0.450397789478302\n",
      "epoch: 1 batch: 23 current batch loss: 0.4039956331253052\n",
      "epoch: 1 batch: 24 current batch loss: 0.40244418382644653\n",
      "epoch: 1 batch: 25 current batch loss: 0.4166643023490906\n",
      "epoch: 1 batch: 26 current batch loss: 0.396676629781723\n",
      "epoch: 1 batch: 27 current batch loss: 0.3819045126438141\n",
      "epoch: 1 batch: 28 current batch loss: 0.3563823699951172\n",
      "epoch: 1 batch: 29 current batch loss: 0.39920634031295776\n",
      "epoch: 2 batch: 0 current batch loss: 0.37011194229125977\n",
      "epoch: 2 batch: 1 current batch loss: 0.3611413538455963\n",
      "epoch: 2 batch: 2 current batch loss: 0.33602261543273926\n",
      "epoch: 2 batch: 3 current batch loss: 0.3876209557056427\n",
      "epoch: 2 batch: 4 current batch loss: 0.34938448667526245\n",
      "epoch: 2 batch: 5 current batch loss: 0.34144800901412964\n",
      "epoch: 2 batch: 6 current batch loss: 0.3210464417934418\n",
      "epoch: 2 batch: 7 current batch loss: 0.3243325352668762\n",
      "epoch: 2 batch: 8 current batch loss: 0.3549499213695526\n",
      "epoch: 2 batch: 9 current batch loss: 0.3216312527656555\n",
      "epoch: 2 batch: 10 current batch loss: 0.3475233018398285\n",
      "epoch: 2 batch: 11 current batch loss: 0.3014439344406128\n",
      "epoch: 2 batch: 12 current batch loss: 0.3153377175331116\n",
      "epoch: 2 batch: 13 current batch loss: 0.3036322593688965\n",
      "epoch: 2 batch: 14 current batch loss: 0.2927100658416748\n",
      "epoch: 2 batch: 15 current batch loss: 0.3188979923725128\n",
      "epoch: 2 batch: 16 current batch loss: 0.3013380765914917\n",
      "epoch: 2 batch: 17 current batch loss: 0.26745253801345825\n",
      "epoch: 2 batch: 18 current batch loss: 0.30405375361442566\n",
      "epoch: 2 batch: 19 current batch loss: 0.2869931161403656\n",
      "epoch: 2 batch: 20 current batch loss: 0.2789386212825775\n",
      "epoch: 2 batch: 21 current batch loss: 0.2713213264942169\n",
      "epoch: 2 batch: 22 current batch loss: 0.2648283541202545\n",
      "epoch: 2 batch: 23 current batch loss: 0.2799828052520752\n",
      "epoch: 2 batch: 24 current batch loss: 0.2910761833190918\n",
      "epoch: 2 batch: 25 current batch loss: 0.2583945095539093\n",
      "epoch: 2 batch: 26 current batch loss: 0.25032496452331543\n",
      "epoch: 2 batch: 27 current batch loss: 0.2621779441833496\n",
      "epoch: 2 batch: 28 current batch loss: 0.24947871267795563\n",
      "epoch: 2 batch: 29 current batch loss: 0.3065839111804962\n",
      "epoch: 3 batch: 0 current batch loss: 0.25326865911483765\n",
      "epoch: 3 batch: 1 current batch loss: 0.2272810935974121\n",
      "epoch: 3 batch: 2 current batch loss: 0.25503116846084595\n",
      "epoch: 3 batch: 3 current batch loss: 0.250522643327713\n",
      "epoch: 3 batch: 4 current batch loss: 0.2745402753353119\n",
      "epoch: 3 batch: 5 current batch loss: 0.22845278680324554\n",
      "epoch: 3 batch: 6 current batch loss: 0.22214454412460327\n",
      "epoch: 3 batch: 7 current batch loss: 0.22420059144496918\n",
      "epoch: 3 batch: 8 current batch loss: 0.23109294474124908\n",
      "epoch: 3 batch: 9 current batch loss: 0.23788057267665863\n",
      "epoch: 3 batch: 10 current batch loss: 0.22771063446998596\n",
      "epoch: 3 batch: 11 current batch loss: 0.22783112525939941\n",
      "epoch: 3 batch: 12 current batch loss: 0.22650928795337677\n",
      "epoch: 3 batch: 13 current batch loss: 0.2257152795791626\n",
      "epoch: 3 batch: 14 current batch loss: 0.25341564416885376\n",
      "epoch: 3 batch: 15 current batch loss: 0.24545949697494507\n",
      "epoch: 3 batch: 16 current batch loss: 0.21264179050922394\n",
      "epoch: 3 batch: 17 current batch loss: 0.2003089189529419\n",
      "epoch: 3 batch: 18 current batch loss: 0.23317547142505646\n",
      "epoch: 3 batch: 19 current batch loss: 0.2202156037092209\n",
      "epoch: 3 batch: 20 current batch loss: 0.2134964019060135\n",
      "epoch: 3 batch: 21 current batch loss: 0.19649246335029602\n",
      "epoch: 3 batch: 22 current batch loss: 0.20197825133800507\n",
      "epoch: 3 batch: 23 current batch loss: 0.20255692303180695\n",
      "epoch: 3 batch: 24 current batch loss: 0.19762834906578064\n",
      "epoch: 3 batch: 25 current batch loss: 0.2049819976091385\n",
      "epoch: 3 batch: 26 current batch loss: 0.19132916629314423\n",
      "epoch: 3 batch: 27 current batch loss: 0.21700771152973175\n",
      "epoch: 3 batch: 28 current batch loss: 0.20032919943332672\n",
      "epoch: 3 batch: 29 current batch loss: 0.19573265314102173\n",
      "epoch: 4 batch: 0 current batch loss: 0.21586258709430695\n",
      "epoch: 4 batch: 1 current batch loss: 0.20563463866710663\n",
      "epoch: 4 batch: 2 current batch loss: 0.17694959044456482\n",
      "epoch: 4 batch: 3 current batch loss: 0.19279471039772034\n",
      "epoch: 4 batch: 4 current batch loss: 0.16771060228347778\n",
      "epoch: 4 batch: 5 current batch loss: 0.18642380833625793\n",
      "epoch: 4 batch: 6 current batch loss: 0.1857965737581253\n",
      "epoch: 4 batch: 7 current batch loss: 0.18118000030517578\n",
      "epoch: 4 batch: 8 current batch loss: 0.1714894026517868\n",
      "epoch: 4 batch: 9 current batch loss: 0.194099560379982\n",
      "epoch: 4 batch: 10 current batch loss: 0.15701165795326233\n",
      "epoch: 4 batch: 11 current batch loss: 0.17639845609664917\n",
      "epoch: 4 batch: 12 current batch loss: 0.17975854873657227\n",
      "epoch: 4 batch: 13 current batch loss: 0.16468974947929382\n",
      "epoch: 4 batch: 14 current batch loss: 0.18324939906597137\n",
      "epoch: 4 batch: 15 current batch loss: 0.20538149774074554\n",
      "epoch: 4 batch: 16 current batch loss: 0.16374069452285767\n",
      "epoch: 4 batch: 17 current batch loss: 0.1911410093307495\n",
      "epoch: 4 batch: 18 current batch loss: 0.17267458140850067\n",
      "epoch: 4 batch: 19 current batch loss: 0.14401854574680328\n",
      "epoch: 4 batch: 20 current batch loss: 0.1540190726518631\n",
      "epoch: 4 batch: 21 current batch loss: 0.17576493322849274\n",
      "epoch: 4 batch: 22 current batch loss: 0.16188740730285645\n",
      "epoch: 4 batch: 23 current batch loss: 0.15216206014156342\n",
      "epoch: 4 batch: 24 current batch loss: 0.17289204895496368\n",
      "epoch: 4 batch: 25 current batch loss: 0.15378053486347198\n",
      "epoch: 4 batch: 26 current batch loss: 0.15423810482025146\n",
      "epoch: 4 batch: 27 current batch loss: 0.15581874549388885\n",
      "epoch: 4 batch: 28 current batch loss: 0.17466425895690918\n",
      "epoch: 4 batch: 29 current batch loss: 0.12250743061304092\n",
      "epoch: 5 batch: 0 current batch loss: 0.14417745172977448\n",
      "epoch: 5 batch: 1 current batch loss: 0.16463546454906464\n",
      "epoch: 5 batch: 2 current batch loss: 0.15374219417572021\n",
      "epoch: 5 batch: 3 current batch loss: 0.13349029421806335\n",
      "epoch: 5 batch: 4 current batch loss: 0.15296299755573273\n",
      "epoch: 5 batch: 5 current batch loss: 0.14285145699977875\n",
      "epoch: 5 batch: 6 current batch loss: 0.14541195333003998\n",
      "epoch: 5 batch: 7 current batch loss: 0.12639778852462769\n",
      "epoch: 5 batch: 8 current batch loss: 0.11899445205926895\n",
      "epoch: 5 batch: 9 current batch loss: 0.11221688240766525\n",
      "epoch: 5 batch: 10 current batch loss: 0.14573097229003906\n",
      "epoch: 5 batch: 11 current batch loss: 0.10703641176223755\n",
      "epoch: 5 batch: 12 current batch loss: 0.13656356930732727\n",
      "epoch: 5 batch: 13 current batch loss: 0.16404828429222107\n",
      "epoch: 5 batch: 14 current batch loss: 0.1337587833404541\n",
      "epoch: 5 batch: 15 current batch loss: 0.14073534309864044\n",
      "epoch: 5 batch: 16 current batch loss: 0.1378430724143982\n",
      "epoch: 5 batch: 17 current batch loss: 0.14358177781105042\n",
      "epoch: 5 batch: 18 current batch loss: 0.12265173345804214\n",
      "epoch: 5 batch: 19 current batch loss: 0.11980853974819183\n",
      "epoch: 5 batch: 20 current batch loss: 0.14144422113895416\n",
      "epoch: 5 batch: 21 current batch loss: 0.13066956400871277\n",
      "epoch: 5 batch: 22 current batch loss: 0.12225352227687836\n",
      "epoch: 5 batch: 23 current batch loss: 0.13925108313560486\n",
      "epoch: 5 batch: 24 current batch loss: 0.1310952752828598\n",
      "epoch: 5 batch: 25 current batch loss: 0.16309784352779388\n",
      "epoch: 5 batch: 26 current batch loss: 0.12950542569160461\n",
      "epoch: 5 batch: 27 current batch loss: 0.15062087774276733\n",
      "epoch: 5 batch: 28 current batch loss: 0.12654602527618408\n",
      "epoch: 5 batch: 29 current batch loss: 0.1400514841079712\n",
      "epoch: 6 batch: 0 current batch loss: 0.10714453458786011\n",
      "epoch: 6 batch: 1 current batch loss: 0.12052936106920242\n",
      "epoch: 6 batch: 2 current batch loss: 0.12497265636920929\n",
      "epoch: 6 batch: 3 current batch loss: 0.12076107412576675\n",
      "epoch: 6 batch: 4 current batch loss: 0.1195073053240776\n",
      "epoch: 6 batch: 5 current batch loss: 0.12338483333587646\n",
      "epoch: 6 batch: 6 current batch loss: 0.1195305585861206\n",
      "epoch: 6 batch: 7 current batch loss: 0.1284836381673813\n",
      "epoch: 6 batch: 8 current batch loss: 0.11281394958496094\n",
      "epoch: 6 batch: 9 current batch loss: 0.1182253286242485\n",
      "epoch: 6 batch: 10 current batch loss: 0.11410883069038391\n",
      "epoch: 6 batch: 11 current batch loss: 0.12421765178442001\n",
      "epoch: 6 batch: 12 current batch loss: 0.11681410670280457\n",
      "epoch: 6 batch: 13 current batch loss: 0.11569289118051529\n",
      "epoch: 6 batch: 14 current batch loss: 0.11744308471679688\n",
      "epoch: 6 batch: 15 current batch loss: 0.10168323665857315\n",
      "epoch: 6 batch: 16 current batch loss: 0.09872904419898987\n",
      "epoch: 6 batch: 17 current batch loss: 0.10169997066259384\n",
      "epoch: 6 batch: 18 current batch loss: 0.11929253488779068\n",
      "epoch: 6 batch: 19 current batch loss: 0.1214921772480011\n",
      "epoch: 6 batch: 20 current batch loss: 0.11441001296043396\n",
      "epoch: 6 batch: 21 current batch loss: 0.10377052426338196\n",
      "epoch: 6 batch: 22 current batch loss: 0.11596029996871948\n",
      "epoch: 6 batch: 23 current batch loss: 0.11505509167909622\n",
      "epoch: 6 batch: 24 current batch loss: 0.09368015080690384\n",
      "epoch: 6 batch: 25 current batch loss: 0.1169283390045166\n",
      "epoch: 6 batch: 26 current batch loss: 0.11554905027151108\n",
      "epoch: 6 batch: 27 current batch loss: 0.09544871002435684\n",
      "epoch: 6 batch: 28 current batch loss: 0.12495049834251404\n",
      "epoch: 6 batch: 29 current batch loss: 0.10825210064649582\n",
      "epoch: 7 batch: 0 current batch loss: 0.09114785492420197\n",
      "epoch: 7 batch: 1 current batch loss: 0.09177520126104355\n",
      "epoch: 7 batch: 2 current batch loss: 0.10698011517524719\n",
      "epoch: 7 batch: 3 current batch loss: 0.10144874453544617\n",
      "epoch: 7 batch: 4 current batch loss: 0.07583723217248917\n",
      "epoch: 7 batch: 5 current batch loss: 0.1032697781920433\n",
      "epoch: 7 batch: 6 current batch loss: 0.0821964368224144\n",
      "epoch: 7 batch: 7 current batch loss: 0.0984378382563591\n",
      "epoch: 7 batch: 8 current batch loss: 0.12032070010900497\n",
      "epoch: 7 batch: 9 current batch loss: 0.09755424410104752\n",
      "epoch: 7 batch: 10 current batch loss: 0.10626378655433655\n",
      "epoch: 7 batch: 11 current batch loss: 0.09530135989189148\n",
      "epoch: 7 batch: 12 current batch loss: 0.09465760737657547\n",
      "epoch: 7 batch: 13 current batch loss: 0.08747076243162155\n",
      "epoch: 7 batch: 14 current batch loss: 0.09106872230768204\n",
      "epoch: 7 batch: 15 current batch loss: 0.09121844172477722\n",
      "epoch: 7 batch: 16 current batch loss: 0.0962793305516243\n",
      "epoch: 7 batch: 17 current batch loss: 0.08421836793422699\n",
      "epoch: 7 batch: 18 current batch loss: 0.10203388333320618\n",
      "epoch: 7 batch: 19 current batch loss: 0.10250860452651978\n",
      "epoch: 7 batch: 20 current batch loss: 0.08303132653236389\n",
      "epoch: 7 batch: 21 current batch loss: 0.09066345542669296\n",
      "epoch: 7 batch: 22 current batch loss: 0.08118272572755814\n",
      "epoch: 7 batch: 23 current batch loss: 0.09561116993427277\n",
      "epoch: 7 batch: 24 current batch loss: 0.09872841835021973\n",
      "epoch: 7 batch: 25 current batch loss: 0.08388274163007736\n",
      "epoch: 7 batch: 26 current batch loss: 0.08238281309604645\n",
      "epoch: 7 batch: 27 current batch loss: 0.06833796948194504\n",
      "epoch: 7 batch: 28 current batch loss: 0.0818164274096489\n",
      "epoch: 7 batch: 29 current batch loss: 0.07181817293167114\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
    "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #4\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - the second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3355751037597656 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.422093629837036 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.365288496017456 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.310039520263672 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.2756290435791016 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.267622709274292 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.2713749408721924 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.2461493015289307 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.2040727138519287 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.1449124813079834 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.0831246376037598 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.023315191268921 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 1.9494261741638184 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.8719233274459839 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.7890697717666626 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.7208176851272583 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.643847942352295 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.566788911819458 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.4790711402893066 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.445091724395752 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.3598262071609497 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.3091952800750732 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.2398337125778198 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.1847808361053467 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.103668212890625 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.0589776039123535 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 1.0104925632476807 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 0.9542796611785889 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 0.9417077302932739 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.8841932415962219 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.8357163667678833 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.7945919632911682 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.7745445966720581 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.78659987449646 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.7219691276550293 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.7157072424888611 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.6699817776679993 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.661857008934021 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.6168370842933655 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.608775794506073 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.605340301990509 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 11 current batch loss: 0.5979672074317932 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 12 current batch loss: 0.5757798552513123 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 13 current batch loss: 0.5526236295700073 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 14 current batch loss: 0.5258044600486755 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 15 current batch loss: 0.47961923480033875 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 16 current batch loss: 0.5075336694717407 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 17 current batch loss: 0.4969443678855896 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 18 current batch loss: 0.49091559648513794 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 19 current batch loss: 0.4918593764305115 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 20 current batch loss: 0.4909295439720154 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 21 current batch loss: 0.42952728271484375 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 22 current batch loss: 0.4317115247249603 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 23 current batch loss: 0.4297320544719696 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 24 current batch loss: 0.4212612211704254 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 25 current batch loss: 0.4096820652484894 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 26 current batch loss: 0.41055062413215637 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 27 current batch loss: 0.405116468667984 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 28 current batch loss: 0.3845278322696686 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 29 current batch loss: 0.33081990480422974 current lr: 0.0009000000000000001\n",
      "epoch: 2 batch: 0 current batch loss: 0.4084441065788269 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 1 current batch loss: 0.39587604999542236 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 2 current batch loss: 0.34875059127807617 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 3 current batch loss: 0.3531351387500763 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 4 current batch loss: 0.35652589797973633 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 5 current batch loss: 0.3889079988002777 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 6 current batch loss: 0.3499378263950348 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 7 current batch loss: 0.33274534344673157 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 8 current batch loss: 0.3395632803440094 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 9 current batch loss: 0.3662106692790985 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 10 current batch loss: 0.35526299476623535 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 11 current batch loss: 0.35429707169532776 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 12 current batch loss: 0.35692745447158813 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 13 current batch loss: 0.31256937980651855 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 14 current batch loss: 0.3176056444644928 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 15 current batch loss: 0.33645787835121155 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 16 current batch loss: 0.3286695182323456 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 17 current batch loss: 0.2992543876171112 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 18 current batch loss: 0.3161833882331848 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 19 current batch loss: 0.3180919289588928 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 20 current batch loss: 0.2782544791698456 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 21 current batch loss: 0.30410224199295044 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 22 current batch loss: 0.30653807520866394 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 23 current batch loss: 0.29486241936683655 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 24 current batch loss: 0.30175885558128357 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 25 current batch loss: 0.28980752825737 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 26 current batch loss: 0.2835313379764557 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 27 current batch loss: 0.2869481146335602 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 28 current batch loss: 0.2659110128879547 current lr: 0.0008100000000000001\n",
      "epoch: 2 batch: 29 current batch loss: 0.24900047481060028 current lr: 0.0008100000000000001\n",
      "epoch: 3 batch: 0 current batch loss: 0.2643008232116699 current lr: 0.000729\n",
      "epoch: 3 batch: 1 current batch loss: 0.27594831585884094 current lr: 0.000729\n",
      "epoch: 3 batch: 2 current batch loss: 0.24255183339118958 current lr: 0.000729\n",
      "epoch: 3 batch: 3 current batch loss: 0.2845521867275238 current lr: 0.000729\n",
      "epoch: 3 batch: 4 current batch loss: 0.2679736316204071 current lr: 0.000729\n",
      "epoch: 3 batch: 5 current batch loss: 0.2782166302204132 current lr: 0.000729\n",
      "epoch: 3 batch: 6 current batch loss: 0.26457351446151733 current lr: 0.000729\n",
      "epoch: 3 batch: 7 current batch loss: 0.22623920440673828 current lr: 0.000729\n",
      "epoch: 3 batch: 8 current batch loss: 0.280222624540329 current lr: 0.000729\n",
      "epoch: 3 batch: 9 current batch loss: 0.2391212433576584 current lr: 0.000729\n",
      "epoch: 3 batch: 10 current batch loss: 0.26795297861099243 current lr: 0.000729\n",
      "epoch: 3 batch: 11 current batch loss: 0.25407132506370544 current lr: 0.000729\n",
      "epoch: 3 batch: 12 current batch loss: 0.2326977401971817 current lr: 0.000729\n",
      "epoch: 3 batch: 13 current batch loss: 0.23169483244419098 current lr: 0.000729\n",
      "epoch: 3 batch: 14 current batch loss: 0.2450423538684845 current lr: 0.000729\n",
      "epoch: 3 batch: 15 current batch loss: 0.2383878082036972 current lr: 0.000729\n",
      "epoch: 3 batch: 16 current batch loss: 0.24709144234657288 current lr: 0.000729\n",
      "epoch: 3 batch: 17 current batch loss: 0.2561957836151123 current lr: 0.000729\n",
      "epoch: 3 batch: 18 current batch loss: 0.23471330106258392 current lr: 0.000729\n",
      "epoch: 3 batch: 19 current batch loss: 0.2302798330783844 current lr: 0.000729\n",
      "epoch: 3 batch: 20 current batch loss: 0.23521441221237183 current lr: 0.000729\n",
      "epoch: 3 batch: 21 current batch loss: 0.2262263149023056 current lr: 0.000729\n",
      "epoch: 3 batch: 22 current batch loss: 0.24220718443393707 current lr: 0.000729\n",
      "epoch: 3 batch: 23 current batch loss: 0.22524456679821014 current lr: 0.000729\n",
      "epoch: 3 batch: 24 current batch loss: 0.25161245465278625 current lr: 0.000729\n",
      "epoch: 3 batch: 25 current batch loss: 0.25814658403396606 current lr: 0.000729\n",
      "epoch: 3 batch: 26 current batch loss: 0.21479636430740356 current lr: 0.000729\n",
      "epoch: 3 batch: 27 current batch loss: 0.23013754189014435 current lr: 0.000729\n",
      "epoch: 3 batch: 28 current batch loss: 0.228657528758049 current lr: 0.000729\n",
      "epoch: 3 batch: 29 current batch loss: 0.22985245287418365 current lr: 0.000729\n",
      "epoch: 4 batch: 0 current batch loss: 0.2066584676504135 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 1 current batch loss: 0.20759791135787964 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 2 current batch loss: 0.21775491535663605 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 3 current batch loss: 0.1962827891111374 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 4 current batch loss: 0.20001693069934845 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 5 current batch loss: 0.2074940800666809 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 6 current batch loss: 0.20153620839118958 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 7 current batch loss: 0.19059161841869354 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 8 current batch loss: 0.19843226671218872 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 9 current batch loss: 0.21865585446357727 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 10 current batch loss: 0.20793958008289337 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 11 current batch loss: 0.20324799418449402 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 12 current batch loss: 0.17920145392417908 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 13 current batch loss: 0.2197694331407547 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 14 current batch loss: 0.21014860272407532 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 15 current batch loss: 0.1705201268196106 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 16 current batch loss: 0.20215706527233124 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 17 current batch loss: 0.19405807554721832 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 18 current batch loss: 0.23967720568180084 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 19 current batch loss: 0.21234044432640076 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 20 current batch loss: 0.19830606877803802 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 21 current batch loss: 0.21798138320446014 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 22 current batch loss: 0.19958031177520752 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 23 current batch loss: 0.20090694725513458 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 24 current batch loss: 0.1924349069595337 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 25 current batch loss: 0.17234010994434357 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 26 current batch loss: 0.19312195479869843 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 27 current batch loss: 0.1968056559562683 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 28 current batch loss: 0.18704195320606232 current lr: 0.0006561000000000001\n",
      "epoch: 4 batch: 29 current batch loss: 0.2058786302804947 current lr: 0.0006561000000000001\n",
      "epoch: 5 batch: 0 current batch loss: 0.16902697086334229 current lr: 0.00059049\n",
      "epoch: 5 batch: 1 current batch loss: 0.1835336834192276 current lr: 0.00059049\n",
      "epoch: 5 batch: 2 current batch loss: 0.16271986067295074 current lr: 0.00059049\n",
      "epoch: 5 batch: 3 current batch loss: 0.1915673017501831 current lr: 0.00059049\n",
      "epoch: 5 batch: 4 current batch loss: 0.14859573543071747 current lr: 0.00059049\n",
      "epoch: 5 batch: 5 current batch loss: 0.17102806270122528 current lr: 0.00059049\n",
      "epoch: 5 batch: 6 current batch loss: 0.1604175865650177 current lr: 0.00059049\n",
      "epoch: 5 batch: 7 current batch loss: 0.19089990854263306 current lr: 0.00059049\n",
      "epoch: 5 batch: 8 current batch loss: 0.1506805419921875 current lr: 0.00059049\n",
      "epoch: 5 batch: 9 current batch loss: 0.16085857152938843 current lr: 0.00059049\n",
      "epoch: 5 batch: 10 current batch loss: 0.1777154505252838 current lr: 0.00059049\n",
      "epoch: 5 batch: 11 current batch loss: 0.18580366671085358 current lr: 0.00059049\n",
      "epoch: 5 batch: 12 current batch loss: 0.1331270933151245 current lr: 0.00059049\n",
      "epoch: 5 batch: 13 current batch loss: 0.17262476682662964 current lr: 0.00059049\n",
      "epoch: 5 batch: 14 current batch loss: 0.1713070124387741 current lr: 0.00059049\n",
      "epoch: 5 batch: 15 current batch loss: 0.1478903591632843 current lr: 0.00059049\n",
      "epoch: 5 batch: 16 current batch loss: 0.18772129714488983 current lr: 0.00059049\n",
      "epoch: 5 batch: 17 current batch loss: 0.1654534637928009 current lr: 0.00059049\n",
      "epoch: 5 batch: 18 current batch loss: 0.17166811227798462 current lr: 0.00059049\n",
      "epoch: 5 batch: 19 current batch loss: 0.1745714396238327 current lr: 0.00059049\n",
      "epoch: 5 batch: 20 current batch loss: 0.16228291392326355 current lr: 0.00059049\n",
      "epoch: 5 batch: 21 current batch loss: 0.15937621891498566 current lr: 0.00059049\n",
      "epoch: 5 batch: 22 current batch loss: 0.15671104192733765 current lr: 0.00059049\n",
      "epoch: 5 batch: 23 current batch loss: 0.152472123503685 current lr: 0.00059049\n",
      "epoch: 5 batch: 24 current batch loss: 0.15941648185253143 current lr: 0.00059049\n",
      "epoch: 5 batch: 25 current batch loss: 0.15299734473228455 current lr: 0.00059049\n",
      "epoch: 5 batch: 26 current batch loss: 0.16309694945812225 current lr: 0.00059049\n",
      "epoch: 5 batch: 27 current batch loss: 0.17106449604034424 current lr: 0.00059049\n",
      "epoch: 5 batch: 28 current batch loss: 0.16018801927566528 current lr: 0.00059049\n",
      "epoch: 5 batch: 29 current batch loss: 0.14339181780815125 current lr: 0.00059049\n",
      "epoch: 6 batch: 0 current batch loss: 0.15190662443637848 current lr: 0.000531441\n",
      "epoch: 6 batch: 1 current batch loss: 0.15504366159439087 current lr: 0.000531441\n",
      "epoch: 6 batch: 2 current batch loss: 0.14660529792308807 current lr: 0.000531441\n",
      "epoch: 6 batch: 3 current batch loss: 0.17784345149993896 current lr: 0.000531441\n",
      "epoch: 6 batch: 4 current batch loss: 0.1443638801574707 current lr: 0.000531441\n",
      "epoch: 6 batch: 5 current batch loss: 0.144562765955925 current lr: 0.000531441\n",
      "epoch: 6 batch: 6 current batch loss: 0.142711341381073 current lr: 0.000531441\n",
      "epoch: 6 batch: 7 current batch loss: 0.16760742664337158 current lr: 0.000531441\n",
      "epoch: 6 batch: 8 current batch loss: 0.13869895040988922 current lr: 0.000531441\n",
      "epoch: 6 batch: 9 current batch loss: 0.1497792750597 current lr: 0.000531441\n",
      "epoch: 6 batch: 10 current batch loss: 0.14096763730049133 current lr: 0.000531441\n",
      "epoch: 6 batch: 11 current batch loss: 0.15794046223163605 current lr: 0.000531441\n",
      "epoch: 6 batch: 12 current batch loss: 0.1431860774755478 current lr: 0.000531441\n",
      "epoch: 6 batch: 13 current batch loss: 0.15532930195331573 current lr: 0.000531441\n",
      "epoch: 6 batch: 14 current batch loss: 0.13836051523685455 current lr: 0.000531441\n",
      "epoch: 6 batch: 15 current batch loss: 0.1409444659948349 current lr: 0.000531441\n",
      "epoch: 6 batch: 16 current batch loss: 0.1319880336523056 current lr: 0.000531441\n",
      "epoch: 6 batch: 17 current batch loss: 0.15501895546913147 current lr: 0.000531441\n",
      "epoch: 6 batch: 18 current batch loss: 0.12598486244678497 current lr: 0.000531441\n",
      "epoch: 6 batch: 19 current batch loss: 0.12659022212028503 current lr: 0.000531441\n",
      "epoch: 6 batch: 20 current batch loss: 0.12619560956954956 current lr: 0.000531441\n",
      "epoch: 6 batch: 21 current batch loss: 0.1512756198644638 current lr: 0.000531441\n",
      "epoch: 6 batch: 22 current batch loss: 0.14142577350139618 current lr: 0.000531441\n",
      "epoch: 6 batch: 23 current batch loss: 0.14680175483226776 current lr: 0.000531441\n",
      "epoch: 6 batch: 24 current batch loss: 0.13818977773189545 current lr: 0.000531441\n",
      "epoch: 6 batch: 25 current batch loss: 0.1363157033920288 current lr: 0.000531441\n",
      "epoch: 6 batch: 26 current batch loss: 0.1465299129486084 current lr: 0.000531441\n",
      "epoch: 6 batch: 27 current batch loss: 0.12275464832782745 current lr: 0.000531441\n",
      "epoch: 6 batch: 28 current batch loss: 0.13328278064727783 current lr: 0.000531441\n",
      "epoch: 6 batch: 29 current batch loss: 0.14571690559387207 current lr: 0.000531441\n",
      "epoch: 7 batch: 0 current batch loss: 0.12868443131446838 current lr: 0.0004782969\n",
      "epoch: 7 batch: 1 current batch loss: 0.1136385127902031 current lr: 0.0004782969\n",
      "epoch: 7 batch: 2 current batch loss: 0.1195346787571907 current lr: 0.0004782969\n",
      "epoch: 7 batch: 3 current batch loss: 0.1243068277835846 current lr: 0.0004782969\n",
      "epoch: 7 batch: 4 current batch loss: 0.13596586883068085 current lr: 0.0004782969\n",
      "epoch: 7 batch: 5 current batch loss: 0.1307670623064041 current lr: 0.0004782969\n",
      "epoch: 7 batch: 6 current batch loss: 0.13208669424057007 current lr: 0.0004782969\n",
      "epoch: 7 batch: 7 current batch loss: 0.14713793992996216 current lr: 0.0004782969\n",
      "epoch: 7 batch: 8 current batch loss: 0.14881713688373566 current lr: 0.0004782969\n",
      "epoch: 7 batch: 9 current batch loss: 0.12084921449422836 current lr: 0.0004782969\n",
      "epoch: 7 batch: 10 current batch loss: 0.12074632197618484 current lr: 0.0004782969\n",
      "epoch: 7 batch: 11 current batch loss: 0.12441133707761765 current lr: 0.0004782969\n",
      "epoch: 7 batch: 12 current batch loss: 0.1437309831380844 current lr: 0.0004782969\n",
      "epoch: 7 batch: 13 current batch loss: 0.1465131640434265 current lr: 0.0004782969\n",
      "epoch: 7 batch: 14 current batch loss: 0.11871866136789322 current lr: 0.0004782969\n",
      "epoch: 7 batch: 15 current batch loss: 0.13038259744644165 current lr: 0.0004782969\n",
      "epoch: 7 batch: 16 current batch loss: 0.12913507223129272 current lr: 0.0004782969\n",
      "epoch: 7 batch: 17 current batch loss: 0.12477213889360428 current lr: 0.0004782969\n",
      "epoch: 7 batch: 18 current batch loss: 0.10402859002351761 current lr: 0.0004782969\n",
      "epoch: 7 batch: 19 current batch loss: 0.13535211980342865 current lr: 0.0004782969\n",
      "epoch: 7 batch: 20 current batch loss: 0.13048449158668518 current lr: 0.0004782969\n",
      "epoch: 7 batch: 21 current batch loss: 0.12098798155784607 current lr: 0.0004782969\n",
      "epoch: 7 batch: 22 current batch loss: 0.10489294677972794 current lr: 0.0004782969\n",
      "epoch: 7 batch: 23 current batch loss: 0.12689723074436188 current lr: 0.0004782969\n",
      "epoch: 7 batch: 24 current batch loss: 0.11023015528917313 current lr: 0.0004782969\n",
      "epoch: 7 batch: 25 current batch loss: 0.11221943795681 current lr: 0.0004782969\n",
      "epoch: 7 batch: 26 current batch loss: 0.12602904438972473 current lr: 0.0004782969\n",
      "epoch: 7 batch: 27 current batch loss: 0.1099097728729248 current lr: 0.0004782969\n",
      "epoch: 7 batch: 28 current batch loss: 0.1323641836643219 current lr: 0.0004782969\n",
      "epoch: 7 batch: 29 current batch loss: 0.12370186299085617 current lr: 0.0004782969\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #5\n",
    "\n",
    "Well, it seems that we were able to get the learning rate much below 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) Some other schedulers you might want to experiment with: \n",
    "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
    "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
    "\n",
    "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
    "### Your task #6\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9713\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9604\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603b03e-a7da-4b3e-a2eb-5e8b87324fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
