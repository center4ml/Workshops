{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers separated by nonlinear components (also called *activation functions*). A nonlinear component in-between the linear layers is essential: without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. Also, please note that it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
    "\n",
    "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. You can read more about this dataset here: https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f69064-608d-4e2c-9179-9ff00a3afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
   "metadata": {},
   "source": [
    "### Reading MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snowakowski/venv/jupyter/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
    "pyplot.imshow(train_image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target    #check if you classified it correctly in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
   "metadata": {},
   "source": [
    "### Your task #1\n",
    "\n",
    "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199903-bff6-431b-b855-32d37683ef2b",
   "metadata": {},
   "source": [
    "### Your task #2\n",
    "\n",
    "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
    "\n",
    "*Tips:* \n",
    "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
    "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
   "metadata": {},
   "source": [
    "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c16a443-c40d-430f-977b-e6749192950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815882fa-4f93-403a-9221-36bb34649579",
   "metadata": {},
   "source": [
    "Let us visualise the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th batch labels : tensor([2, 5, 9,  ..., 7, 2, 3])\n",
      "1 -th batch labels : tensor([5, 9, 3,  ..., 4, 7, 7])\n",
      "2 -th batch labels : tensor([2, 8, 8,  ..., 9, 9, 8])\n",
      "3 -th batch labels : tensor([4, 1, 1,  ..., 6, 7, 1])\n",
      "4 -th batch labels : tensor([0, 1, 1,  ..., 8, 0, 8])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "\n",
    "        if i<5:\n",
    "            print(i, \"-th batch labels :\", batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
   "metadata": {},
   "source": [
    "### Your taks #3\n",
    "\n",
    "Labels are entities of order zero (constants), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
    "\n",
    "Your task is to visualise and inspect the number of orders in data in batch_inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
   "metadata": {},
   "source": [
    "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Now, a definition of a simple MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
    "                                          #that output from a preceding layer serves as input to the next layer \n",
    "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
    "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
    "            torch.nn.Linear(256, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.345842123031616\n",
      "epoch: 0 batch: 1 current batch loss: 2.410470724105835\n",
      "epoch: 0 batch: 2 current batch loss: 2.386631488800049\n",
      "epoch: 0 batch: 3 current batch loss: 2.3326854705810547\n",
      "epoch: 0 batch: 4 current batch loss: 2.2959706783294678\n",
      "epoch: 0 batch: 5 current batch loss: 2.2800686359405518\n",
      "epoch: 0 batch: 6 current batch loss: 2.259901762008667\n",
      "epoch: 0 batch: 7 current batch loss: 2.2586305141448975\n",
      "epoch: 0 batch: 8 current batch loss: 2.2351584434509277\n",
      "epoch: 0 batch: 9 current batch loss: 2.1836163997650146\n",
      "epoch: 0 batch: 10 current batch loss: 2.1334941387176514\n",
      "epoch: 0 batch: 11 current batch loss: 2.0596354007720947\n",
      "epoch: 0 batch: 12 current batch loss: 1.9855166673660278\n",
      "epoch: 0 batch: 13 current batch loss: 1.916955828666687\n",
      "epoch: 0 batch: 14 current batch loss: 1.8230472803115845\n",
      "epoch: 0 batch: 15 current batch loss: 1.7361259460449219\n",
      "epoch: 0 batch: 16 current batch loss: 1.6495928764343262\n",
      "epoch: 0 batch: 17 current batch loss: 1.5627086162567139\n",
      "epoch: 0 batch: 18 current batch loss: 1.4551892280578613\n",
      "epoch: 0 batch: 19 current batch loss: 1.4027959108352661\n",
      "epoch: 0 batch: 20 current batch loss: 1.3345847129821777\n",
      "epoch: 0 batch: 21 current batch loss: 1.3036247491836548\n",
      "epoch: 0 batch: 22 current batch loss: 1.2355667352676392\n",
      "epoch: 0 batch: 23 current batch loss: 1.1969889402389526\n",
      "epoch: 0 batch: 24 current batch loss: 1.1459412574768066\n",
      "epoch: 0 batch: 25 current batch loss: 1.0905147790908813\n",
      "epoch: 0 batch: 26 current batch loss: 1.069223403930664\n",
      "epoch: 0 batch: 27 current batch loss: 1.0167796611785889\n",
      "epoch: 0 batch: 28 current batch loss: 0.9880630373954773\n",
      "epoch: 0 batch: 29 current batch loss: 0.9760959148406982\n",
      "epoch: 1 batch: 0 current batch loss: 0.9128032922744751\n",
      "epoch: 1 batch: 1 current batch loss: 0.901719331741333\n",
      "epoch: 1 batch: 2 current batch loss: 0.8671702146530151\n",
      "epoch: 1 batch: 3 current batch loss: 0.8610166311264038\n",
      "epoch: 1 batch: 4 current batch loss: 0.8205967545509338\n",
      "epoch: 1 batch: 5 current batch loss: 0.783053457736969\n",
      "epoch: 1 batch: 6 current batch loss: 0.7447918057441711\n",
      "epoch: 1 batch: 7 current batch loss: 0.7311275601387024\n",
      "epoch: 1 batch: 8 current batch loss: 0.711770236492157\n",
      "epoch: 1 batch: 9 current batch loss: 0.6883838176727295\n",
      "epoch: 1 batch: 10 current batch loss: 0.6467869281768799\n",
      "epoch: 1 batch: 11 current batch loss: 0.6180897951126099\n",
      "epoch: 1 batch: 12 current batch loss: 0.6196107864379883\n",
      "epoch: 1 batch: 13 current batch loss: 0.6058779954910278\n",
      "epoch: 1 batch: 14 current batch loss: 0.5686425566673279\n",
      "epoch: 1 batch: 15 current batch loss: 0.5087456107139587\n",
      "epoch: 1 batch: 16 current batch loss: 0.5676192045211792\n",
      "epoch: 1 batch: 17 current batch loss: 0.5058216452598572\n",
      "epoch: 1 batch: 18 current batch loss: 0.5085778832435608\n",
      "epoch: 1 batch: 19 current batch loss: 0.4859914779663086\n",
      "epoch: 1 batch: 20 current batch loss: 0.4766615927219391\n",
      "epoch: 1 batch: 21 current batch loss: 0.45212623476982117\n",
      "epoch: 1 batch: 22 current batch loss: 0.4551834464073181\n",
      "epoch: 1 batch: 23 current batch loss: 0.43008533120155334\n",
      "epoch: 1 batch: 24 current batch loss: 0.4349852502346039\n",
      "epoch: 1 batch: 25 current batch loss: 0.435148149728775\n",
      "epoch: 1 batch: 26 current batch loss: 0.41549256443977356\n",
      "epoch: 1 batch: 27 current batch loss: 0.4301007390022278\n",
      "epoch: 1 batch: 28 current batch loss: 0.3931475579738617\n",
      "epoch: 1 batch: 29 current batch loss: 0.4008346199989319\n",
      "epoch: 2 batch: 0 current batch loss: 0.41106072068214417\n",
      "epoch: 2 batch: 1 current batch loss: 0.38296306133270264\n",
      "epoch: 2 batch: 2 current batch loss: 0.35397979617118835\n",
      "epoch: 2 batch: 3 current batch loss: 0.3679245114326477\n",
      "epoch: 2 batch: 4 current batch loss: 0.38609281182289124\n",
      "epoch: 2 batch: 5 current batch loss: 0.3634611964225769\n",
      "epoch: 2 batch: 6 current batch loss: 0.3518840968608856\n",
      "epoch: 2 batch: 7 current batch loss: 0.3574931025505066\n",
      "epoch: 2 batch: 8 current batch loss: 0.33552929759025574\n",
      "epoch: 2 batch: 9 current batch loss: 0.34790152311325073\n",
      "epoch: 2 batch: 10 current batch loss: 0.37154099345207214\n",
      "epoch: 2 batch: 11 current batch loss: 0.3655388057231903\n",
      "epoch: 2 batch: 12 current batch loss: 0.3348107933998108\n",
      "epoch: 2 batch: 13 current batch loss: 0.31493067741394043\n",
      "epoch: 2 batch: 14 current batch loss: 0.3076575696468353\n",
      "epoch: 2 batch: 15 current batch loss: 0.30484655499458313\n",
      "epoch: 2 batch: 16 current batch loss: 0.3063247799873352\n",
      "epoch: 2 batch: 17 current batch loss: 0.31455475091934204\n",
      "epoch: 2 batch: 18 current batch loss: 0.28076350688934326\n",
      "epoch: 2 batch: 19 current batch loss: 0.2807953953742981\n",
      "epoch: 2 batch: 20 current batch loss: 0.2929641902446747\n",
      "epoch: 2 batch: 21 current batch loss: 0.30859604477882385\n",
      "epoch: 2 batch: 22 current batch loss: 0.30733972787857056\n",
      "epoch: 2 batch: 23 current batch loss: 0.32931315898895264\n",
      "epoch: 2 batch: 24 current batch loss: 0.302605003118515\n",
      "epoch: 2 batch: 25 current batch loss: 0.2678808569908142\n",
      "epoch: 2 batch: 26 current batch loss: 0.28126829862594604\n",
      "epoch: 2 batch: 27 current batch loss: 0.27306458353996277\n",
      "epoch: 2 batch: 28 current batch loss: 0.264273077249527\n",
      "epoch: 2 batch: 29 current batch loss: 0.23424823582172394\n",
      "epoch: 3 batch: 0 current batch loss: 0.28406786918640137\n",
      "epoch: 3 batch: 1 current batch loss: 0.272766649723053\n",
      "epoch: 3 batch: 2 current batch loss: 0.2533355951309204\n",
      "epoch: 3 batch: 3 current batch loss: 0.27108249068260193\n",
      "epoch: 3 batch: 4 current batch loss: 0.24283601343631744\n",
      "epoch: 3 batch: 5 current batch loss: 0.23687225580215454\n",
      "epoch: 3 batch: 6 current batch loss: 0.24233390390872955\n",
      "epoch: 3 batch: 7 current batch loss: 0.24530749022960663\n",
      "epoch: 3 batch: 8 current batch loss: 0.2548263967037201\n",
      "epoch: 3 batch: 9 current batch loss: 0.255996972322464\n",
      "epoch: 3 batch: 10 current batch loss: 0.23587742447853088\n",
      "epoch: 3 batch: 11 current batch loss: 0.2464604526758194\n",
      "epoch: 3 batch: 12 current batch loss: 0.23173172771930695\n",
      "epoch: 3 batch: 13 current batch loss: 0.23870959877967834\n",
      "epoch: 3 batch: 14 current batch loss: 0.21316497027873993\n",
      "epoch: 3 batch: 15 current batch loss: 0.22287896275520325\n",
      "epoch: 3 batch: 16 current batch loss: 0.2410597801208496\n",
      "epoch: 3 batch: 17 current batch loss: 0.22774632275104523\n",
      "epoch: 3 batch: 18 current batch loss: 0.22548454999923706\n",
      "epoch: 3 batch: 19 current batch loss: 0.22909605503082275\n",
      "epoch: 3 batch: 20 current batch loss: 0.2248799353837967\n",
      "epoch: 3 batch: 21 current batch loss: 0.21137818694114685\n",
      "epoch: 3 batch: 22 current batch loss: 0.21313363313674927\n",
      "epoch: 3 batch: 23 current batch loss: 0.21001367270946503\n",
      "epoch: 3 batch: 24 current batch loss: 0.21903696656227112\n",
      "epoch: 3 batch: 25 current batch loss: 0.19563041627407074\n",
      "epoch: 3 batch: 26 current batch loss: 0.21139328181743622\n",
      "epoch: 3 batch: 27 current batch loss: 0.2142532765865326\n",
      "epoch: 3 batch: 28 current batch loss: 0.2011943906545639\n",
      "epoch: 3 batch: 29 current batch loss: 0.1927172839641571\n",
      "epoch: 4 batch: 0 current batch loss: 0.2086113542318344\n",
      "epoch: 4 batch: 1 current batch loss: 0.18604514002799988\n",
      "epoch: 4 batch: 2 current batch loss: 0.19454993307590485\n",
      "epoch: 4 batch: 3 current batch loss: 0.1907394379377365\n",
      "epoch: 4 batch: 4 current batch loss: 0.21332883834838867\n",
      "epoch: 4 batch: 5 current batch loss: 0.1982295960187912\n",
      "epoch: 4 batch: 6 current batch loss: 0.16561195254325867\n",
      "epoch: 4 batch: 7 current batch loss: 0.182851180434227\n",
      "epoch: 4 batch: 8 current batch loss: 0.1851772964000702\n",
      "epoch: 4 batch: 9 current batch loss: 0.1973874419927597\n",
      "epoch: 4 batch: 10 current batch loss: 0.18110014498233795\n",
      "epoch: 4 batch: 11 current batch loss: 0.17568613588809967\n",
      "epoch: 4 batch: 12 current batch loss: 0.19177398085594177\n",
      "epoch: 4 batch: 13 current batch loss: 0.164094939827919\n",
      "epoch: 4 batch: 14 current batch loss: 0.19578216969966888\n",
      "epoch: 4 batch: 15 current batch loss: 0.18034429848194122\n",
      "epoch: 4 batch: 16 current batch loss: 0.1723530888557434\n",
      "epoch: 4 batch: 17 current batch loss: 0.15260575711727142\n",
      "epoch: 4 batch: 18 current batch loss: 0.1761394441127777\n",
      "epoch: 4 batch: 19 current batch loss: 0.18505775928497314\n",
      "epoch: 4 batch: 20 current batch loss: 0.16088970005512238\n",
      "epoch: 4 batch: 21 current batch loss: 0.17661774158477783\n",
      "epoch: 4 batch: 22 current batch loss: 0.17706911265850067\n",
      "epoch: 4 batch: 23 current batch loss: 0.1765943467617035\n",
      "epoch: 4 batch: 24 current batch loss: 0.15691843628883362\n",
      "epoch: 4 batch: 25 current batch loss: 0.1806156188249588\n",
      "epoch: 4 batch: 26 current batch loss: 0.1777859628200531\n",
      "epoch: 4 batch: 27 current batch loss: 0.16071531176567078\n",
      "epoch: 4 batch: 28 current batch loss: 0.1630719155073166\n",
      "epoch: 4 batch: 29 current batch loss: 0.20312261581420898\n",
      "epoch: 5 batch: 0 current batch loss: 0.15797847509384155\n",
      "epoch: 5 batch: 1 current batch loss: 0.1513703465461731\n",
      "epoch: 5 batch: 2 current batch loss: 0.13838757574558258\n",
      "epoch: 5 batch: 3 current batch loss: 0.15173539519309998\n",
      "epoch: 5 batch: 4 current batch loss: 0.14938688278198242\n",
      "epoch: 5 batch: 5 current batch loss: 0.14705848693847656\n",
      "epoch: 5 batch: 6 current batch loss: 0.13456018269062042\n",
      "epoch: 5 batch: 7 current batch loss: 0.1795015037059784\n",
      "epoch: 5 batch: 8 current batch loss: 0.15617507696151733\n",
      "epoch: 5 batch: 9 current batch loss: 0.13771165907382965\n",
      "epoch: 5 batch: 10 current batch loss: 0.1369188278913498\n",
      "epoch: 5 batch: 11 current batch loss: 0.14015299081802368\n",
      "epoch: 5 batch: 12 current batch loss: 0.1450171321630478\n",
      "epoch: 5 batch: 13 current batch loss: 0.1438405066728592\n",
      "epoch: 5 batch: 14 current batch loss: 0.15350262820720673\n",
      "epoch: 5 batch: 15 current batch loss: 0.12916293740272522\n",
      "epoch: 5 batch: 16 current batch loss: 0.1504916250705719\n",
      "epoch: 5 batch: 17 current batch loss: 0.16534733772277832\n",
      "epoch: 5 batch: 18 current batch loss: 0.1316576451063156\n",
      "epoch: 5 batch: 19 current batch loss: 0.13514107465744019\n",
      "epoch: 5 batch: 20 current batch loss: 0.13673648238182068\n",
      "epoch: 5 batch: 21 current batch loss: 0.11505517363548279\n",
      "epoch: 5 batch: 22 current batch loss: 0.13758598268032074\n",
      "epoch: 5 batch: 23 current batch loss: 0.1306663602590561\n",
      "epoch: 5 batch: 24 current batch loss: 0.13975273072719574\n",
      "epoch: 5 batch: 25 current batch loss: 0.12568701803684235\n",
      "epoch: 5 batch: 26 current batch loss: 0.14526566863059998\n",
      "epoch: 5 batch: 27 current batch loss: 0.11423419415950775\n",
      "epoch: 5 batch: 28 current batch loss: 0.1455848664045334\n",
      "epoch: 5 batch: 29 current batch loss: 0.14361131191253662\n",
      "epoch: 6 batch: 0 current batch loss: 0.15033210813999176\n",
      "epoch: 6 batch: 1 current batch loss: 0.12446779012680054\n",
      "epoch: 6 batch: 2 current batch loss: 0.11122284084558487\n",
      "epoch: 6 batch: 3 current batch loss: 0.12518957257270813\n",
      "epoch: 6 batch: 4 current batch loss: 0.12237968295812607\n",
      "epoch: 6 batch: 5 current batch loss: 0.10928967595100403\n",
      "epoch: 6 batch: 6 current batch loss: 0.1271771639585495\n",
      "epoch: 6 batch: 7 current batch loss: 0.1260058432817459\n",
      "epoch: 6 batch: 8 current batch loss: 0.10969278216362\n",
      "epoch: 6 batch: 9 current batch loss: 0.12583209574222565\n",
      "epoch: 6 batch: 10 current batch loss: 0.11184777319431305\n",
      "epoch: 6 batch: 11 current batch loss: 0.10954837501049042\n",
      "epoch: 6 batch: 12 current batch loss: 0.111087366938591\n",
      "epoch: 6 batch: 13 current batch loss: 0.10805050283670425\n",
      "epoch: 6 batch: 14 current batch loss: 0.13724158704280853\n",
      "epoch: 6 batch: 15 current batch loss: 0.10291669517755508\n",
      "epoch: 6 batch: 16 current batch loss: 0.11116960644721985\n",
      "epoch: 6 batch: 17 current batch loss: 0.09879793971776962\n",
      "epoch: 6 batch: 18 current batch loss: 0.10337359458208084\n",
      "epoch: 6 batch: 19 current batch loss: 0.10893608629703522\n",
      "epoch: 6 batch: 20 current batch loss: 0.12584443390369415\n",
      "epoch: 6 batch: 21 current batch loss: 0.10956162214279175\n",
      "epoch: 6 batch: 22 current batch loss: 0.09182075411081314\n",
      "epoch: 6 batch: 23 current batch loss: 0.11808481812477112\n",
      "epoch: 6 batch: 24 current batch loss: 0.10920882970094681\n",
      "epoch: 6 batch: 25 current batch loss: 0.10097481310367584\n",
      "epoch: 6 batch: 26 current batch loss: 0.11138016730546951\n",
      "epoch: 6 batch: 27 current batch loss: 0.08741601556539536\n",
      "epoch: 6 batch: 28 current batch loss: 0.11427844315767288\n",
      "epoch: 6 batch: 29 current batch loss: 0.09948942810297012\n",
      "epoch: 7 batch: 0 current batch loss: 0.09422846138477325\n",
      "epoch: 7 batch: 1 current batch loss: 0.09809767454862595\n",
      "epoch: 7 batch: 2 current batch loss: 0.10010821372270584\n",
      "epoch: 7 batch: 3 current batch loss: 0.07653812319040298\n",
      "epoch: 7 batch: 4 current batch loss: 0.1136331558227539\n",
      "epoch: 7 batch: 5 current batch loss: 0.09268616884946823\n",
      "epoch: 7 batch: 6 current batch loss: 0.08104625344276428\n",
      "epoch: 7 batch: 7 current batch loss: 0.0770440623164177\n",
      "epoch: 7 batch: 8 current batch loss: 0.10609840601682663\n",
      "epoch: 7 batch: 9 current batch loss: 0.11314208805561066\n",
      "epoch: 7 batch: 10 current batch loss: 0.08842610567808151\n",
      "epoch: 7 batch: 11 current batch loss: 0.10213681310415268\n",
      "epoch: 7 batch: 12 current batch loss: 0.1112983450293541\n",
      "epoch: 7 batch: 13 current batch loss: 0.0953979566693306\n",
      "epoch: 7 batch: 14 current batch loss: 0.08054952323436737\n",
      "epoch: 7 batch: 15 current batch loss: 0.10591849684715271\n",
      "epoch: 7 batch: 16 current batch loss: 0.11482138186693192\n",
      "epoch: 7 batch: 17 current batch loss: 0.10123106837272644\n",
      "epoch: 7 batch: 18 current batch loss: 0.07733862847089767\n",
      "epoch: 7 batch: 19 current batch loss: 0.07905426621437073\n",
      "epoch: 7 batch: 20 current batch loss: 0.09765687584877014\n",
      "epoch: 7 batch: 21 current batch loss: 0.07566694170236588\n",
      "epoch: 7 batch: 22 current batch loss: 0.07500139623880386\n",
      "epoch: 7 batch: 23 current batch loss: 0.08939220011234283\n",
      "epoch: 7 batch: 24 current batch loss: 0.09166985750198364\n",
      "epoch: 7 batch: 25 current batch loss: 0.08202578127384186\n",
      "epoch: 7 batch: 26 current batch loss: 0.08211582154035568\n",
      "epoch: 7 batch: 27 current batch loss: 0.09641250967979431\n",
      "epoch: 7 batch: 28 current batch loss: 0.10154283791780472\n",
      "epoch: 7 batch: 29 current batch loss: 0.08514370769262314\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
    "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
    "\n",
    "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
   "metadata": {},
   "source": [
    "### Your task #4\n",
    "\n",
    "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
   "metadata": {},
   "source": [
    "### Training - the second approach\n",
    "\n",
    "\n",
    "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
    "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
    "- an initiation of a network\n",
    "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
    "- a definition of a scheduler to update the learning rate in an optimizer\n",
    "- running through multiple epochs and updating the network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch: 0 current batch loss: 2.3240222930908203 current lr: 0.001\n",
      "epoch: 0 batch: 1 current batch loss: 2.4209816455841064 current lr: 0.001\n",
      "epoch: 0 batch: 2 current batch loss: 2.352947950363159 current lr: 0.001\n",
      "epoch: 0 batch: 3 current batch loss: 2.291206121444702 current lr: 0.001\n",
      "epoch: 0 batch: 4 current batch loss: 2.2934882640838623 current lr: 0.001\n",
      "epoch: 0 batch: 5 current batch loss: 2.307936429977417 current lr: 0.001\n",
      "epoch: 0 batch: 6 current batch loss: 2.284846305847168 current lr: 0.001\n",
      "epoch: 0 batch: 7 current batch loss: 2.2221567630767822 current lr: 0.001\n",
      "epoch: 0 batch: 8 current batch loss: 2.190464735031128 current lr: 0.001\n",
      "epoch: 0 batch: 9 current batch loss: 2.1379220485687256 current lr: 0.001\n",
      "epoch: 0 batch: 10 current batch loss: 2.0991933345794678 current lr: 0.001\n",
      "epoch: 0 batch: 11 current batch loss: 2.0359067916870117 current lr: 0.001\n",
      "epoch: 0 batch: 12 current batch loss: 1.956426739692688 current lr: 0.001\n",
      "epoch: 0 batch: 13 current batch loss: 1.8864818811416626 current lr: 0.001\n",
      "epoch: 0 batch: 14 current batch loss: 1.8068108558654785 current lr: 0.001\n",
      "epoch: 0 batch: 15 current batch loss: 1.7023675441741943 current lr: 0.001\n",
      "epoch: 0 batch: 16 current batch loss: 1.6183403730392456 current lr: 0.001\n",
      "epoch: 0 batch: 17 current batch loss: 1.5323405265808105 current lr: 0.001\n",
      "epoch: 0 batch: 18 current batch loss: 1.45321786403656 current lr: 0.001\n",
      "epoch: 0 batch: 19 current batch loss: 1.3766857385635376 current lr: 0.001\n",
      "epoch: 0 batch: 20 current batch loss: 1.2941226959228516 current lr: 0.001\n",
      "epoch: 0 batch: 21 current batch loss: 1.2471544742584229 current lr: 0.001\n",
      "epoch: 0 batch: 22 current batch loss: 1.2057241201400757 current lr: 0.001\n",
      "epoch: 0 batch: 23 current batch loss: 1.1232072114944458 current lr: 0.001\n",
      "epoch: 0 batch: 24 current batch loss: 1.0954514741897583 current lr: 0.001\n",
      "epoch: 0 batch: 25 current batch loss: 1.0480867624282837 current lr: 0.001\n",
      "epoch: 0 batch: 26 current batch loss: 0.991519570350647 current lr: 0.001\n",
      "epoch: 0 batch: 27 current batch loss: 0.9398000240325928 current lr: 0.001\n",
      "epoch: 0 batch: 28 current batch loss: 0.91096431016922 current lr: 0.001\n",
      "epoch: 0 batch: 29 current batch loss: 0.8663027882575989 current lr: 0.001\n",
      "epoch: 1 batch: 0 current batch loss: 0.8560907244682312 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 1 current batch loss: 0.8036168813705444 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 2 current batch loss: 0.8089337944984436 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 3 current batch loss: 0.764367401599884 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 4 current batch loss: 0.7404723763465881 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 5 current batch loss: 0.7322911620140076 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 6 current batch loss: 0.6675602197647095 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 7 current batch loss: 0.6898825168609619 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 8 current batch loss: 0.6311261653900146 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 9 current batch loss: 0.635235071182251 current lr: 0.0009000000000000001\n",
      "epoch: 1 batch: 10 current batch loss: 0.6235344409942627 current lr: 0.0009000000000000001\n"
     ]
    }
   ],
   "source": [
    "net_with_scheduler = MLP()\n",
    "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
    "\n",
    "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
    "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
    "\n",
    "    loss = 0.0\n",
    "    for batch, data in enumerate(trainloader):\n",
    "        batch_inputs, batch_labels = data\n",
    "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
    "                                            #and MLP doesn't apply \n",
    "                                            #the nonlinear activation after the last layer\n",
    "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
    "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
    "        optimizer.step()     #but this line in fact updates our neural network. \n",
    "                                \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
   "metadata": {},
   "source": [
    "### Your task #5\n",
    "\n",
    "Well, it seems that we were able to get the learning rate much below 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. Ask the workshop trainer, there are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
    "\n",
    "Some other schedulers you might want to experiment with: \n",
    "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
    "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
    "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
    "\n",
    "### Your task #6\n",
    "\n",
    "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now we will test those two nets - the one without and the one with the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = 0\n",
    "wrong = 0\n",
    "\n",
    "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
    "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
    "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
    "\n",
    "        datapoint, label = data\n",
    "\n",
    "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
    "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
    "\n",
    "        if classification.item() == label.item():\n",
    "            good += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        \n",
    "print(\"accuracy = \", good/(good+wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
   "metadata": {},
   "source": [
    "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603b03e-a7da-4b3e-a2eb-5e8b87324fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
