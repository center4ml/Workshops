{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Workshops/blob/2023_1/Day1/1_simple_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
      "metadata": {
        "id": "fa309ca0-8167-40d3-8531-f52500c9e23d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers \n",
        "\n",
        "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
        "\n",
        "Linear layers must be separated by nonlinear components (also called *activation functions*). \n",
        "\n",
        "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
        "\n",
        "### Answers to task #1\n",
        "**REMOVED**\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf6b5b7-3bda-4b22-8e23-0fb9edfef2c3",
      "metadata": {
        "id": "ebf6b5b7-3bda-4b22-8e23-0fb9edfef2c3"
      },
      "source": [
        "# Before we begin\n",
        "\n",
        "There is a departure from the neuron-like biological terminology in ANN community. You will see even in this very simple example, that it is more convenient to think of layers not as of data points (neurons) but as of mathematical transformations (so a layer would be a matrix multiplication by weights or a layer would be aplication of nonlinear transform, or both, and in this latter case a layer would decompose further). \n",
        "\n",
        "In the case of more complex networks like Transformers it would be even hard to find a neuron analogy. Transformers explicitly work with matrices and generally with mathematical abstractions.\n",
        "\n",
        "The departure from the neural terminology is also justified by biology, itself. It turns out, that a single neuron in a brain behaves much more like a full artificial neural network than like an artificial neuron. \n",
        "\n",
        "The authors of the paper cited below show that, at the very least, a 5-layer 128-unit TCN — temporal convolutional network — is needed to simulate the I/O patterns of a pyramidal neuron at the millisecond resolution (single spike precision). To make a gross comparison: This means a single biological neuron needs between 640 and 2048 artificial neurons to be simulated adequately.\n",
        "\n",
        "[Beniaguev D, Segev I, London M. Single cortical neurons as deep artificial neural networks. Neuron. 2021 Sep 1;109(17):2727-2739.e3. doi: 10.1016/j.neuron.2021.07.002](https://pubmed.ncbi.nlm.nih.gov/34380016/)\n",
        "\n",
        "For the interested reader: [here you can read about Transformers](https://jalammar.github.io/illustrated-transformer/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68",
      "metadata": {
        "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
      "metadata": {
        "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72"
      },
      "source": [
        "### Reading MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
      "metadata": {
        "id": "a3f72963-c85d-49a6-9297-e14f823acc21"
      },
      "outputs": [],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
      "metadata": {
        "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
        "outputId": "07428acc-81d5-4e26-f6d7-30aa113878d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcTUlEQVR4nO3df3DU9b3v8dcCyQqaLI0hv0rAgD+wAvEWJWZAxJJLSOc4gIwHf3QGvF4cMXiKaPXGUZHWM2nxjrV6qd7TqURnxB+cEaiO5Y4GE441oQNKGW7blNBY4iEJFSe7IUgIyef+wXXrQgJ+1l3eSXg+Zr4zZPf75vvx69Znv9nNNwHnnBMAAOfYMOsFAADOTwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGG9gFP19vbq4MGDSktLUyAQsF4OAMCTc04dHR3Ky8vTsGH9X+cMuAAdPHhQ+fn51ssAAHxDzc3NGjt2bL/PD7gApaWlSZJm6vsaoRTj1QAAfJ1Qtz7QO9H/nvcnaQFat26dnnrqKbW2tqqwsFDPPfecpk+ffta5L7/tNkIpGhEgQAAw6Pz/O4ye7W2UpHwI4fXXX9eqVau0evVqffTRRyosLFRpaakOHTqUjMMBAAahpATo6aef1rJly3TnnXfqO9/5jl544QWNGjVKL774YjIOBwAYhBIeoOPHj2vXrl0qKSn5x0GGDVNJSYnq6upO27+rq0uRSCRmAwAMfQkP0Geffaaenh5lZ2fHPJ6dna3W1tbT9q+srFQoFIpufAIOAM4P5j+IWlFRoXA4HN2am5utlwQAOAcS/im4zMxMDR8+XG1tbTGPt7W1KScn57T9g8GggsFgopcBABjgEn4FlJqaqmnTpqm6ujr6WG9vr6qrq1VcXJzowwEABqmk/BzQqlWrtGTJEl1zzTWaPn26nnnmGXV2durOO+9MxuEAAINQUgK0ePFi/f3vf9fjjz+u1tZWXX311dq6detpH0wAAJy/As45Z72Ir4pEIgqFQpqt+dwJAQAGoROuWzXaonA4rPT09H73M/8UHADg/ESAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGG9AGAgCYzw/5/E8DGZSVhJYjQ8eElccz2jer1nxk885D0z6t6A90zr06neMx9d87r3jCR91tPpPVO08QHvmUtX1XvPDAVcAQEATBAgAICJhAfoiSeeUCAQiNkmTZqU6MMAAAa5pLwHdNVVV+m99977x0Hi+L46AGBoS0oZRowYoZycnGT81QCAISIp7wHt27dPeXl5mjBhgu644w4dOHCg3327uroUiURiNgDA0JfwABUVFamqqkpbt27V888/r6amJl1//fXq6Ojoc//KykqFQqHolp+fn+glAQAGoIQHqKysTLfccoumTp2q0tJSvfPOO2pvb9cbb7zR5/4VFRUKh8PRrbm5OdFLAgAMQEn/dMDo0aN1+eWXq7Gxsc/ng8GggsFgspcBABhgkv5zQEeOHNH+/fuVm5ub7EMBAAaRhAfowQcfVG1trT755BN9+OGHWrhwoYYPH67bbrst0YcCAAxiCf8W3KeffqrbbrtNhw8f1pgxYzRz5kzV19drzJgxiT4UAGAQS3iAXnvttUT/lRighl95mfeMC6Z4zxy8YbT3zBfX+d9EUpIyQv5z/1EY340uh5rfHk3znvnZ/5rnPbNjygbvmabuL7xnJOmnbf/VeybvP1xcxzofcS84AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBE0n8hHQa+ntnfjWvu6ap13jOXp6TGdSycW92ux3vm8eeWes+M6PS/cWfxxhXeM2n/ecJ7RpKCn/nfxHTUzh1xHet8xBUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHA3bCjYcDCuuV3H8r1nLk9pi+tYQ80DLdd5z/z1SKb3TNXEf/eekaRwr/9dqrOf/TCuYw1k/mcBPrgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNS6ERLa1xzz/3sFu+Zf53X6T0zfM9F3jN/uPc575l4PfnZVO+ZxpJR3jM97S3eM7cX3+s9I0mf/Iv/TIH+ENexcP7iCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSBG3jPV13jNj3rrYe6bn8OfeM1dN/m/eM5L0f2e96D3zm3+7wXsmq/1D75l4BOriu0Fogf+/WsAbV0AAABMECABgwjtA27dv10033aS8vDwFAgFt3rw55nnnnB5//HHl5uZq5MiRKikp0b59+xK1XgDAEOEdoM7OThUWFmrdunV9Pr927Vo9++yzeuGFF7Rjxw5deOGFKi0t1bFjx77xYgEAQ4f3hxDKyspUVlbW53POOT3zzDN69NFHNX/+fEnSyy+/rOzsbG3evFm33nrrN1stAGDISOh7QE1NTWptbVVJSUn0sVAopKKiItXV9f2xmq6uLkUikZgNADD0JTRAra2tkqTs7OyYx7Ozs6PPnaqyslKhUCi65efnJ3JJAIAByvxTcBUVFQqHw9GtubnZekkAgHMgoQHKycmRJLW1tcU83tbWFn3uVMFgUOnp6TEbAGDoS2iACgoKlJOTo+rq6uhjkUhEO3bsUHFxcSIPBQAY5Lw/BXfkyBE1NjZGv25qatLu3buVkZGhcePGaeXKlXryySd12WWXqaCgQI899pjy8vK0YMGCRK4bADDIeQdo586duvHGG6Nfr1q1SpK0ZMkSVVVV6aGHHlJnZ6fuvvtutbe3a+bMmdq6dasuuOCCxK0aADDoBZxzznoRXxWJRBQKhTRb8zUikGK9HAxSf/nf18Y3908veM/c+bc53jN/n9nhPaPeHv8ZwMAJ160abVE4HD7j+/rmn4IDAJyfCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYML71zEAg8GVD/8lrrk7p/jf2Xr9+Oqz73SKG24p955Je73eewYYyLgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNSDEk97eG45g4vv9J75sBvvvCe+R9Pvuw9U/HPC71n3Mch7xlJyv/XOv8h5+I6Fs5fXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSnwFb1/+JP3zK1rfuQ988rq/+k9s/s6/xuY6jr/EUm66sIV3jOX/arFe+bEXz/xnsHQwRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAi4Jxz1ov4qkgkolAopNmarxGBFOvlAEnhZlztPZP+00+9Z16d8H+8Z+I16f3/7j1zxZqw90zPvr96z+DcOuG6VaMtCofDSk9P73c/roAAACYIEADAhHeAtm/frptuukl5eXkKBALavHlzzPNLly5VIBCI2ebNm5eo9QIAhgjvAHV2dqqwsFDr1q3rd5958+appaUlur366qvfaJEAgKHH+zeilpWVqays7Iz7BINB5eTkxL0oAMDQl5T3gGpqapSVlaUrrrhCy5cv1+HDh/vdt6urS5FIJGYDAAx9CQ/QvHnz9PLLL6u6ulo/+9nPVFtbq7KyMvX09PS5f2VlpUKhUHTLz89P9JIAAAOQ97fgzubWW2+N/nnKlCmaOnWqJk6cqJqaGs2ZM+e0/SsqKrRq1aro15FIhAgBwHkg6R/DnjBhgjIzM9XY2Njn88FgUOnp6TEbAGDoS3qAPv30Ux0+fFi5ubnJPhQAYBDx/hbckSNHYq5mmpqatHv3bmVkZCgjI0Nr1qzRokWLlJOTo/379+uhhx7SpZdeqtLS0oQuHAAwuHkHaOfOnbrxxhujX3/5/s2SJUv0/PPPa8+ePXrppZfU3t6uvLw8zZ07Vz/5yU8UDAYTt2oAwKDHzUiBQWJ4dpb3zMHFl8Z1rB0P/8J7Zlgc39G/o2mu90x4Zv8/1oGBgZuRAgAGNAIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhI+K/kBpAcPW2HvGeyn/WfkaRjD53wnhkVSPWe+dUlb3vP/NPCld4zozbt8J5B8nEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GakgIHemVd7z+y/5QLvmclXf+I9I8V3Y9F4PPf5f/GeGbVlZxJWAgtcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKfAVgWsme8/85V/8b9z5qxkvec/MuuC498y51OW6vWfqPy/wP1Bvi/8MBiSugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFAPeiILx3jP778yL61hPLH7Ne2bRRZ/FdayB7JG2a7xnan9xnffMt16q857B0MEVEADABAECAJjwClBlZaWuvfZapaWlKSsrSwsWLFBDQ0PMPseOHVN5ebkuvvhiXXTRRVq0aJHa2toSumgAwODnFaDa2lqVl5ervr5e7777rrq7uzV37lx1dnZG97n//vv11ltvaePGjaqtrdXBgwd18803J3zhAIDBzetDCFu3bo35uqqqSllZWdq1a5dmzZqlcDisX//619qwYYO+973vSZLWr1+vK6+8UvX19bruOv83KQEAQ9M3eg8oHA5LkjIyMiRJu3btUnd3t0pKSqL7TJo0SePGjVNdXd+fdunq6lIkEonZAABDX9wB6u3t1cqVKzVjxgxNnjxZktTa2qrU1FSNHj06Zt/s7Gy1trb2+fdUVlYqFApFt/z8/HiXBAAYROIOUHl5ufbu3avXXvP/uYmvqqioUDgcjm7Nzc3f6O8DAAwOcf0g6ooVK/T2229r+/btGjt2bPTxnJwcHT9+XO3t7TFXQW1tbcrJyenz7woGgwoGg/EsAwAwiHldATnntGLFCm3atEnbtm1TQUFBzPPTpk1TSkqKqquro481NDTowIEDKi4uTsyKAQBDgtcVUHl5uTZs2KAtW7YoLS0t+r5OKBTSyJEjFQqFdNddd2nVqlXKyMhQenq67rvvPhUXF/MJOABADK8APf/885Kk2bNnxzy+fv16LV26VJL085//XMOGDdOiRYvU1dWl0tJS/fKXv0zIYgEAQ0fAOeesF/FVkUhEoVBIszVfIwIp1svBGYy4ZJz3THharvfM4h9vPftOp7hn9F+9Zwa6B1r8v4tQ90v/m4pKUkbV7/2HenviOhaGnhOuWzXaonA4rPT09H73415wAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMBHXb0TFwDUit+/fPHsmn794YVzHWl5Q6z1zW1pbXMcayFb850zvmY+ev9p7JvPf93rPZHTUec8A5wpXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5Geo4cL73Gf+b+z71nHrn0He+ZuSM7vWcGuraeL+Kam/WbB7xnJj36Z++ZjHb/m4T2ek8AAxtXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5Geo58ssC/9X+ZsjEJK0mcde0TvWd+UTvXeybQE/CemfRkk/eMJF3WtsN7pieuIwHgCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMBFwzjnrRXxVJBJRKBTSbM3XiECK9XIAAJ5OuG7VaIvC4bDS09P73Y8rIACACQIEADDhFaDKykpde+21SktLU1ZWlhYsWKCGhoaYfWbPnq1AIBCz3XPPPQldNABg8PMKUG1trcrLy1VfX693331X3d3dmjt3rjo7O2P2W7ZsmVpaWqLb2rVrE7poAMDg5/UbUbdu3RrzdVVVlbKysrRr1y7NmjUr+vioUaOUk5OTmBUCAIakb/QeUDgcliRlZGTEPP7KK68oMzNTkydPVkVFhY4ePdrv39HV1aVIJBKzAQCGPq8roK/q7e3VypUrNWPGDE2ePDn6+O23367x48crLy9Pe/bs0cMPP6yGhga9+eabff49lZWVWrNmTbzLAAAMUnH/HNDy5cv129/+Vh988IHGjh3b737btm3TnDlz1NjYqIkTJ572fFdXl7q6uqJfRyIR5efn83NAADBIfd2fA4rrCmjFihV6++23tX379jPGR5KKiookqd8ABYNBBYPBeJYBABjEvALknNN9992nTZs2qaamRgUFBWed2b17tyQpNzc3rgUCAIYmrwCVl5drw4YN2rJli9LS0tTa2ipJCoVCGjlypPbv368NGzbo+9//vi6++GLt2bNH999/v2bNmqWpU6cm5R8AADA4eb0HFAgE+nx8/fr1Wrp0qZqbm/WDH/xAe/fuVWdnp/Lz87Vw4UI9+uijZ/w+4FdxLzgAGNyS8h7Q2VqVn5+v2tpan78SAHCe4l5wAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATI6wXcCrnnCTphLolZ7wYAIC3E+qW9I//nvdnwAWoo6NDkvSB3jFeCQDgm+jo6FAoFOr3+YA7W6LOsd7eXh08eFBpaWkKBAIxz0UiEeXn56u5uVnp6elGK7THeTiJ83AS5+EkzsNJA+E8OOfU0dGhvLw8DRvW/zs9A+4KaNiwYRo7duwZ90lPTz+vX2Bf4jycxHk4ifNwEufhJOvzcKYrny/xIQQAgAkCBAAwMagCFAwGtXr1agWDQeulmOI8nMR5OInzcBLn4aTBdB4G3IcQAADnh0F1BQQAGDoIEADABAECAJggQAAAE4MmQOvWrdMll1yiCy64QEVFRfr9739vvaRz7oknnlAgEIjZJk2aZL2spNu+fbtuuukm5eXlKRAIaPPmzTHPO+f0+OOPKzc3VyNHjlRJSYn27dtns9gkOtt5WLp06Wmvj3nz5tksNkkqKyt17bXXKi0tTVlZWVqwYIEaGhpi9jl27JjKy8t18cUX66KLLtKiRYvU1tZmtOLk+DrnYfbs2ae9Hu655x6jFfdtUATo9ddf16pVq7R69Wp99NFHKiwsVGlpqQ4dOmS9tHPuqquuUktLS3T74IMPrJeUdJ2dnSosLNS6dev6fH7t2rV69tln9cILL2jHjh268MILVVpaqmPHjp3jlSbX2c6DJM2bNy/m9fHqq6+ewxUmX21trcrLy1VfX693331X3d3dmjt3rjo7O6P73H///Xrrrbe0ceNG1dbW6uDBg7r55psNV514X+c8SNKyZctiXg9r1641WnE/3CAwffp0V15eHv26p6fH5eXlucrKSsNVnXurV692hYWF1sswJclt2rQp+nVvb6/LyclxTz31VPSx9vZ2FwwG3auvvmqwwnPj1PPgnHNLlixx8+fPN1mPlUOHDjlJrra21jl38t99SkqK27hxY3SfP/3pT06Sq6urs1pm0p16Hpxz7oYbbnA//OEP7Rb1NQz4K6Djx49r165dKikpiT42bNgwlZSUqK6uznBlNvbt26e8vDxNmDBBd9xxhw4cOGC9JFNNTU1qbW2NeX2EQiEVFRWdl6+PmpoaZWVl6YorrtDy5ct1+PBh6yUlVTgcliRlZGRIknbt2qXu7u6Y18OkSZM0bty4If16OPU8fOmVV15RZmamJk+erIqKCh09etRief0acDcjPdVnn32mnp4eZWdnxzyenZ2tP//5z0arslFUVKSqqipdccUVamlp0Zo1a3T99ddr7969SktLs16eidbWVknq8/Xx5XPni3nz5unmm29WQUGB9u/fr0ceeURlZWWqq6vT8OHDrZeXcL29vVq5cqVmzJihyZMnSzr5ekhNTdXo0aNj9h3Kr4e+zoMk3X777Ro/frzy8vK0Z88ePfzww2poaNCbb75puNpYAz5A+IeysrLon6dOnaqioiKNHz9eb7zxhu666y7DlWEguPXWW6N/njJliqZOnaqJEyeqpqZGc+bMMVxZcpSXl2vv3r3nxfugZ9Lfebj77rujf54yZYpyc3M1Z84c7d+/XxMnTjzXy+zTgP8WXGZmpoYPH37ap1ja2tqUk5NjtKqBYfTo0br88svV2NhovRQzX74GeH2cbsKECcrMzBySr48VK1bo7bff1vvvvx/z61tycnJ0/Phxtbe3x+w/VF8P/Z2HvhQVFUnSgHo9DPgApaamatq0aaquro4+1tvbq+rqahUXFxuuzN6RI0e0f/9+5ebmWi/FTEFBgXJycmJeH5FIRDt27DjvXx+ffvqpDh8+PKReH845rVixQps2bdK2bdtUUFAQ8/y0adOUkpIS83poaGjQgQMHhtTr4WznoS+7d++WpIH1erD+FMTX8dprr7lgMOiqqqrcH//4R3f33Xe70aNHu9bWVuulnVMPPPCAq6mpcU1NTe53v/udKykpcZmZme7QoUPWS0uqjo4O9/HHH7uPP/7YSXJPP/20+/jjj93f/vY355xzP/3pT93o0aPdli1b3J49e9z8+fNdQUGB++KLL4xXnlhnOg8dHR3uwQcfdHV1da6pqcm999577rvf/a677LLL3LFjx6yXnjDLly93oVDI1dTUuJaWluh29OjR6D733HOPGzdunNu2bZvbuXOnKy4udsXFxYarTryznYfGxkb34x//2O3cudM1NTW5LVu2uAkTJrhZs2YZrzzWoAiQc84999xzbty4cS41NdVNnz7d1dfXWy/pnFu8eLHLzc11qamp7tvf/rZbvHixa2xstF5W0r3//vtO0mnbkiVLnHMnP4r92GOPuezsbBcMBt2cOXNcQ0OD7aKT4Ezn4ejRo27u3LluzJgxLiUlxY0fP94tW7ZsyP2ftL7++SW59evXR/f54osv3L333uu+9a1vuVGjRrmFCxe6lpYWu0UnwdnOw4EDB9ysWbNcRkaGCwaD7tJLL3U/+tGPXDgctl34Kfh1DAAAEwP+PSAAwNBEgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJj4f4W4/AnknuSPAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
      "metadata": {
        "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
        "outputId": "f6bec96b-6a85-4c1e-83ba-b4d5e6043b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
      "metadata": {
        "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
        "outputId": "6cd57dfb-7778-4c21-cf6d-9380f2b23f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
      "metadata": {
        "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86"
      },
      "source": [
        "\n",
        "### Your task #2\n",
        "\n",
        "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28199903-bff6-431b-b855-32d37683ef2b",
      "metadata": {
        "id": "28199903-bff6-431b-b855-32d37683ef2b"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
        "\n",
        "*Tips:* \n",
        "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
        "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
      "metadata": {
        "id": "a756edcc-434b-4bbd-b171-b589a27b7f37"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
      "metadata": {
        "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69"
      },
      "source": [
        "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c16a443-c40d-430f-977b-e6749192950e",
      "metadata": {
        "id": "5c16a443-c40d-430f-977b-e6749192950e"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815882fa-4f93-403a-9221-36bb34649579",
      "metadata": {
        "id": "815882fa-4f93-403a-9221-36bb34649579"
      },
      "source": [
        "Let us visualise the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
      "metadata": {
        "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
        "outputId": "622648c8-3ff4-47ad-c5ee-b567ca361bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([4, 0, 4,  ..., 1, 8, 7])\n",
            "1 -th batch labels : tensor([7, 1, 4,  ..., 4, 2, 1])\n",
            "2 -th batch labels : tensor([8, 1, 2,  ..., 6, 5, 7])\n",
            "3 -th batch labels : tensor([2, 2, 7,  ..., 0, 3, 4])\n",
            "4 -th batch labels : tensor([9, 1, 0,  ..., 7, 7, 9])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
      "metadata": {
        "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2"
      },
      "source": [
        "\n",
        "### Your taks #4\n",
        "\n",
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
        "\n",
        "Your task is to visualise and inspect the number of orders in data in batch_inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
      "metadata": {
        "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
      "metadata": {
        "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168"
      },
      "source": [
        "**REMOVED**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
      "metadata": {
        "id": "19b73524-9d07-4882-a2b0-3e29233213a8"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Now, a definition of a simple MLP network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
      "metadata": {
        "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
        "                                          #that output from a preceding layer serves as input to the next layer \n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
      "metadata": {
        "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training consists of \n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
      "metadata": {
        "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
        "outputId": "7d8b7c54-006c-4a0c-aa37-920757c16e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.320082187652588\n",
            "epoch: 0 batch: 1 current batch loss: 2.4529953002929688\n",
            "epoch: 0 batch: 2 current batch loss: 2.3369109630584717\n",
            "epoch: 0 batch: 3 current batch loss: 2.2928473949432373\n",
            "epoch: 0 batch: 4 current batch loss: 2.29526948928833\n",
            "epoch: 0 batch: 5 current batch loss: 2.3071696758270264\n",
            "epoch: 0 batch: 6 current batch loss: 2.2649247646331787\n",
            "epoch: 0 batch: 7 current batch loss: 2.241553544998169\n",
            "epoch: 0 batch: 8 current batch loss: 2.187735080718994\n",
            "epoch: 0 batch: 9 current batch loss: 2.148198366165161\n",
            "epoch: 0 batch: 10 current batch loss: 2.090977430343628\n",
            "epoch: 0 batch: 11 current batch loss: 2.030855894088745\n",
            "epoch: 0 batch: 12 current batch loss: 1.9510005712509155\n",
            "epoch: 0 batch: 13 current batch loss: 1.865456223487854\n",
            "epoch: 0 batch: 14 current batch loss: 1.768835186958313\n",
            "epoch: 0 batch: 15 current batch loss: 1.658790111541748\n",
            "epoch: 0 batch: 16 current batch loss: 1.6109082698822021\n",
            "epoch: 0 batch: 17 current batch loss: 1.5265185832977295\n",
            "epoch: 0 batch: 18 current batch loss: 1.466223120689392\n",
            "epoch: 0 batch: 19 current batch loss: 1.4136770963668823\n",
            "epoch: 0 batch: 20 current batch loss: 1.3254109621047974\n",
            "epoch: 0 batch: 21 current batch loss: 1.2766714096069336\n",
            "epoch: 0 batch: 22 current batch loss: 1.2123762369155884\n",
            "epoch: 0 batch: 23 current batch loss: 1.1853829622268677\n",
            "epoch: 0 batch: 24 current batch loss: 1.1059932708740234\n",
            "epoch: 0 batch: 25 current batch loss: 1.0370205640792847\n",
            "epoch: 0 batch: 26 current batch loss: 1.0163806676864624\n",
            "epoch: 0 batch: 27 current batch loss: 0.9909726977348328\n",
            "epoch: 0 batch: 28 current batch loss: 0.9404571652412415\n",
            "epoch: 0 batch: 29 current batch loss: 0.8938575387001038\n",
            "epoch: 1 batch: 0 current batch loss: 0.8563140034675598\n",
            "epoch: 1 batch: 1 current batch loss: 0.8157800436019897\n",
            "epoch: 1 batch: 2 current batch loss: 0.8094865679740906\n",
            "epoch: 1 batch: 3 current batch loss: 0.7392494082450867\n",
            "epoch: 1 batch: 4 current batch loss: 0.7445806860923767\n",
            "epoch: 1 batch: 5 current batch loss: 0.7209315896034241\n",
            "epoch: 1 batch: 6 current batch loss: 0.6938214302062988\n",
            "epoch: 1 batch: 7 current batch loss: 0.6723260283470154\n",
            "epoch: 1 batch: 8 current batch loss: 0.6285607218742371\n",
            "epoch: 1 batch: 9 current batch loss: 0.6159926652908325\n",
            "epoch: 1 batch: 10 current batch loss: 0.6153861284255981\n",
            "epoch: 1 batch: 11 current batch loss: 0.6127125024795532\n",
            "epoch: 1 batch: 12 current batch loss: 0.5576858520507812\n",
            "epoch: 1 batch: 13 current batch loss: 0.5330199599266052\n",
            "epoch: 1 batch: 14 current batch loss: 0.539739727973938\n",
            "epoch: 1 batch: 15 current batch loss: 0.5280892848968506\n",
            "epoch: 1 batch: 16 current batch loss: 0.5184484720230103\n",
            "epoch: 1 batch: 17 current batch loss: 0.4546082019805908\n",
            "epoch: 1 batch: 18 current batch loss: 0.4731418192386627\n",
            "epoch: 1 batch: 19 current batch loss: 0.47228097915649414\n",
            "epoch: 1 batch: 20 current batch loss: 0.4537380039691925\n",
            "epoch: 1 batch: 21 current batch loss: 0.42467015981674194\n",
            "epoch: 1 batch: 22 current batch loss: 0.46799296140670776\n",
            "epoch: 1 batch: 23 current batch loss: 0.44346868991851807\n",
            "epoch: 1 batch: 24 current batch loss: 0.4021635949611664\n",
            "epoch: 1 batch: 25 current batch loss: 0.44752758741378784\n",
            "epoch: 1 batch: 26 current batch loss: 0.4374679625034332\n",
            "epoch: 1 batch: 27 current batch loss: 0.4102936089038849\n",
            "epoch: 1 batch: 28 current batch loss: 0.3722129464149475\n",
            "epoch: 1 batch: 29 current batch loss: 0.4052269160747528\n",
            "epoch: 2 batch: 0 current batch loss: 0.3661634624004364\n",
            "epoch: 2 batch: 1 current batch loss: 0.3762901723384857\n",
            "epoch: 2 batch: 2 current batch loss: 0.3659723401069641\n",
            "epoch: 2 batch: 3 current batch loss: 0.37976640462875366\n",
            "epoch: 2 batch: 4 current batch loss: 0.33565813302993774\n",
            "epoch: 2 batch: 5 current batch loss: 0.35603466629981995\n",
            "epoch: 2 batch: 6 current batch loss: 0.3542960286140442\n",
            "epoch: 2 batch: 7 current batch loss: 0.3483886122703552\n",
            "epoch: 2 batch: 8 current batch loss: 0.38852444291114807\n",
            "epoch: 2 batch: 9 current batch loss: 0.3310663104057312\n",
            "epoch: 2 batch: 10 current batch loss: 0.29867133498191833\n",
            "epoch: 2 batch: 11 current batch loss: 0.3307861089706421\n",
            "epoch: 2 batch: 12 current batch loss: 0.3349192142486572\n",
            "epoch: 2 batch: 13 current batch loss: 0.33701133728027344\n",
            "epoch: 2 batch: 14 current batch loss: 0.31319916248321533\n",
            "epoch: 2 batch: 15 current batch loss: 0.2922707200050354\n",
            "epoch: 2 batch: 16 current batch loss: 0.3083009421825409\n",
            "epoch: 2 batch: 17 current batch loss: 0.30573713779449463\n",
            "epoch: 2 batch: 18 current batch loss: 0.3216206729412079\n",
            "epoch: 2 batch: 19 current batch loss: 0.2925354242324829\n",
            "epoch: 2 batch: 20 current batch loss: 0.3076823055744171\n",
            "epoch: 2 batch: 21 current batch loss: 0.2792307734489441\n",
            "epoch: 2 batch: 22 current batch loss: 0.28752610087394714\n",
            "epoch: 2 batch: 23 current batch loss: 0.2753826379776001\n",
            "epoch: 2 batch: 24 current batch loss: 0.2860901355743408\n",
            "epoch: 2 batch: 25 current batch loss: 0.2926761209964752\n",
            "epoch: 2 batch: 26 current batch loss: 0.3070618212223053\n",
            "epoch: 2 batch: 27 current batch loss: 0.2699209451675415\n",
            "epoch: 2 batch: 28 current batch loss: 0.2669820189476013\n",
            "epoch: 2 batch: 29 current batch loss: 0.2689298689365387\n",
            "epoch: 3 batch: 0 current batch loss: 0.26996710896492004\n",
            "epoch: 3 batch: 1 current batch loss: 0.26528191566467285\n",
            "epoch: 3 batch: 2 current batch loss: 0.2402791529893875\n",
            "epoch: 3 batch: 3 current batch loss: 0.25291767716407776\n",
            "epoch: 3 batch: 4 current batch loss: 0.2749948501586914\n",
            "epoch: 3 batch: 5 current batch loss: 0.243163600564003\n",
            "epoch: 3 batch: 6 current batch loss: 0.2570948600769043\n",
            "epoch: 3 batch: 7 current batch loss: 0.25031954050064087\n",
            "epoch: 3 batch: 8 current batch loss: 0.24255697429180145\n",
            "epoch: 3 batch: 9 current batch loss: 0.23800618946552277\n",
            "epoch: 3 batch: 10 current batch loss: 0.2271188348531723\n",
            "epoch: 3 batch: 11 current batch loss: 0.25750890374183655\n",
            "epoch: 3 batch: 12 current batch loss: 0.2290147989988327\n",
            "epoch: 3 batch: 13 current batch loss: 0.23017632961273193\n",
            "epoch: 3 batch: 14 current batch loss: 0.21205411851406097\n",
            "epoch: 3 batch: 15 current batch loss: 0.24894237518310547\n",
            "epoch: 3 batch: 16 current batch loss: 0.24084380269050598\n",
            "epoch: 3 batch: 17 current batch loss: 0.20714791119098663\n",
            "epoch: 3 batch: 18 current batch loss: 0.20013974606990814\n",
            "epoch: 3 batch: 19 current batch loss: 0.21695764362812042\n",
            "epoch: 3 batch: 20 current batch loss: 0.2108086198568344\n",
            "epoch: 3 batch: 21 current batch loss: 0.2125406712293625\n",
            "epoch: 3 batch: 22 current batch loss: 0.2463451474905014\n",
            "epoch: 3 batch: 23 current batch loss: 0.20132187008857727\n",
            "epoch: 3 batch: 24 current batch loss: 0.22914955019950867\n",
            "epoch: 3 batch: 25 current batch loss: 0.20909222960472107\n",
            "epoch: 3 batch: 26 current batch loss: 0.2086840569972992\n",
            "epoch: 3 batch: 27 current batch loss: 0.21496066451072693\n",
            "epoch: 3 batch: 28 current batch loss: 0.19335587322711945\n",
            "epoch: 3 batch: 29 current batch loss: 0.1973763257265091\n",
            "epoch: 4 batch: 0 current batch loss: 0.2028309851884842\n",
            "epoch: 4 batch: 1 current batch loss: 0.17807503044605255\n",
            "epoch: 4 batch: 2 current batch loss: 0.18489593267440796\n",
            "epoch: 4 batch: 3 current batch loss: 0.20037426054477692\n",
            "epoch: 4 batch: 4 current batch loss: 0.1796359121799469\n",
            "epoch: 4 batch: 5 current batch loss: 0.1805015355348587\n",
            "epoch: 4 batch: 6 current batch loss: 0.1765422374010086\n",
            "epoch: 4 batch: 7 current batch loss: 0.19785423576831818\n",
            "epoch: 4 batch: 8 current batch loss: 0.20456360280513763\n",
            "epoch: 4 batch: 9 current batch loss: 0.17675216495990753\n",
            "epoch: 4 batch: 10 current batch loss: 0.18850965797901154\n",
            "epoch: 4 batch: 11 current batch loss: 0.16034406423568726\n",
            "epoch: 4 batch: 12 current batch loss: 0.2008916735649109\n",
            "epoch: 4 batch: 13 current batch loss: 0.19383519887924194\n",
            "epoch: 4 batch: 14 current batch loss: 0.17745745182037354\n",
            "epoch: 4 batch: 15 current batch loss: 0.1770756095647812\n",
            "epoch: 4 batch: 16 current batch loss: 0.1731976717710495\n",
            "epoch: 4 batch: 17 current batch loss: 0.18238452076911926\n",
            "epoch: 4 batch: 18 current batch loss: 0.17127183079719543\n",
            "epoch: 4 batch: 19 current batch loss: 0.16055917739868164\n",
            "epoch: 4 batch: 20 current batch loss: 0.2002904713153839\n",
            "epoch: 4 batch: 21 current batch loss: 0.1504679173231125\n",
            "epoch: 4 batch: 22 current batch loss: 0.14434956014156342\n",
            "epoch: 4 batch: 23 current batch loss: 0.1729087084531784\n",
            "epoch: 4 batch: 24 current batch loss: 0.16901086270809174\n",
            "epoch: 4 batch: 25 current batch loss: 0.16437280178070068\n",
            "epoch: 4 batch: 26 current batch loss: 0.16408440470695496\n",
            "epoch: 4 batch: 27 current batch loss: 0.15731140971183777\n",
            "epoch: 4 batch: 28 current batch loss: 0.1772727072238922\n",
            "epoch: 4 batch: 29 current batch loss: 0.1603214144706726\n",
            "epoch: 5 batch: 0 current batch loss: 0.14526821672916412\n",
            "epoch: 5 batch: 1 current batch loss: 0.14083945751190186\n",
            "epoch: 5 batch: 2 current batch loss: 0.1437561810016632\n",
            "epoch: 5 batch: 3 current batch loss: 0.13995493948459625\n",
            "epoch: 5 batch: 4 current batch loss: 0.13579431176185608\n",
            "epoch: 5 batch: 5 current batch loss: 0.14208120107650757\n",
            "epoch: 5 batch: 6 current batch loss: 0.16093392670154572\n",
            "epoch: 5 batch: 7 current batch loss: 0.1518005132675171\n",
            "epoch: 5 batch: 8 current batch loss: 0.13581916689872742\n",
            "epoch: 5 batch: 9 current batch loss: 0.15214823186397552\n",
            "epoch: 5 batch: 10 current batch loss: 0.1587102711200714\n",
            "epoch: 5 batch: 11 current batch loss: 0.1532595157623291\n",
            "epoch: 5 batch: 12 current batch loss: 0.1369962990283966\n",
            "epoch: 5 batch: 13 current batch loss: 0.12423840910196304\n",
            "epoch: 5 batch: 14 current batch loss: 0.15584313869476318\n",
            "epoch: 5 batch: 15 current batch loss: 0.15562796592712402\n",
            "epoch: 5 batch: 16 current batch loss: 0.14287538826465607\n",
            "epoch: 5 batch: 17 current batch loss: 0.14354726672172546\n",
            "epoch: 5 batch: 18 current batch loss: 0.14114245772361755\n",
            "epoch: 5 batch: 19 current batch loss: 0.14412686228752136\n",
            "epoch: 5 batch: 20 current batch loss: 0.14786195755004883\n",
            "epoch: 5 batch: 21 current batch loss: 0.13977952301502228\n",
            "epoch: 5 batch: 22 current batch loss: 0.14132265746593475\n",
            "epoch: 5 batch: 23 current batch loss: 0.12623894214630127\n",
            "epoch: 5 batch: 24 current batch loss: 0.1404140442609787\n",
            "epoch: 5 batch: 25 current batch loss: 0.1216658279299736\n",
            "epoch: 5 batch: 26 current batch loss: 0.13374169170856476\n",
            "epoch: 5 batch: 27 current batch loss: 0.12735462188720703\n",
            "epoch: 5 batch: 28 current batch loss: 0.11273004859685898\n",
            "epoch: 5 batch: 29 current batch loss: 0.12318740040063858\n",
            "epoch: 6 batch: 0 current batch loss: 0.12921886146068573\n",
            "epoch: 6 batch: 1 current batch loss: 0.10719265043735504\n",
            "epoch: 6 batch: 2 current batch loss: 0.11827875673770905\n",
            "epoch: 6 batch: 3 current batch loss: 0.11353127658367157\n",
            "epoch: 6 batch: 4 current batch loss: 0.1410764753818512\n",
            "epoch: 6 batch: 5 current batch loss: 0.11295631527900696\n",
            "epoch: 6 batch: 6 current batch loss: 0.10310260951519012\n",
            "epoch: 6 batch: 7 current batch loss: 0.12603524327278137\n",
            "epoch: 6 batch: 8 current batch loss: 0.11052960902452469\n",
            "epoch: 6 batch: 9 current batch loss: 0.12903717160224915\n",
            "epoch: 6 batch: 10 current batch loss: 0.1125345453619957\n",
            "epoch: 6 batch: 11 current batch loss: 0.10704595595598221\n",
            "epoch: 6 batch: 12 current batch loss: 0.11069299280643463\n",
            "epoch: 6 batch: 13 current batch loss: 0.11032547056674957\n",
            "epoch: 6 batch: 14 current batch loss: 0.13464833796024323\n",
            "epoch: 6 batch: 15 current batch loss: 0.10557408630847931\n",
            "epoch: 6 batch: 16 current batch loss: 0.11312809586524963\n",
            "epoch: 6 batch: 17 current batch loss: 0.1039837896823883\n",
            "epoch: 6 batch: 18 current batch loss: 0.11612699925899506\n",
            "epoch: 6 batch: 19 current batch loss: 0.10188185423612595\n",
            "epoch: 6 batch: 20 current batch loss: 0.11154916882514954\n",
            "epoch: 6 batch: 21 current batch loss: 0.09987003356218338\n",
            "epoch: 6 batch: 22 current batch loss: 0.10883086174726486\n",
            "epoch: 6 batch: 23 current batch loss: 0.10459212213754654\n",
            "epoch: 6 batch: 24 current batch loss: 0.12457143515348434\n",
            "epoch: 6 batch: 25 current batch loss: 0.12764358520507812\n",
            "epoch: 6 batch: 26 current batch loss: 0.10844439268112183\n",
            "epoch: 6 batch: 27 current batch loss: 0.11449543386697769\n",
            "epoch: 6 batch: 28 current batch loss: 0.11272942274808884\n",
            "epoch: 6 batch: 29 current batch loss: 0.06869018822908401\n",
            "epoch: 7 batch: 0 current batch loss: 0.10923793166875839\n",
            "epoch: 7 batch: 1 current batch loss: 0.10459423810243607\n",
            "epoch: 7 batch: 2 current batch loss: 0.1011362299323082\n",
            "epoch: 7 batch: 3 current batch loss: 0.09672142565250397\n",
            "epoch: 7 batch: 4 current batch loss: 0.0934469997882843\n",
            "epoch: 7 batch: 5 current batch loss: 0.10128828138113022\n",
            "epoch: 7 batch: 6 current batch loss: 0.10711904615163803\n",
            "epoch: 7 batch: 7 current batch loss: 0.11194735765457153\n",
            "epoch: 7 batch: 8 current batch loss: 0.09509765356779099\n",
            "epoch: 7 batch: 9 current batch loss: 0.11342913657426834\n",
            "epoch: 7 batch: 10 current batch loss: 0.0931563675403595\n",
            "epoch: 7 batch: 11 current batch loss: 0.09183415025472641\n",
            "epoch: 7 batch: 12 current batch loss: 0.10235928744077682\n",
            "epoch: 7 batch: 13 current batch loss: 0.08620192110538483\n",
            "epoch: 7 batch: 14 current batch loss: 0.0905948206782341\n",
            "epoch: 7 batch: 15 current batch loss: 0.07994703203439713\n",
            "epoch: 7 batch: 16 current batch loss: 0.0900314450263977\n",
            "epoch: 7 batch: 17 current batch loss: 0.09177052229642868\n",
            "epoch: 7 batch: 18 current batch loss: 0.10474107414484024\n",
            "epoch: 7 batch: 19 current batch loss: 0.09389811009168625\n",
            "epoch: 7 batch: 20 current batch loss: 0.10580044239759445\n",
            "epoch: 7 batch: 21 current batch loss: 0.08857923746109009\n",
            "epoch: 7 batch: 22 current batch loss: 0.08662010729312897\n",
            "epoch: 7 batch: 23 current batch loss: 0.08051564544439316\n",
            "epoch: 7 batch: 24 current batch loss: 0.08978964388370514\n",
            "epoch: 7 batch: 25 current batch loss: 0.0764014795422554\n",
            "epoch: 7 batch: 26 current batch loss: 0.09479863196611404\n",
            "epoch: 7 batch: 27 current batch loss: 0.08169112354516983\n",
            "epoch: 7 batch: 28 current batch loss: 0.09169759601354599\n",
            "epoch: 7 batch: 29 current batch loss: 0.09274864941835403\n"
          ]
        }
      ],
      "source": [
        "net = MLP()\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
        "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
        "                                            #and MLP doesn't apply \n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network. \n",
        "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
      "metadata": {
        "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8"
      },
      "source": [
        "\n",
        "### Your task #5\n",
        "\n",
        "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either.\n",
        "\n",
        "An epoch is a one full passage through the whole training data. Why then, on the second epoch, the losses are different than in the first epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
      "metadata": {
        "id": "9b257c80-3965-4bdf-9e47-0da1be6891de"
      },
      "source": [
        "### Answers to task #5\n",
        "\n",
        "**REMOVED**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
      "metadata": {
        "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141"
      },
      "source": [
        "### Training - the second approach\n",
        "\n",
        "\n",
        "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
        "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- a definition of a scheduler to update the learning rate in an optimizer\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
      "metadata": {
        "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
        "outputId": "d7971d39-5ec0-499e-d069-994a0dc54b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.37093448638916 current lr: 0.001\n",
            "epoch: 0 batch: 1 current batch loss: 2.3960514068603516 current lr: 0.001\n",
            "epoch: 0 batch: 2 current batch loss: 2.386112689971924 current lr: 0.001\n",
            "epoch: 0 batch: 3 current batch loss: 2.327414035797119 current lr: 0.001\n",
            "epoch: 0 batch: 4 current batch loss: 2.309817314147949 current lr: 0.001\n",
            "epoch: 0 batch: 5 current batch loss: 2.2888028621673584 current lr: 0.001\n",
            "epoch: 0 batch: 6 current batch loss: 2.2699129581451416 current lr: 0.001\n",
            "epoch: 0 batch: 7 current batch loss: 2.2731716632843018 current lr: 0.001\n",
            "epoch: 0 batch: 8 current batch loss: 2.2644124031066895 current lr: 0.001\n",
            "epoch: 0 batch: 9 current batch loss: 2.2327938079833984 current lr: 0.001\n",
            "epoch: 0 batch: 10 current batch loss: 2.1863491535186768 current lr: 0.001\n",
            "epoch: 0 batch: 11 current batch loss: 2.132901430130005 current lr: 0.001\n",
            "epoch: 0 batch: 12 current batch loss: 2.0757265090942383 current lr: 0.001\n",
            "epoch: 0 batch: 13 current batch loss: 2.0075225830078125 current lr: 0.001\n",
            "epoch: 0 batch: 14 current batch loss: 1.9350117444992065 current lr: 0.001\n",
            "epoch: 0 batch: 15 current batch loss: 1.8536938428878784 current lr: 0.001\n",
            "epoch: 0 batch: 16 current batch loss: 1.759337067604065 current lr: 0.001\n",
            "epoch: 0 batch: 17 current batch loss: 1.6622824668884277 current lr: 0.001\n",
            "epoch: 0 batch: 18 current batch loss: 1.5923643112182617 current lr: 0.001\n",
            "epoch: 0 batch: 19 current batch loss: 1.5089240074157715 current lr: 0.001\n",
            "epoch: 0 batch: 20 current batch loss: 1.4342039823532104 current lr: 0.001\n",
            "epoch: 0 batch: 21 current batch loss: 1.3897831439971924 current lr: 0.001\n",
            "epoch: 0 batch: 22 current batch loss: 1.3163090944290161 current lr: 0.001\n",
            "epoch: 0 batch: 23 current batch loss: 1.2599138021469116 current lr: 0.001\n",
            "epoch: 0 batch: 24 current batch loss: 1.2012203931808472 current lr: 0.001\n",
            "epoch: 0 batch: 25 current batch loss: 1.1667190790176392 current lr: 0.001\n",
            "epoch: 0 batch: 26 current batch loss: 1.114136815071106 current lr: 0.001\n",
            "epoch: 0 batch: 27 current batch loss: 1.0558182001113892 current lr: 0.001\n",
            "epoch: 0 batch: 28 current batch loss: 1.0054365396499634 current lr: 0.001\n",
            "epoch: 0 batch: 29 current batch loss: 0.9469367861747742 current lr: 0.001\n",
            "epoch: 1 batch: 0 current batch loss: 0.9271702170372009 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 1 current batch loss: 0.9204674363136292 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 2 current batch loss: 0.8732812404632568 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 3 current batch loss: 0.8265628814697266 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 4 current batch loss: 0.8189936876296997 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 5 current batch loss: 0.7936018705368042 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 6 current batch loss: 0.7625749111175537 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 7 current batch loss: 0.7158132791519165 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 8 current batch loss: 0.7089443802833557 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 9 current batch loss: 0.6725167632102966 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 10 current batch loss: 0.6387351751327515 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 11 current batch loss: 0.6391707062721252 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 12 current batch loss: 0.6115565896034241 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 13 current batch loss: 0.6260900497436523 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 14 current batch loss: 0.5713925361633301 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 15 current batch loss: 0.6027611494064331 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 16 current batch loss: 0.5737786293029785 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 17 current batch loss: 0.5356172919273376 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 18 current batch loss: 0.5251199007034302 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 19 current batch loss: 0.518517017364502 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 20 current batch loss: 0.5063316822052002 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 21 current batch loss: 0.5024979114532471 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 22 current batch loss: 0.46066349744796753 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 23 current batch loss: 0.4499451816082001 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 24 current batch loss: 0.44322171807289124 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 25 current batch loss: 0.44955986738204956 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 26 current batch loss: 0.4648272395133972 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 27 current batch loss: 0.426055371761322 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 28 current batch loss: 0.4232441782951355 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 29 current batch loss: 0.3927304744720459 current lr: 0.0009000000000000001\n",
            "epoch: 2 batch: 0 current batch loss: 0.39832866191864014 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 1 current batch loss: 0.4002821147441864 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 2 current batch loss: 0.40948280692100525 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 3 current batch loss: 0.3941798210144043 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 4 current batch loss: 0.37547823786735535 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 5 current batch loss: 0.3614468276500702 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 6 current batch loss: 0.38171082735061646 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 7 current batch loss: 0.35915330052375793 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 8 current batch loss: 0.3778025209903717 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 9 current batch loss: 0.3856425881385803 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 10 current batch loss: 0.3164721727371216 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 11 current batch loss: 0.34902939200401306 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 12 current batch loss: 0.32794734835624695 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 13 current batch loss: 0.3384737968444824 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 14 current batch loss: 0.3191523551940918 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 15 current batch loss: 0.35253894329071045 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 16 current batch loss: 0.3458652198314667 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 17 current batch loss: 0.3302338123321533 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 18 current batch loss: 0.3150867521762848 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 19 current batch loss: 0.28608813881874084 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 20 current batch loss: 0.3118443787097931 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 21 current batch loss: 0.2927325963973999 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 22 current batch loss: 0.2913323938846588 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 23 current batch loss: 0.2932654917240143 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 24 current batch loss: 0.29021355509757996 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 25 current batch loss: 0.35237976908683777 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 26 current batch loss: 0.2772715389728546 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 27 current batch loss: 0.28872621059417725 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 28 current batch loss: 0.27760326862335205 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 29 current batch loss: 0.2803160846233368 current lr: 0.0008100000000000001\n",
            "epoch: 3 batch: 0 current batch loss: 0.2543969750404358 current lr: 0.000729\n",
            "epoch: 3 batch: 1 current batch loss: 0.2740750014781952 current lr: 0.000729\n",
            "epoch: 3 batch: 2 current batch loss: 0.26546981930732727 current lr: 0.000729\n",
            "epoch: 3 batch: 3 current batch loss: 0.27322766184806824 current lr: 0.000729\n",
            "epoch: 3 batch: 4 current batch loss: 0.29179006814956665 current lr: 0.000729\n",
            "epoch: 3 batch: 5 current batch loss: 0.2701450288295746 current lr: 0.000729\n",
            "epoch: 3 batch: 6 current batch loss: 0.24563904106616974 current lr: 0.000729\n",
            "epoch: 3 batch: 7 current batch loss: 0.25709062814712524 current lr: 0.000729\n",
            "epoch: 3 batch: 8 current batch loss: 0.25944870710372925 current lr: 0.000729\n",
            "epoch: 3 batch: 9 current batch loss: 0.2659680247306824 current lr: 0.000729\n",
            "epoch: 3 batch: 10 current batch loss: 0.25385695695877075 current lr: 0.000729\n",
            "epoch: 3 batch: 11 current batch loss: 0.244866743683815 current lr: 0.000729\n",
            "epoch: 3 batch: 12 current batch loss: 0.24227911233901978 current lr: 0.000729\n",
            "epoch: 3 batch: 13 current batch loss: 0.25501394271850586 current lr: 0.000729\n",
            "epoch: 3 batch: 14 current batch loss: 0.2552390992641449 current lr: 0.000729\n",
            "epoch: 3 batch: 15 current batch loss: 0.2881684899330139 current lr: 0.000729\n",
            "epoch: 3 batch: 16 current batch loss: 0.2758869230747223 current lr: 0.000729\n",
            "epoch: 3 batch: 17 current batch loss: 0.24715429544448853 current lr: 0.000729\n",
            "epoch: 3 batch: 18 current batch loss: 0.21780414879322052 current lr: 0.000729\n",
            "epoch: 3 batch: 19 current batch loss: 0.25658419728279114 current lr: 0.000729\n",
            "epoch: 3 batch: 20 current batch loss: 0.2399003803730011 current lr: 0.000729\n",
            "epoch: 3 batch: 21 current batch loss: 0.2480452060699463 current lr: 0.000729\n",
            "epoch: 3 batch: 22 current batch loss: 0.20988154411315918 current lr: 0.000729\n",
            "epoch: 3 batch: 23 current batch loss: 0.2539362907409668 current lr: 0.000729\n",
            "epoch: 3 batch: 24 current batch loss: 0.26011234521865845 current lr: 0.000729\n",
            "epoch: 3 batch: 25 current batch loss: 0.23370635509490967 current lr: 0.000729\n",
            "epoch: 3 batch: 26 current batch loss: 0.21153689920902252 current lr: 0.000729\n",
            "epoch: 3 batch: 27 current batch loss: 0.25927263498306274 current lr: 0.000729\n",
            "epoch: 3 batch: 28 current batch loss: 0.20611946284770966 current lr: 0.000729\n",
            "epoch: 3 batch: 29 current batch loss: 0.21520933508872986 current lr: 0.000729\n",
            "epoch: 4 batch: 0 current batch loss: 0.2201709896326065 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 1 current batch loss: 0.22170118987560272 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 2 current batch loss: 0.19512571394443512 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 3 current batch loss: 0.22039131820201874 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 4 current batch loss: 0.20174196362495422 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 5 current batch loss: 0.19566810131072998 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 6 current batch loss: 0.19150838255882263 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 7 current batch loss: 0.23471346497535706 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 8 current batch loss: 0.22137711942195892 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 9 current batch loss: 0.18458949029445648 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 10 current batch loss: 0.21140269935131073 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 11 current batch loss: 0.20072148740291595 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 12 current batch loss: 0.17755235731601715 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 13 current batch loss: 0.20169806480407715 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 14 current batch loss: 0.20752757787704468 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 15 current batch loss: 0.2141721397638321 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 16 current batch loss: 0.18876565992832184 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 17 current batch loss: 0.19858594238758087 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 18 current batch loss: 0.21404366195201874 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 19 current batch loss: 0.22089003026485443 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 20 current batch loss: 0.20109334588050842 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 21 current batch loss: 0.19582806527614594 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 22 current batch loss: 0.18970267474651337 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 23 current batch loss: 0.20876488089561462 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 24 current batch loss: 0.2071329653263092 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 25 current batch loss: 0.20028018951416016 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 26 current batch loss: 0.1888199746608734 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 27 current batch loss: 0.1773873269557953 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 28 current batch loss: 0.18320587277412415 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 29 current batch loss: 0.16726373136043549 current lr: 0.0006561000000000001\n",
            "epoch: 5 batch: 0 current batch loss: 0.18302305042743683 current lr: 0.00059049\n",
            "epoch: 5 batch: 1 current batch loss: 0.1953229457139969 current lr: 0.00059049\n",
            "epoch: 5 batch: 2 current batch loss: 0.17531374096870422 current lr: 0.00059049\n",
            "epoch: 5 batch: 3 current batch loss: 0.16740645468235016 current lr: 0.00059049\n",
            "epoch: 5 batch: 4 current batch loss: 0.19361363351345062 current lr: 0.00059049\n",
            "epoch: 5 batch: 5 current batch loss: 0.17696867883205414 current lr: 0.00059049\n",
            "epoch: 5 batch: 6 current batch loss: 0.16426652669906616 current lr: 0.00059049\n",
            "epoch: 5 batch: 7 current batch loss: 0.19607123732566833 current lr: 0.00059049\n",
            "epoch: 5 batch: 8 current batch loss: 0.16352292895317078 current lr: 0.00059049\n",
            "epoch: 5 batch: 9 current batch loss: 0.18021073937416077 current lr: 0.00059049\n",
            "epoch: 5 batch: 10 current batch loss: 0.17881621420383453 current lr: 0.00059049\n",
            "epoch: 5 batch: 11 current batch loss: 0.13990671932697296 current lr: 0.00059049\n",
            "epoch: 5 batch: 12 current batch loss: 0.18433107435703278 current lr: 0.00059049\n",
            "epoch: 5 batch: 13 current batch loss: 0.15478378534317017 current lr: 0.00059049\n",
            "epoch: 5 batch: 14 current batch loss: 0.15776623785495758 current lr: 0.00059049\n",
            "epoch: 5 batch: 15 current batch loss: 0.16227449476718903 current lr: 0.00059049\n",
            "epoch: 5 batch: 16 current batch loss: 0.15894867479801178 current lr: 0.00059049\n",
            "epoch: 5 batch: 17 current batch loss: 0.17743471264839172 current lr: 0.00059049\n",
            "epoch: 5 batch: 18 current batch loss: 0.15903061628341675 current lr: 0.00059049\n",
            "epoch: 5 batch: 19 current batch loss: 0.15637081861495972 current lr: 0.00059049\n",
            "epoch: 5 batch: 20 current batch loss: 0.18055187165737152 current lr: 0.00059049\n",
            "epoch: 5 batch: 21 current batch loss: 0.17121799290180206 current lr: 0.00059049\n",
            "epoch: 5 batch: 22 current batch loss: 0.1577192097902298 current lr: 0.00059049\n",
            "epoch: 5 batch: 23 current batch loss: 0.16138838231563568 current lr: 0.00059049\n",
            "epoch: 5 batch: 24 current batch loss: 0.16422335803508759 current lr: 0.00059049\n",
            "epoch: 5 batch: 25 current batch loss: 0.17361868917942047 current lr: 0.00059049\n",
            "epoch: 5 batch: 26 current batch loss: 0.15125201642513275 current lr: 0.00059049\n",
            "epoch: 5 batch: 27 current batch loss: 0.15588566660881042 current lr: 0.00059049\n",
            "epoch: 5 batch: 28 current batch loss: 0.16458642482757568 current lr: 0.00059049\n",
            "epoch: 5 batch: 29 current batch loss: 0.1883687824010849 current lr: 0.00059049\n",
            "epoch: 6 batch: 0 current batch loss: 0.15151788294315338 current lr: 0.000531441\n",
            "epoch: 6 batch: 1 current batch loss: 0.15391750633716583 current lr: 0.000531441\n",
            "epoch: 6 batch: 2 current batch loss: 0.1463266909122467 current lr: 0.000531441\n",
            "epoch: 6 batch: 3 current batch loss: 0.1415705531835556 current lr: 0.000531441\n",
            "epoch: 6 batch: 4 current batch loss: 0.1737806349992752 current lr: 0.000531441\n",
            "epoch: 6 batch: 5 current batch loss: 0.14862456917762756 current lr: 0.000531441\n",
            "epoch: 6 batch: 6 current batch loss: 0.1597893089056015 current lr: 0.000531441\n",
            "epoch: 6 batch: 7 current batch loss: 0.13843555748462677 current lr: 0.000531441\n",
            "epoch: 6 batch: 8 current batch loss: 0.14772988855838776 current lr: 0.000531441\n",
            "epoch: 6 batch: 9 current batch loss: 0.1393110156059265 current lr: 0.000531441\n",
            "epoch: 6 batch: 10 current batch loss: 0.16442644596099854 current lr: 0.000531441\n",
            "epoch: 6 batch: 11 current batch loss: 0.13439038395881653 current lr: 0.000531441\n",
            "epoch: 6 batch: 12 current batch loss: 0.13504314422607422 current lr: 0.000531441\n",
            "epoch: 6 batch: 13 current batch loss: 0.16245262324810028 current lr: 0.000531441\n",
            "epoch: 6 batch: 14 current batch loss: 0.15217795968055725 current lr: 0.000531441\n",
            "epoch: 6 batch: 15 current batch loss: 0.14014099538326263 current lr: 0.000531441\n",
            "epoch: 6 batch: 16 current batch loss: 0.1459033340215683 current lr: 0.000531441\n",
            "epoch: 6 batch: 17 current batch loss: 0.14781229197978973 current lr: 0.000531441\n",
            "epoch: 6 batch: 18 current batch loss: 0.1319490671157837 current lr: 0.000531441\n",
            "epoch: 6 batch: 19 current batch loss: 0.1537110060453415 current lr: 0.000531441\n",
            "epoch: 6 batch: 20 current batch loss: 0.14540761709213257 current lr: 0.000531441\n",
            "epoch: 6 batch: 21 current batch loss: 0.1438547968864441 current lr: 0.000531441\n",
            "epoch: 6 batch: 22 current batch loss: 0.16214428842067719 current lr: 0.000531441\n",
            "epoch: 6 batch: 23 current batch loss: 0.13996149599552155 current lr: 0.000531441\n",
            "epoch: 6 batch: 24 current batch loss: 0.1529041826725006 current lr: 0.000531441\n",
            "epoch: 6 batch: 25 current batch loss: 0.1473025530576706 current lr: 0.000531441\n",
            "epoch: 6 batch: 26 current batch loss: 0.13816401362419128 current lr: 0.000531441\n",
            "epoch: 6 batch: 27 current batch loss: 0.1331188678741455 current lr: 0.000531441\n",
            "epoch: 6 batch: 28 current batch loss: 0.1571023166179657 current lr: 0.000531441\n",
            "epoch: 6 batch: 29 current batch loss: 0.12824873626232147 current lr: 0.000531441\n",
            "epoch: 7 batch: 0 current batch loss: 0.13408546149730682 current lr: 0.0004782969\n",
            "epoch: 7 batch: 1 current batch loss: 0.1573995053768158 current lr: 0.0004782969\n",
            "epoch: 7 batch: 2 current batch loss: 0.13513271510601044 current lr: 0.0004782969\n",
            "epoch: 7 batch: 3 current batch loss: 0.12484467029571533 current lr: 0.0004782969\n",
            "epoch: 7 batch: 4 current batch loss: 0.11269000917673111 current lr: 0.0004782969\n",
            "epoch: 7 batch: 5 current batch loss: 0.15483349561691284 current lr: 0.0004782969\n",
            "epoch: 7 batch: 6 current batch loss: 0.1376362144947052 current lr: 0.0004782969\n",
            "epoch: 7 batch: 7 current batch loss: 0.142442524433136 current lr: 0.0004782969\n",
            "epoch: 7 batch: 8 current batch loss: 0.12545245885849 current lr: 0.0004782969\n",
            "epoch: 7 batch: 9 current batch loss: 0.12114936858415604 current lr: 0.0004782969\n",
            "epoch: 7 batch: 10 current batch loss: 0.15704253315925598 current lr: 0.0004782969\n",
            "epoch: 7 batch: 11 current batch loss: 0.12410000711679459 current lr: 0.0004782969\n",
            "epoch: 7 batch: 12 current batch loss: 0.1281479001045227 current lr: 0.0004782969\n",
            "epoch: 7 batch: 13 current batch loss: 0.14829309284687042 current lr: 0.0004782969\n",
            "epoch: 7 batch: 14 current batch loss: 0.13221564888954163 current lr: 0.0004782969\n",
            "epoch: 7 batch: 15 current batch loss: 0.12263485789299011 current lr: 0.0004782969\n",
            "epoch: 7 batch: 16 current batch loss: 0.1378784030675888 current lr: 0.0004782969\n",
            "epoch: 7 batch: 17 current batch loss: 0.13384225964546204 current lr: 0.0004782969\n",
            "epoch: 7 batch: 18 current batch loss: 0.11938416957855225 current lr: 0.0004782969\n",
            "epoch: 7 batch: 19 current batch loss: 0.11393001675605774 current lr: 0.0004782969\n",
            "epoch: 7 batch: 20 current batch loss: 0.13601984083652496 current lr: 0.0004782969\n",
            "epoch: 7 batch: 21 current batch loss: 0.12015075236558914 current lr: 0.0004782969\n",
            "epoch: 7 batch: 22 current batch loss: 0.10949793457984924 current lr: 0.0004782969\n",
            "epoch: 7 batch: 23 current batch loss: 0.11663378030061722 current lr: 0.0004782969\n",
            "epoch: 7 batch: 24 current batch loss: 0.11330106854438782 current lr: 0.0004782969\n",
            "epoch: 7 batch: 25 current batch loss: 0.1184765174984932 current lr: 0.0004782969\n",
            "epoch: 7 batch: 26 current batch loss: 0.126288041472435 current lr: 0.0004782969\n",
            "epoch: 7 batch: 27 current batch loss: 0.1387299746274948 current lr: 0.0004782969\n",
            "epoch: 7 batch: 28 current batch loss: 0.12486161291599274 current lr: 0.0004782969\n",
            "epoch: 7 batch: 29 current batch loss: 0.07514286786317825 current lr: 0.0004782969\n"
          ]
        }
      ],
      "source": [
        "net_with_scheduler = MLP()\n",
        "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
        "\n",
        "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
        "                                            #and MLP doesn't apply \n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        \n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network. \n",
        "                                \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
      "metadata": {
        "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592"
      },
      "source": [
        " \n",
        "### Your task #6\n",
        "\n",
        "Well, it seems that we were able to get the learning to levels of 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. There are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
        "\n",
        "Some other schedulers you might want to experiment with: \n",
        "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
        "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
        "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
        "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
        "\n",
        "### Your task #7\n",
        "\n",
        "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
      "metadata": {
        "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Now we will test those two nets - the one without and the one with the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
      "metadata": {
        "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
        "outputId": "9980da2a-7e4b-4132-f792-dc36d011b7b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9687\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
      "metadata": {
        "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
        "outputId": "ad1344da-174c-4bc7-c741-442d367cc9c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9607\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
      "metadata": {
        "id": "e4831d2c-ee3f-42be-969b-627d888692c8"
      },
      "source": [
        "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler.\n",
        "\n",
        "# Phase 2 - Hands on\n",
        "\n",
        "## Moving computations to GPU\n",
        "\n",
        "In the code above we didn't move the computations to GPU. The first task is moving the computations to GPU. How do you do that?\n",
        "\n",
        "Ability to compute on GPU is called CUDA:  Compute Unified Device Architecture.\n",
        "\n",
        "First, we check if CUDA is available:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DakCowPLQk9S",
        "outputId": "226b308e-7dfe-4cfb-9900-7ca89ad7d6f5"
      },
      "id": "DakCowPLQk9S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to make CUDA available if it is not? \n",
        "\n",
        "* On a local machine you must connect and properly configure a GPU card, it is out of scope of this workshop.\n",
        "\n",
        "* In Google Colab online, you click `Runtime` > `Change runtime` and change `Hardware acceleration` setting to `GPU`.\n",
        "\n",
        "* In Google Colab **Polish version**, you click `Środowisko wykonawcze` > `Zmień typ środowiska wykonawczego` and change `Akcelerator sprzętowy` setting to `GPU`.\n",
        "\n",
        "Once we have CUDA (or not: this case should be handled in the code, too), we are ready to determine the calculation device. Note that it is just a character string:\n"
      ],
      "metadata": {
        "id": "052Wt1tVRYYG"
      },
      "id": "052Wt1tVRYYG"
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  computational_device = 'cuda'\n",
        "else:\n",
        "  computational_device = 'cpu'\n",
        "\n",
        "print(computational_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcAjuc3iTBDe",
        "outputId": "5b9107cf-a4a8-4189-c0ad-93426bed6d7e"
      },
      "id": "pcAjuc3iTBDe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we move our neural network model and each tensor with batch data to GPU. We can do it by passing the `device = computational_device` argument during the model or tensor construction, or by calling a `to(computational_device)` method on a model or a tensor:"
      ],
      "metadata": {
        "id": "v5KnjANmTKJH"
      },
      "id": "v5KnjANmTKJH"
    },
    {
      "cell_type": "code",
      "source": [
        "layer = torch.nn.Linear(2048, 256, device = computational_device),\n",
        "layer = torch.nn.Linear(2048, 256).to(computational_device)"
      ],
      "metadata": {
        "id": "DXq45N-kTJYP"
      },
      "id": "DXq45N-kTJYP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "\n",
        "Enable CUDA computations in all the code above:\n",
        "\n",
        "1. Check if CUDA is available before the MLP definition\n",
        "2. Add the device field to the constructor and pass it to all constructed layers\n",
        "3. Add the device argument when constructing MLP\n",
        "4. Add the device arguments to all tensors that get passed to MLP"
      ],
      "metadata": {
        "id": "qL9aZbEYY2F3"
      },
      "id": "qL9aZbEYY2F3"
    },
    {
      "cell_type": "code",
      "source": [
        "########################################### check  if CUDA available #################################################\n",
        "\n",
        "\n",
        "########################################### MLP definition on GPU #################################################\n",
        "\n",
        "\n",
        "########################################### TRAINING ON GPU #################################################  \n",
        "\n"
      ],
      "metadata": {
        "id": "-kBZ_fJ5Y0dQ"
      },
      "id": "-kBZ_fJ5Y0dQ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ TESTING ON GPU #####################################################\n"
      ],
      "metadata": {
        "id": "dxuAIBnoczmP"
      },
      "id": "dxuAIBnoczmP",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desequencing forward computations\n",
        "\n",
        "In the MLP definition above we used a sequential layer. In order to demonstrate how in general one can perform arbitrary forward computations, we will remove the sequential layer now.\n",
        "\n",
        "## Task\n",
        "\n",
        "Remove the sequential layer\n",
        "\n",
        "1. Remove the sequential layer by using separate variables for the layers within.\n",
        "2. Change the forward flow to pass `x` through all layers.\n"
      ],
      "metadata": {
        "id": "mf5Svh3FY0G2"
      },
      "id": "mf5Svh3FY0G2"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################### MLP definition without sequential #################################################\n",
        "\n",
        "\n",
        "########################################### TRAINING ARBITRARY COMPUTATIONS NET #################################################  \n",
        "\n"
      ],
      "metadata": {
        "id": "IrCDoLrGg5CI"
      },
      "id": "IrCDoLrGg5CI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ TESTING ARBITRARY COMPUTATIONS NET #####################################################\n"
      ],
      "metadata": {
        "id": "uLOCWGZZjaXv"
      },
      "id": "uLOCWGZZjaXv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet implementation\n",
        "\n",
        "In the MLP definition above we used linear layers. Now we will implement LeCun network consisting of convolutional layers. Convolutions are much better suited for image analysis than linear layers.\n",
        "\n",
        "![LeNet](https://miro.medium.com/v2/resize:fit:1400/1*LYiQq1Gg_yHKszun23MocQ.png)\n",
        "(above: LeNet image from https://medium.com/mlearning-ai/lenet-and-mnist-handwritten-digit-classification-354f5646c590 by Khuyen Le)\n",
        "\n",
        "\n",
        "![LeNet from Wikipedia](https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg)\n",
        "\n",
        "\n",
        "\n",
        "## Task\n",
        "Replace the network definition with 2D covolutions as per the pictures above.\n",
        "\n",
        "Train and test the network."
      ],
      "metadata": {
        "id": "eGasjh6FlLy3"
      },
      "id": "eGasjh6FlLy3"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################### LeCun definition without sequential #################################################\n",
        "\n",
        "\n",
        "########################################### TRAINING ARBITRARY COMPUTATIONS NET #################################################  \n",
        "\n"
      ],
      "metadata": {
        "id": "z43yfBl1nYoc"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "z43yfBl1nYoc"
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ TESTING LeNet #####################################################\n"
      ],
      "metadata": {
        "id": "eK7LGapmp1_o"
      },
      "id": "eK7LGapmp1_o",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 What next\n",
        "\n",
        "First and foremost - **save your updated Colab notebook into GDrive**, so you can work with it later. \n",
        "\n",
        "Then - experiment yourself. Try to add a scheduler into LeNet training. Observe that in the final epochs the loss didn't improve much and that the final accuracy was very mediocre. It should supercede than of the MLP network!\n",
        "\n",
        "There are also aspects of ML we didn't even touch:\n",
        "\n",
        "1. Validation on an additional validation set, or cross-validation\n",
        "2. Initializing the network\n",
        "3. Choice of a scheduler (we used exclusively Adam)\n",
        "4. Choice of a batch size and accompanying learning rate\n",
        "5. Choice of a specific loss function for very specific tasks\n",
        "\n",
        "Those concepts constitute a broad and separate subject, each of them. Explore them and have fun!"
      ],
      "metadata": {
        "id": "5CAqB64su8J2"
      },
      "id": "5CAqB64su8J2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}