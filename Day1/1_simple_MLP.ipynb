{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Workshops/blob/2023_1_solutions/Day1/1_simple_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
      "metadata": {
        "id": "fa309ca0-8167-40d3-8531-f52500c9e23d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers \n",
        "\n",
        "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
        "\n",
        "Linear layers must be separated by nonlinear components (also called *activation functions*). \n",
        "\n",
        "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
        "\n",
        "### Answers to task #1\n",
        "A nonlinear component in-between the linear layers is essential: \n",
        "1. without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. \n",
        "2. it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf6b5b7-3bda-4b22-8e23-0fb9edfef2c3",
      "metadata": {
        "id": "ebf6b5b7-3bda-4b22-8e23-0fb9edfef2c3"
      },
      "source": [
        "# Before we begin\n",
        "\n",
        "There is a departure from the neuron-like biological terminology in ANN community. You will see even in this very simple example, that it is more convenient to think of layers not as of data points (neurons) but as of mathematical transformations (so a layer would be a matrix multiplication by weights or a layer would be aplication of nonlinear transform, or both, and in this latter case a layer would decompose further). \n",
        "\n",
        "In the case of more complex networks like Transformers it would be even hard to find a neuron analogy. Transformers explicitly work with matrices and generally with mathematical abstractions.\n",
        "\n",
        "The departure from the neural terminology is also justified by biology, itself. It turns out, that a single neuron in a brain behaves much more like a full artificial neural network than like an artificial neuron. \n",
        "\n",
        "The authors of the paper cited below show that, at the very least, a 5-layer 128-unit TCN — temporal convolutional network — is needed to simulate the I/O patterns of a pyramidal neuron at the millisecond resolution (single spike precision). To make a gross comparison: This means a single biological neuron needs between 640 and 2048 artificial neurons to be simulated adequately.\n",
        "\n",
        "[Beniaguev D, Segev I, London M. Single cortical neurons as deep artificial neural networks. Neuron. 2021 Sep 1;109(17):2727-2739.e3. doi: 10.1016/j.neuron.2021.07.002](https://pubmed.ncbi.nlm.nih.gov/34380016/)\n",
        "\n",
        "For the interested reader: [here you can read about Transformers](https://jalammar.github.io/illustrated-transformer/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68",
      "metadata": {
        "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
      "metadata": {
        "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72"
      },
      "source": [
        "### Reading MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
      "metadata": {
        "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e309bb1f-31f8-4502-e61b-fe05a2a22ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 315462211.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 105518897.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 127840056.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7361100.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
      "metadata": {
        "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
        "outputId": "21ed610f-c09d-44ed-c698-dd37356de8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
      "metadata": {
        "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
        "outputId": "3ec3cf3d-f70c-467b-c695-886676d1ef27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
      "metadata": {
        "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
        "outputId": "a16fc336-26c6-4ebe-8934-4ab3fdeea166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
      "metadata": {
        "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86"
      },
      "source": [
        "\n",
        "### Your task #2\n",
        "\n",
        "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28199903-bff6-431b-b855-32d37683ef2b",
      "metadata": {
        "id": "28199903-bff6-431b-b855-32d37683ef2b"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
        "\n",
        "*Tips:* \n",
        "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
        "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
      "metadata": {
        "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
        "outputId": "d6b15055-43e8-4301-9ddf-e39ee39adf63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1306604762738429, 0.30810780385646264)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
      "metadata": {
        "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69"
      },
      "source": [
        "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5c16a443-c40d-430f-977b-e6749192950e",
      "metadata": {
        "id": "5c16a443-c40d-430f-977b-e6749192950e"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815882fa-4f93-403a-9221-36bb34649579",
      "metadata": {
        "id": "815882fa-4f93-403a-9221-36bb34649579"
      },
      "source": [
        "Let us visualise the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
      "metadata": {
        "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
        "outputId": "b40b7968-2eca-47b2-c685-9155bd209fb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([5, 0, 1,  ..., 8, 1, 0])\n",
            "1 -th batch labels : tensor([5, 8, 8,  ..., 4, 1, 1])\n",
            "2 -th batch labels : tensor([7, 8, 2,  ..., 7, 9, 6])\n",
            "3 -th batch labels : tensor([6, 7, 1,  ..., 1, 8, 0])\n",
            "4 -th batch labels : tensor([1, 5, 6,  ..., 7, 7, 2])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
      "metadata": {
        "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2"
      },
      "source": [
        "\n",
        "### Your taks #4\n",
        "\n",
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
        "\n",
        "Your task is to visualise and inspect the number of orders in data in batch_inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
      "metadata": {
        "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
        "outputId": "5a7a6b8b-cc7c-4258-aa85-1b5fbcd11719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i==0:\n",
        "            print(i, \"-th batch inputs :\", batch_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
      "metadata": {
        "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168"
      },
      "source": [
        "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
      "metadata": {
        "id": "19b73524-9d07-4882-a2b0-3e29233213a8"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Now, a definition of a simple MLP network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
      "metadata": {
        "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
        "                                          #that output from a preceding layer serves as input to the next layer \n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
      "metadata": {
        "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training consists of \n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
      "metadata": {
        "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
        "outputId": "95886dc0-e7d1-43db-8ab8-7c060e86708f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.339576244354248\n",
            "epoch: 0 batch: 1 current batch loss: 2.413186550140381\n",
            "epoch: 0 batch: 2 current batch loss: 2.3464481830596924\n",
            "epoch: 0 batch: 3 current batch loss: 2.3209261894226074\n",
            "epoch: 0 batch: 4 current batch loss: 2.2870354652404785\n",
            "epoch: 0 batch: 5 current batch loss: 2.272700309753418\n",
            "epoch: 0 batch: 6 current batch loss: 2.2631330490112305\n",
            "epoch: 0 batch: 7 current batch loss: 2.251457929611206\n",
            "epoch: 0 batch: 8 current batch loss: 2.2118048667907715\n",
            "epoch: 0 batch: 9 current batch loss: 2.164492607116699\n",
            "epoch: 0 batch: 10 current batch loss: 2.097403049468994\n",
            "epoch: 0 batch: 11 current batch loss: 2.0331943035125732\n",
            "epoch: 0 batch: 12 current batch loss: 1.962060809135437\n",
            "epoch: 0 batch: 13 current batch loss: 1.8801320791244507\n",
            "epoch: 0 batch: 14 current batch loss: 1.8094474077224731\n",
            "epoch: 0 batch: 15 current batch loss: 1.704168677330017\n",
            "epoch: 0 batch: 16 current batch loss: 1.5998749732971191\n",
            "epoch: 0 batch: 17 current batch loss: 1.5295321941375732\n",
            "epoch: 0 batch: 18 current batch loss: 1.4429978132247925\n",
            "epoch: 0 batch: 19 current batch loss: 1.388857364654541\n",
            "epoch: 0 batch: 20 current batch loss: 1.3625450134277344\n",
            "epoch: 0 batch: 21 current batch loss: 1.3146946430206299\n",
            "epoch: 0 batch: 22 current batch loss: 1.2536834478378296\n",
            "epoch: 0 batch: 23 current batch loss: 1.2041386365890503\n",
            "epoch: 0 batch: 24 current batch loss: 1.142351508140564\n",
            "epoch: 0 batch: 25 current batch loss: 1.128132700920105\n",
            "epoch: 0 batch: 26 current batch loss: 1.1158382892608643\n",
            "epoch: 0 batch: 27 current batch loss: 1.0663455724716187\n",
            "epoch: 0 batch: 28 current batch loss: 1.0235899686813354\n",
            "epoch: 0 batch: 29 current batch loss: 1.0699381828308105\n",
            "epoch: 1 batch: 0 current batch loss: 0.9617345333099365\n",
            "epoch: 1 batch: 1 current batch loss: 0.9501630067825317\n",
            "epoch: 1 batch: 2 current batch loss: 0.9034892916679382\n",
            "epoch: 1 batch: 3 current batch loss: 0.8802212476730347\n",
            "epoch: 1 batch: 4 current batch loss: 0.8601652383804321\n",
            "epoch: 1 batch: 5 current batch loss: 0.8563631772994995\n",
            "epoch: 1 batch: 6 current batch loss: 0.8098488450050354\n",
            "epoch: 1 batch: 7 current batch loss: 0.7570850253105164\n",
            "epoch: 1 batch: 8 current batch loss: 0.7519600987434387\n",
            "epoch: 1 batch: 9 current batch loss: 0.7317857146263123\n",
            "epoch: 1 batch: 10 current batch loss: 0.714092493057251\n",
            "epoch: 1 batch: 11 current batch loss: 0.6894610524177551\n",
            "epoch: 1 batch: 12 current batch loss: 0.6996822953224182\n",
            "epoch: 1 batch: 13 current batch loss: 0.6371006965637207\n",
            "epoch: 1 batch: 14 current batch loss: 0.6456646919250488\n",
            "epoch: 1 batch: 15 current batch loss: 0.623633861541748\n",
            "epoch: 1 batch: 16 current batch loss: 0.63374924659729\n",
            "epoch: 1 batch: 17 current batch loss: 0.5693068504333496\n",
            "epoch: 1 batch: 18 current batch loss: 0.5887876749038696\n",
            "epoch: 1 batch: 19 current batch loss: 0.5941922664642334\n",
            "epoch: 1 batch: 20 current batch loss: 0.5611308813095093\n",
            "epoch: 1 batch: 21 current batch loss: 0.5361999273300171\n",
            "epoch: 1 batch: 22 current batch loss: 0.517997145652771\n",
            "epoch: 1 batch: 23 current batch loss: 0.5644095540046692\n",
            "epoch: 1 batch: 24 current batch loss: 0.5398820638656616\n",
            "epoch: 1 batch: 25 current batch loss: 0.5256354212760925\n",
            "epoch: 1 batch: 26 current batch loss: 0.48250654339790344\n",
            "epoch: 1 batch: 27 current batch loss: 0.5092335343360901\n",
            "epoch: 1 batch: 28 current batch loss: 0.47927126288414\n",
            "epoch: 1 batch: 29 current batch loss: 0.4736352860927582\n",
            "epoch: 2 batch: 0 current batch loss: 0.4613741338253021\n",
            "epoch: 2 batch: 1 current batch loss: 0.4488326609134674\n",
            "epoch: 2 batch: 2 current batch loss: 0.44865795969963074\n",
            "epoch: 2 batch: 3 current batch loss: 0.42822709679603577\n",
            "epoch: 2 batch: 4 current batch loss: 0.4264706075191498\n",
            "epoch: 2 batch: 5 current batch loss: 0.44786620140075684\n",
            "epoch: 2 batch: 6 current batch loss: 0.4085191488265991\n",
            "epoch: 2 batch: 7 current batch loss: 0.4411567747592926\n",
            "epoch: 2 batch: 8 current batch loss: 0.4086630642414093\n",
            "epoch: 2 batch: 9 current batch loss: 0.3705041706562042\n",
            "epoch: 2 batch: 10 current batch loss: 0.427926629781723\n",
            "epoch: 2 batch: 11 current batch loss: 0.411675363779068\n",
            "epoch: 2 batch: 12 current batch loss: 0.3722054660320282\n",
            "epoch: 2 batch: 13 current batch loss: 0.3980078101158142\n",
            "epoch: 2 batch: 14 current batch loss: 0.4026845693588257\n",
            "epoch: 2 batch: 15 current batch loss: 0.3721158504486084\n",
            "epoch: 2 batch: 16 current batch loss: 0.3612242341041565\n",
            "epoch: 2 batch: 17 current batch loss: 0.36871007084846497\n",
            "epoch: 2 batch: 18 current batch loss: 0.3601363003253937\n",
            "epoch: 2 batch: 19 current batch loss: 0.3406991958618164\n",
            "epoch: 2 batch: 20 current batch loss: 0.35078659653663635\n",
            "epoch: 2 batch: 21 current batch loss: 0.36361929774284363\n",
            "epoch: 2 batch: 22 current batch loss: 0.33368003368377686\n",
            "epoch: 2 batch: 23 current batch loss: 0.3325369954109192\n",
            "epoch: 2 batch: 24 current batch loss: 0.3281629979610443\n",
            "epoch: 2 batch: 25 current batch loss: 0.3487291634082794\n",
            "epoch: 2 batch: 26 current batch loss: 0.3351816236972809\n",
            "epoch: 2 batch: 27 current batch loss: 0.3339684009552002\n",
            "epoch: 2 batch: 28 current batch loss: 0.3120819926261902\n",
            "epoch: 2 batch: 29 current batch loss: 0.3417530953884125\n",
            "epoch: 3 batch: 0 current batch loss: 0.30422699451446533\n",
            "epoch: 3 batch: 1 current batch loss: 0.32660290598869324\n",
            "epoch: 3 batch: 2 current batch loss: 0.3369261920452118\n",
            "epoch: 3 batch: 3 current batch loss: 0.3042367994785309\n",
            "epoch: 3 batch: 4 current batch loss: 0.2979031503200531\n",
            "epoch: 3 batch: 5 current batch loss: 0.3157424032688141\n",
            "epoch: 3 batch: 6 current batch loss: 0.3010181188583374\n",
            "epoch: 3 batch: 7 current batch loss: 0.2710925042629242\n",
            "epoch: 3 batch: 8 current batch loss: 0.2896905243396759\n",
            "epoch: 3 batch: 9 current batch loss: 0.29207533597946167\n",
            "epoch: 3 batch: 10 current batch loss: 0.31522446870803833\n",
            "epoch: 3 batch: 11 current batch loss: 0.30009475350379944\n",
            "epoch: 3 batch: 12 current batch loss: 0.2607369124889374\n",
            "epoch: 3 batch: 13 current batch loss: 0.2729494571685791\n",
            "epoch: 3 batch: 14 current batch loss: 0.283927857875824\n",
            "epoch: 3 batch: 15 current batch loss: 0.2672329843044281\n",
            "epoch: 3 batch: 16 current batch loss: 0.31160303950309753\n",
            "epoch: 3 batch: 17 current batch loss: 0.2818063497543335\n",
            "epoch: 3 batch: 18 current batch loss: 0.2623670697212219\n",
            "epoch: 3 batch: 19 current batch loss: 0.2464682012796402\n",
            "epoch: 3 batch: 20 current batch loss: 0.23993851244449615\n",
            "epoch: 3 batch: 21 current batch loss: 0.27605167031288147\n",
            "epoch: 3 batch: 22 current batch loss: 0.26187223196029663\n",
            "epoch: 3 batch: 23 current batch loss: 0.2745954394340515\n",
            "epoch: 3 batch: 24 current batch loss: 0.2503940761089325\n",
            "epoch: 3 batch: 25 current batch loss: 0.2537965476512909\n",
            "epoch: 3 batch: 26 current batch loss: 0.23410901427268982\n",
            "epoch: 3 batch: 27 current batch loss: 0.25684693455696106\n",
            "epoch: 3 batch: 28 current batch loss: 0.22922661900520325\n",
            "epoch: 3 batch: 29 current batch loss: 0.25319787859916687\n",
            "epoch: 4 batch: 0 current batch loss: 0.2466355711221695\n",
            "epoch: 4 batch: 1 current batch loss: 0.24011139571666718\n",
            "epoch: 4 batch: 2 current batch loss: 0.2353779673576355\n",
            "epoch: 4 batch: 3 current batch loss: 0.24171878397464752\n",
            "epoch: 4 batch: 4 current batch loss: 0.22362841665744781\n",
            "epoch: 4 batch: 5 current batch loss: 0.22754129767417908\n",
            "epoch: 4 batch: 6 current batch loss: 0.22760280966758728\n",
            "epoch: 4 batch: 7 current batch loss: 0.24144698679447174\n",
            "epoch: 4 batch: 8 current batch loss: 0.23344279825687408\n",
            "epoch: 4 batch: 9 current batch loss: 0.2160383015871048\n",
            "epoch: 4 batch: 10 current batch loss: 0.22016535699367523\n",
            "epoch: 4 batch: 11 current batch loss: 0.20559562742710114\n",
            "epoch: 4 batch: 12 current batch loss: 0.23955175280570984\n",
            "epoch: 4 batch: 13 current batch loss: 0.23941469192504883\n",
            "epoch: 4 batch: 14 current batch loss: 0.21409282088279724\n",
            "epoch: 4 batch: 15 current batch loss: 0.19181543588638306\n",
            "epoch: 4 batch: 16 current batch loss: 0.19553212821483612\n",
            "epoch: 4 batch: 17 current batch loss: 0.2098166048526764\n",
            "epoch: 4 batch: 18 current batch loss: 0.19148893654346466\n",
            "epoch: 4 batch: 19 current batch loss: 0.2196340560913086\n",
            "epoch: 4 batch: 20 current batch loss: 0.19392648339271545\n",
            "epoch: 4 batch: 21 current batch loss: 0.20319019258022308\n",
            "epoch: 4 batch: 22 current batch loss: 0.20128341019153595\n",
            "epoch: 4 batch: 23 current batch loss: 0.20457248389720917\n",
            "epoch: 4 batch: 24 current batch loss: 0.19279560446739197\n",
            "epoch: 4 batch: 25 current batch loss: 0.21640199422836304\n",
            "epoch: 4 batch: 26 current batch loss: 0.20722737908363342\n",
            "epoch: 4 batch: 27 current batch loss: 0.21830789744853973\n",
            "epoch: 4 batch: 28 current batch loss: 0.21415609121322632\n",
            "epoch: 4 batch: 29 current batch loss: 0.22599637508392334\n",
            "epoch: 5 batch: 0 current batch loss: 0.2056552767753601\n",
            "epoch: 5 batch: 1 current batch loss: 0.2115100473165512\n",
            "epoch: 5 batch: 2 current batch loss: 0.20507994294166565\n",
            "epoch: 5 batch: 3 current batch loss: 0.19458375871181488\n",
            "epoch: 5 batch: 4 current batch loss: 0.18090182542800903\n",
            "epoch: 5 batch: 5 current batch loss: 0.18074367940425873\n",
            "epoch: 5 batch: 6 current batch loss: 0.17785385251045227\n",
            "epoch: 5 batch: 7 current batch loss: 0.21497052907943726\n",
            "epoch: 5 batch: 8 current batch loss: 0.16792182624340057\n",
            "epoch: 5 batch: 9 current batch loss: 0.17131337523460388\n",
            "epoch: 5 batch: 10 current batch loss: 0.17691963911056519\n",
            "epoch: 5 batch: 11 current batch loss: 0.18580061197280884\n",
            "epoch: 5 batch: 12 current batch loss: 0.19459262490272522\n",
            "epoch: 5 batch: 13 current batch loss: 0.16439126431941986\n",
            "epoch: 5 batch: 14 current batch loss: 0.17791537940502167\n",
            "epoch: 5 batch: 15 current batch loss: 0.18844251334667206\n",
            "epoch: 5 batch: 16 current batch loss: 0.19149662554264069\n",
            "epoch: 5 batch: 17 current batch loss: 0.18139654397964478\n",
            "epoch: 5 batch: 18 current batch loss: 0.20954209566116333\n",
            "epoch: 5 batch: 19 current batch loss: 0.17914389073848724\n",
            "epoch: 5 batch: 20 current batch loss: 0.14905260503292084\n",
            "epoch: 5 batch: 21 current batch loss: 0.18325956165790558\n",
            "epoch: 5 batch: 22 current batch loss: 0.17069384455680847\n",
            "epoch: 5 batch: 23 current batch loss: 0.1695941686630249\n",
            "epoch: 5 batch: 24 current batch loss: 0.1663176566362381\n",
            "epoch: 5 batch: 25 current batch loss: 0.16287900507450104\n",
            "epoch: 5 batch: 26 current batch loss: 0.1601514369249344\n",
            "epoch: 5 batch: 27 current batch loss: 0.16246075928211212\n",
            "epoch: 5 batch: 28 current batch loss: 0.14833104610443115\n",
            "epoch: 5 batch: 29 current batch loss: 0.1682872474193573\n",
            "epoch: 6 batch: 0 current batch loss: 0.15934722125530243\n",
            "epoch: 6 batch: 1 current batch loss: 0.15367744863033295\n",
            "epoch: 6 batch: 2 current batch loss: 0.16811002790927887\n",
            "epoch: 6 batch: 3 current batch loss: 0.14736972749233246\n",
            "epoch: 6 batch: 4 current batch loss: 0.17063897848129272\n",
            "epoch: 6 batch: 5 current batch loss: 0.14416900277137756\n",
            "epoch: 6 batch: 6 current batch loss: 0.14011788368225098\n",
            "epoch: 6 batch: 7 current batch loss: 0.143641397356987\n",
            "epoch: 6 batch: 8 current batch loss: 0.14451910555362701\n",
            "epoch: 6 batch: 9 current batch loss: 0.15899407863616943\n",
            "epoch: 6 batch: 10 current batch loss: 0.1589260697364807\n",
            "epoch: 6 batch: 11 current batch loss: 0.13819214701652527\n",
            "epoch: 6 batch: 12 current batch loss: 0.1499013751745224\n",
            "epoch: 6 batch: 13 current batch loss: 0.14242026209831238\n",
            "epoch: 6 batch: 14 current batch loss: 0.14828190207481384\n",
            "epoch: 6 batch: 15 current batch loss: 0.13346487283706665\n",
            "epoch: 6 batch: 16 current batch loss: 0.1410921812057495\n",
            "epoch: 6 batch: 17 current batch loss: 0.16795676946640015\n",
            "epoch: 6 batch: 18 current batch loss: 0.1603613793849945\n",
            "epoch: 6 batch: 19 current batch loss: 0.14753517508506775\n",
            "epoch: 6 batch: 20 current batch loss: 0.15042902529239655\n",
            "epoch: 6 batch: 21 current batch loss: 0.15441519021987915\n",
            "epoch: 6 batch: 22 current batch loss: 0.1677461564540863\n",
            "epoch: 6 batch: 23 current batch loss: 0.14916659891605377\n",
            "epoch: 6 batch: 24 current batch loss: 0.1649303138256073\n",
            "epoch: 6 batch: 25 current batch loss: 0.137349471449852\n",
            "epoch: 6 batch: 26 current batch loss: 0.16314345598220825\n",
            "epoch: 6 batch: 27 current batch loss: 0.1339898705482483\n",
            "epoch: 6 batch: 28 current batch loss: 0.12975965440273285\n",
            "epoch: 6 batch: 29 current batch loss: 0.10850463062524796\n",
            "epoch: 7 batch: 0 current batch loss: 0.1472436934709549\n",
            "epoch: 7 batch: 1 current batch loss: 0.12495540082454681\n",
            "epoch: 7 batch: 2 current batch loss: 0.13076777756214142\n",
            "epoch: 7 batch: 3 current batch loss: 0.14200398325920105\n",
            "epoch: 7 batch: 4 current batch loss: 0.12941114604473114\n",
            "epoch: 7 batch: 5 current batch loss: 0.14417895674705505\n",
            "epoch: 7 batch: 6 current batch loss: 0.13100744783878326\n",
            "epoch: 7 batch: 7 current batch loss: 0.12698441743850708\n",
            "epoch: 7 batch: 8 current batch loss: 0.10803902894258499\n",
            "epoch: 7 batch: 9 current batch loss: 0.10892921686172485\n",
            "epoch: 7 batch: 10 current batch loss: 0.1211337074637413\n",
            "epoch: 7 batch: 11 current batch loss: 0.14308658242225647\n",
            "epoch: 7 batch: 12 current batch loss: 0.11659284681081772\n",
            "epoch: 7 batch: 13 current batch loss: 0.11542722582817078\n",
            "epoch: 7 batch: 14 current batch loss: 0.11498314887285233\n",
            "epoch: 7 batch: 15 current batch loss: 0.13859355449676514\n",
            "epoch: 7 batch: 16 current batch loss: 0.1370030641555786\n",
            "epoch: 7 batch: 17 current batch loss: 0.11906111240386963\n",
            "epoch: 7 batch: 18 current batch loss: 0.12257979065179825\n",
            "epoch: 7 batch: 19 current batch loss: 0.12383368611335754\n",
            "epoch: 7 batch: 20 current batch loss: 0.12864091992378235\n",
            "epoch: 7 batch: 21 current batch loss: 0.1406872421503067\n",
            "epoch: 7 batch: 22 current batch loss: 0.11751586198806763\n",
            "epoch: 7 batch: 23 current batch loss: 0.13836276531219482\n",
            "epoch: 7 batch: 24 current batch loss: 0.13443565368652344\n",
            "epoch: 7 batch: 25 current batch loss: 0.12767529487609863\n",
            "epoch: 7 batch: 26 current batch loss: 0.11111253499984741\n",
            "epoch: 7 batch: 27 current batch loss: 0.14443683624267578\n",
            "epoch: 7 batch: 28 current batch loss: 0.12513913214206696\n",
            "epoch: 7 batch: 29 current batch loss: 0.10169439762830734\n"
          ]
        }
      ],
      "source": [
        "net = MLP()\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
        "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
        "                                            #and MLP doesn't apply \n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network. \n",
        "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
      "metadata": {
        "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8"
      },
      "source": [
        "\n",
        "### Your task #5\n",
        "\n",
        "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either.\n",
        "\n",
        "An epoch is a one full passage through the whole training data. Why then, on the second epoch, the losses are different than in the first epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
      "metadata": {
        "id": "9b257c80-3965-4bdf-9e47-0da1be6891de"
      },
      "source": [
        "### Answers to task #5\n",
        "\n",
        "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
        "\n",
        "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
      "metadata": {
        "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141"
      },
      "source": [
        "### Training - the second approach\n",
        "\n",
        "\n",
        "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
        "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- a definition of a scheduler to update the learning rate in an optimizer\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
      "metadata": {
        "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
        "outputId": "26e3fc2a-5010-4572-e114-b2d1ff2d2d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.350106954574585 current lr: 0.001\n",
            "epoch: 0 batch: 1 current batch loss: 2.3786466121673584 current lr: 0.001\n",
            "epoch: 0 batch: 2 current batch loss: 2.393559455871582 current lr: 0.001\n",
            "epoch: 0 batch: 3 current batch loss: 2.313286542892456 current lr: 0.001\n",
            "epoch: 0 batch: 4 current batch loss: 2.283032178878784 current lr: 0.001\n",
            "epoch: 0 batch: 5 current batch loss: 2.2645046710968018 current lr: 0.001\n",
            "epoch: 0 batch: 6 current batch loss: 2.258882999420166 current lr: 0.001\n",
            "epoch: 0 batch: 7 current batch loss: 2.2674851417541504 current lr: 0.001\n",
            "epoch: 0 batch: 8 current batch loss: 2.2132697105407715 current lr: 0.001\n",
            "epoch: 0 batch: 9 current batch loss: 2.148334264755249 current lr: 0.001\n",
            "epoch: 0 batch: 10 current batch loss: 2.086092710494995 current lr: 0.001\n",
            "epoch: 0 batch: 11 current batch loss: 2.0169105529785156 current lr: 0.001\n",
            "epoch: 0 batch: 12 current batch loss: 1.9411581754684448 current lr: 0.001\n",
            "epoch: 0 batch: 13 current batch loss: 1.8489885330200195 current lr: 0.001\n",
            "epoch: 0 batch: 14 current batch loss: 1.796928882598877 current lr: 0.001\n",
            "epoch: 0 batch: 15 current batch loss: 1.686322569847107 current lr: 0.001\n",
            "epoch: 0 batch: 16 current batch loss: 1.5774773359298706 current lr: 0.001\n",
            "epoch: 0 batch: 17 current batch loss: 1.5299713611602783 current lr: 0.001\n",
            "epoch: 0 batch: 18 current batch loss: 1.4551056623458862 current lr: 0.001\n",
            "epoch: 0 batch: 19 current batch loss: 1.3753317594528198 current lr: 0.001\n",
            "epoch: 0 batch: 20 current batch loss: 1.3003026247024536 current lr: 0.001\n",
            "epoch: 0 batch: 21 current batch loss: 1.2480632066726685 current lr: 0.001\n",
            "epoch: 0 batch: 22 current batch loss: 1.2274657487869263 current lr: 0.001\n",
            "epoch: 0 batch: 23 current batch loss: 1.209494709968567 current lr: 0.001\n",
            "epoch: 0 batch: 24 current batch loss: 1.1463563442230225 current lr: 0.001\n",
            "epoch: 0 batch: 25 current batch loss: 1.1130067110061646 current lr: 0.001\n",
            "epoch: 0 batch: 26 current batch loss: 1.0626983642578125 current lr: 0.001\n",
            "epoch: 0 batch: 27 current batch loss: 1.0192006826400757 current lr: 0.001\n",
            "epoch: 0 batch: 28 current batch loss: 1.004454255104065 current lr: 0.001\n",
            "epoch: 0 batch: 29 current batch loss: 0.961883544921875 current lr: 0.001\n",
            "epoch: 1 batch: 0 current batch loss: 0.9436493515968323 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 1 current batch loss: 0.9390160441398621 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 2 current batch loss: 0.8875851035118103 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 3 current batch loss: 0.9056934118270874 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 4 current batch loss: 0.838538408279419 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 5 current batch loss: 0.8376216888427734 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 6 current batch loss: 0.8089613318443298 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 7 current batch loss: 0.8026767373085022 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 8 current batch loss: 0.7711541652679443 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 9 current batch loss: 0.7639745473861694 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 10 current batch loss: 0.7436509132385254 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 11 current batch loss: 0.7154844999313354 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 12 current batch loss: 0.7549580931663513 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 13 current batch loss: 0.7021384239196777 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 14 current batch loss: 0.7497634291648865 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 15 current batch loss: 0.6680857539176941 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 16 current batch loss: 0.6294860243797302 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 17 current batch loss: 0.6577500700950623 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 18 current batch loss: 0.6267709136009216 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 19 current batch loss: 0.6670107841491699 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 20 current batch loss: 0.5887795090675354 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 21 current batch loss: 0.5865822434425354 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 22 current batch loss: 0.5939344167709351 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 23 current batch loss: 0.5777781009674072 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 24 current batch loss: 0.5505997538566589 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 25 current batch loss: 0.5147117376327515 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 26 current batch loss: 0.54193514585495 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 27 current batch loss: 0.5414577722549438 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 28 current batch loss: 0.5247154235839844 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 29 current batch loss: 0.4777422845363617 current lr: 0.0009000000000000001\n",
            "epoch: 2 batch: 0 current batch loss: 0.5237354040145874 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 1 current batch loss: 0.4950648248195648 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 2 current batch loss: 0.4611034095287323 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 3 current batch loss: 0.5093418955802917 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 4 current batch loss: 0.46745848655700684 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 5 current batch loss: 0.44939133524894714 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 6 current batch loss: 0.4436526596546173 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 7 current batch loss: 0.4603683352470398 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 8 current batch loss: 0.47095972299575806 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 9 current batch loss: 0.44311344623565674 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 10 current batch loss: 0.4131859838962555 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 11 current batch loss: 0.44164925813674927 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 12 current batch loss: 0.4304991364479065 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 13 current batch loss: 0.41260597109794617 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 14 current batch loss: 0.4220368266105652 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 15 current batch loss: 0.3928171694278717 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 16 current batch loss: 0.4071662724018097 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 17 current batch loss: 0.41715359687805176 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 18 current batch loss: 0.3819968104362488 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 19 current batch loss: 0.3845641613006592 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 20 current batch loss: 0.38582950830459595 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 21 current batch loss: 0.39120203256607056 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 22 current batch loss: 0.418571412563324 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 23 current batch loss: 0.36940696835517883 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 24 current batch loss: 0.35674598813056946 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 25 current batch loss: 0.3681788742542267 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 26 current batch loss: 0.3641771376132965 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 27 current batch loss: 0.3611737787723541 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 28 current batch loss: 0.4013102054595947 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 29 current batch loss: 0.4115962088108063 current lr: 0.0008100000000000001\n",
            "epoch: 3 batch: 0 current batch loss: 0.3680362105369568 current lr: 0.000729\n",
            "epoch: 3 batch: 1 current batch loss: 0.33500203490257263 current lr: 0.000729\n",
            "epoch: 3 batch: 2 current batch loss: 0.34021660685539246 current lr: 0.000729\n",
            "epoch: 3 batch: 3 current batch loss: 0.3541800379753113 current lr: 0.000729\n",
            "epoch: 3 batch: 4 current batch loss: 0.3506304919719696 current lr: 0.000729\n",
            "epoch: 3 batch: 5 current batch loss: 0.33997246623039246 current lr: 0.000729\n",
            "epoch: 3 batch: 6 current batch loss: 0.3498789966106415 current lr: 0.000729\n",
            "epoch: 3 batch: 7 current batch loss: 0.349669873714447 current lr: 0.000729\n",
            "epoch: 3 batch: 8 current batch loss: 0.3447903096675873 current lr: 0.000729\n",
            "epoch: 3 batch: 9 current batch loss: 0.31688985228538513 current lr: 0.000729\n",
            "epoch: 3 batch: 10 current batch loss: 0.30502092838287354 current lr: 0.000729\n",
            "epoch: 3 batch: 11 current batch loss: 0.30249038338661194 current lr: 0.000729\n",
            "epoch: 3 batch: 12 current batch loss: 0.300286203622818 current lr: 0.000729\n",
            "epoch: 3 batch: 13 current batch loss: 0.3044825494289398 current lr: 0.000729\n",
            "epoch: 3 batch: 14 current batch loss: 0.3131309449672699 current lr: 0.000729\n",
            "epoch: 3 batch: 15 current batch loss: 0.3087453842163086 current lr: 0.000729\n",
            "epoch: 3 batch: 16 current batch loss: 0.3172065019607544 current lr: 0.000729\n",
            "epoch: 3 batch: 17 current batch loss: 0.3115677833557129 current lr: 0.000729\n",
            "epoch: 3 batch: 18 current batch loss: 0.30543336272239685 current lr: 0.000729\n",
            "epoch: 3 batch: 19 current batch loss: 0.32815098762512207 current lr: 0.000729\n",
            "epoch: 3 batch: 20 current batch loss: 0.2844214141368866 current lr: 0.000729\n",
            "epoch: 3 batch: 21 current batch loss: 0.2867017984390259 current lr: 0.000729\n",
            "epoch: 3 batch: 22 current batch loss: 0.2788955271244049 current lr: 0.000729\n",
            "epoch: 3 batch: 23 current batch loss: 0.2781358063220978 current lr: 0.000729\n",
            "epoch: 3 batch: 24 current batch loss: 0.3066459596157074 current lr: 0.000729\n",
            "epoch: 3 batch: 25 current batch loss: 0.31967392563819885 current lr: 0.000729\n",
            "epoch: 3 batch: 26 current batch loss: 0.30941668152809143 current lr: 0.000729\n",
            "epoch: 3 batch: 27 current batch loss: 0.2892853319644928 current lr: 0.000729\n",
            "epoch: 3 batch: 28 current batch loss: 0.2861660420894623 current lr: 0.000729\n",
            "epoch: 3 batch: 29 current batch loss: 0.25969198346138 current lr: 0.000729\n",
            "epoch: 4 batch: 0 current batch loss: 0.2859303951263428 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 1 current batch loss: 0.2815287113189697 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 2 current batch loss: 0.2712443172931671 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 3 current batch loss: 0.2949609160423279 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 4 current batch loss: 0.27014249563217163 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 5 current batch loss: 0.2522957921028137 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 6 current batch loss: 0.28451889753341675 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 7 current batch loss: 0.26760008931159973 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 8 current batch loss: 0.2535306215286255 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 9 current batch loss: 0.237657368183136 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 10 current batch loss: 0.2740336060523987 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 11 current batch loss: 0.25455448031425476 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 12 current batch loss: 0.2911996841430664 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 13 current batch loss: 0.2494032233953476 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 14 current batch loss: 0.24205389618873596 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 15 current batch loss: 0.257032573223114 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 16 current batch loss: 0.2431311309337616 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 17 current batch loss: 0.2671036422252655 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 18 current batch loss: 0.24589748680591583 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 19 current batch loss: 0.24414700269699097 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 20 current batch loss: 0.21944601833820343 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 21 current batch loss: 0.23555105924606323 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 22 current batch loss: 0.2541956305503845 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 23 current batch loss: 0.24122141301631927 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 24 current batch loss: 0.26885128021240234 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 25 current batch loss: 0.23436479270458221 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 26 current batch loss: 0.23231230676174164 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 27 current batch loss: 0.2534392178058624 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 28 current batch loss: 0.2349892556667328 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 29 current batch loss: 0.24014341831207275 current lr: 0.0006561000000000001\n",
            "epoch: 5 batch: 0 current batch loss: 0.22251906991004944 current lr: 0.00059049\n",
            "epoch: 5 batch: 1 current batch loss: 0.22938892245292664 current lr: 0.00059049\n",
            "epoch: 5 batch: 2 current batch loss: 0.2417028546333313 current lr: 0.00059049\n",
            "epoch: 5 batch: 3 current batch loss: 0.21361704170703888 current lr: 0.00059049\n",
            "epoch: 5 batch: 4 current batch loss: 0.2419685572385788 current lr: 0.00059049\n",
            "epoch: 5 batch: 5 current batch loss: 0.22161486744880676 current lr: 0.00059049\n",
            "epoch: 5 batch: 6 current batch loss: 0.21846897900104523 current lr: 0.00059049\n",
            "epoch: 5 batch: 7 current batch loss: 0.22902356088161469 current lr: 0.00059049\n",
            "epoch: 5 batch: 8 current batch loss: 0.232306107878685 current lr: 0.00059049\n",
            "epoch: 5 batch: 9 current batch loss: 0.21139779686927795 current lr: 0.00059049\n",
            "epoch: 5 batch: 10 current batch loss: 0.22319245338439941 current lr: 0.00059049\n",
            "epoch: 5 batch: 11 current batch loss: 0.2227875292301178 current lr: 0.00059049\n",
            "epoch: 5 batch: 12 current batch loss: 0.23270021378993988 current lr: 0.00059049\n",
            "epoch: 5 batch: 13 current batch loss: 0.21604107320308685 current lr: 0.00059049\n",
            "epoch: 5 batch: 14 current batch loss: 0.21763038635253906 current lr: 0.00059049\n",
            "epoch: 5 batch: 15 current batch loss: 0.235642209649086 current lr: 0.00059049\n",
            "epoch: 5 batch: 16 current batch loss: 0.22764834761619568 current lr: 0.00059049\n",
            "epoch: 5 batch: 17 current batch loss: 0.21308332681655884 current lr: 0.00059049\n",
            "epoch: 5 batch: 18 current batch loss: 0.23286038637161255 current lr: 0.00059049\n",
            "epoch: 5 batch: 19 current batch loss: 0.19572918117046356 current lr: 0.00059049\n",
            "epoch: 5 batch: 20 current batch loss: 0.21706756949424744 current lr: 0.00059049\n",
            "epoch: 5 batch: 21 current batch loss: 0.21509020030498505 current lr: 0.00059049\n",
            "epoch: 5 batch: 22 current batch loss: 0.21471908688545227 current lr: 0.00059049\n",
            "epoch: 5 batch: 23 current batch loss: 0.1995292603969574 current lr: 0.00059049\n",
            "epoch: 5 batch: 24 current batch loss: 0.2069597989320755 current lr: 0.00059049\n",
            "epoch: 5 batch: 25 current batch loss: 0.21639472246170044 current lr: 0.00059049\n",
            "epoch: 5 batch: 26 current batch loss: 0.22007207572460175 current lr: 0.00059049\n",
            "epoch: 5 batch: 27 current batch loss: 0.17871926724910736 current lr: 0.00059049\n",
            "epoch: 5 batch: 28 current batch loss: 0.1989726573228836 current lr: 0.00059049\n",
            "epoch: 5 batch: 29 current batch loss: 0.25351110100746155 current lr: 0.00059049\n",
            "epoch: 6 batch: 0 current batch loss: 0.2251064032316208 current lr: 0.000531441\n",
            "epoch: 6 batch: 1 current batch loss: 0.22020472586154938 current lr: 0.000531441\n",
            "epoch: 6 batch: 2 current batch loss: 0.18689489364624023 current lr: 0.000531441\n",
            "epoch: 6 batch: 3 current batch loss: 0.17099720239639282 current lr: 0.000531441\n",
            "epoch: 6 batch: 4 current batch loss: 0.21462857723236084 current lr: 0.000531441\n",
            "epoch: 6 batch: 5 current batch loss: 0.194280743598938 current lr: 0.000531441\n",
            "epoch: 6 batch: 6 current batch loss: 0.19158193469047546 current lr: 0.000531441\n",
            "epoch: 6 batch: 7 current batch loss: 0.1930573433637619 current lr: 0.000531441\n",
            "epoch: 6 batch: 8 current batch loss: 0.19098666310310364 current lr: 0.000531441\n",
            "epoch: 6 batch: 9 current batch loss: 0.17673222720623016 current lr: 0.000531441\n",
            "epoch: 6 batch: 10 current batch loss: 0.2056674212217331 current lr: 0.000531441\n",
            "epoch: 6 batch: 11 current batch loss: 0.18154776096343994 current lr: 0.000531441\n",
            "epoch: 6 batch: 12 current batch loss: 0.1909148097038269 current lr: 0.000531441\n",
            "epoch: 6 batch: 13 current batch loss: 0.19876210391521454 current lr: 0.000531441\n",
            "epoch: 6 batch: 14 current batch loss: 0.20971901714801788 current lr: 0.000531441\n",
            "epoch: 6 batch: 15 current batch loss: 0.21276241540908813 current lr: 0.000531441\n",
            "epoch: 6 batch: 16 current batch loss: 0.1890196055173874 current lr: 0.000531441\n",
            "epoch: 6 batch: 17 current batch loss: 0.2094542384147644 current lr: 0.000531441\n",
            "epoch: 6 batch: 18 current batch loss: 0.18612700700759888 current lr: 0.000531441\n",
            "epoch: 6 batch: 19 current batch loss: 0.21188104152679443 current lr: 0.000531441\n",
            "epoch: 6 batch: 20 current batch loss: 0.17013342678546906 current lr: 0.000531441\n",
            "epoch: 6 batch: 21 current batch loss: 0.17780841886997223 current lr: 0.000531441\n",
            "epoch: 6 batch: 22 current batch loss: 0.18664318323135376 current lr: 0.000531441\n",
            "epoch: 6 batch: 23 current batch loss: 0.1862894594669342 current lr: 0.000531441\n",
            "epoch: 6 batch: 24 current batch loss: 0.1815580129623413 current lr: 0.000531441\n",
            "epoch: 6 batch: 25 current batch loss: 0.19257129728794098 current lr: 0.000531441\n",
            "epoch: 6 batch: 26 current batch loss: 0.2001318335533142 current lr: 0.000531441\n",
            "epoch: 6 batch: 27 current batch loss: 0.17900770902633667 current lr: 0.000531441\n",
            "epoch: 6 batch: 28 current batch loss: 0.1805136501789093 current lr: 0.000531441\n",
            "epoch: 6 batch: 29 current batch loss: 0.15367119014263153 current lr: 0.000531441\n",
            "epoch: 7 batch: 0 current batch loss: 0.17466403543949127 current lr: 0.0004782969\n",
            "epoch: 7 batch: 1 current batch loss: 0.17665114998817444 current lr: 0.0004782969\n",
            "epoch: 7 batch: 2 current batch loss: 0.1559390276670456 current lr: 0.0004782969\n",
            "epoch: 7 batch: 3 current batch loss: 0.17027021944522858 current lr: 0.0004782969\n",
            "epoch: 7 batch: 4 current batch loss: 0.16149024665355682 current lr: 0.0004782969\n",
            "epoch: 7 batch: 5 current batch loss: 0.18264281749725342 current lr: 0.0004782969\n",
            "epoch: 7 batch: 6 current batch loss: 0.18563205003738403 current lr: 0.0004782969\n",
            "epoch: 7 batch: 7 current batch loss: 0.17150230705738068 current lr: 0.0004782969\n",
            "epoch: 7 batch: 8 current batch loss: 0.17797650396823883 current lr: 0.0004782969\n",
            "epoch: 7 batch: 9 current batch loss: 0.17435987293720245 current lr: 0.0004782969\n",
            "epoch: 7 batch: 10 current batch loss: 0.1803808957338333 current lr: 0.0004782969\n",
            "epoch: 7 batch: 11 current batch loss: 0.16366799175739288 current lr: 0.0004782969\n",
            "epoch: 7 batch: 12 current batch loss: 0.15724900364875793 current lr: 0.0004782969\n",
            "epoch: 7 batch: 13 current batch loss: 0.1968148648738861 current lr: 0.0004782969\n",
            "epoch: 7 batch: 14 current batch loss: 0.1674962192773819 current lr: 0.0004782969\n",
            "epoch: 7 batch: 15 current batch loss: 0.18498200178146362 current lr: 0.0004782969\n",
            "epoch: 7 batch: 16 current batch loss: 0.17348246276378632 current lr: 0.0004782969\n",
            "epoch: 7 batch: 17 current batch loss: 0.16995716094970703 current lr: 0.0004782969\n",
            "epoch: 7 batch: 18 current batch loss: 0.18226276338100433 current lr: 0.0004782969\n",
            "epoch: 7 batch: 19 current batch loss: 0.179763525724411 current lr: 0.0004782969\n",
            "epoch: 7 batch: 20 current batch loss: 0.15187180042266846 current lr: 0.0004782969\n",
            "epoch: 7 batch: 21 current batch loss: 0.19272637367248535 current lr: 0.0004782969\n",
            "epoch: 7 batch: 22 current batch loss: 0.1445329636335373 current lr: 0.0004782969\n",
            "epoch: 7 batch: 23 current batch loss: 0.1754966378211975 current lr: 0.0004782969\n",
            "epoch: 7 batch: 24 current batch loss: 0.16919663548469543 current lr: 0.0004782969\n",
            "epoch: 7 batch: 25 current batch loss: 0.14179477095603943 current lr: 0.0004782969\n",
            "epoch: 7 batch: 26 current batch loss: 0.1666555255651474 current lr: 0.0004782969\n",
            "epoch: 7 batch: 27 current batch loss: 0.17106105387210846 current lr: 0.0004782969\n",
            "epoch: 7 batch: 28 current batch loss: 0.1721709668636322 current lr: 0.0004782969\n",
            "epoch: 7 batch: 29 current batch loss: 0.16372041404247284 current lr: 0.0004782969\n"
          ]
        }
      ],
      "source": [
        "net_with_scheduler = MLP()\n",
        "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
        "\n",
        "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
        "                                            #and MLP doesn't apply \n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        \n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network. \n",
        "                                \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
      "metadata": {
        "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592"
      },
      "source": [
        " \n",
        "### Your task #6\n",
        "\n",
        "Well, it seems that we were able to get the learning to levels of 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. There are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
        "\n",
        "Some other schedulers you might want to experiment with: \n",
        "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
        "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
        "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
        "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
        "\n",
        "### Your task #7\n",
        "\n",
        "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
      "metadata": {
        "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Now we will test those two nets - the one without and the one with the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
      "metadata": {
        "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
        "outputId": "8b336025-0f2b-428c-8560-4d7229b41fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6bbf3475b7a6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdatapoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m#prediction has values representing the \"prevalence\" of the corresponding class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#the class is the index of maximal \"prevalence\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-451c4d90c5a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
      "metadata": {
        "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
        "outputId": "3133cb84-b13d-424e-84db-b7043aecb40f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.961\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
      "metadata": {
        "id": "e4831d2c-ee3f-42be-969b-627d888692c8"
      },
      "source": [
        "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler.\n",
        "\n",
        "# Phase 2 Hands on\n",
        "\n",
        "## Moving computations to GPU\n",
        "\n",
        "In the code above we didn't move the computations to GPU. The first task is moving the computations to GPU. How do you do that?\n",
        "\n",
        "Ability to compute on GPU is called CUDA:  Compute Unified Device Architecture.\n",
        "\n",
        "First, we check if CUDA is available:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DakCowPLQk9S",
        "outputId": "0fbf8598-d4fa-425c-dba5-76067df6dce7"
      },
      "id": "DakCowPLQk9S",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to make CUDA available if it is not? \n",
        "\n",
        "* On a local machine you must connect and properly configure a GPU card, it is out of scope of this workshop.\n",
        "\n",
        "* In Google Colab online, you click `Runtime` > `Change runtime` and change `Hardware acceleration` setting to `GPU`.\n",
        "\n",
        "* In Google Colab **Polish version**, you click `Środowisko wykonawcze` > `Zmień typ środowiska wykonawczego` and change `Akcelerator sprzętowy` setting to `GPU`.\n",
        "\n",
        "Once we have CUDA (or not: this case should be handled in the code, too), we are ready to determine the calculation device. Note that it is just a character string:\n"
      ],
      "metadata": {
        "id": "052Wt1tVRYYG"
      },
      "id": "052Wt1tVRYYG"
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  computational_device = 'cuda'\n",
        "else:\n",
        "  computational_device = 'cpu'\n",
        "\n",
        "print(computational_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcAjuc3iTBDe",
        "outputId": "138ceac3-c97f-4f4b-c598-2292395032c5"
      },
      "id": "pcAjuc3iTBDe",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we move our neural network model and each tensor with batch data to GPU. We can do it by passing the `device = computational_device` argument during the model or tensor construction, or by calling a `to(computational_device)` method on a model or a tensor:"
      ],
      "metadata": {
        "id": "v5KnjANmTKJH"
      },
      "id": "v5KnjANmTKJH"
    },
    {
      "cell_type": "code",
      "source": [
        "layer = torch.nn.Linear(2048, 256, device = computational_device),\n",
        "layer = torch.nn.Linear(2048, 256).to(computational_device)"
      ],
      "metadata": {
        "id": "DXq45N-kTJYP"
      },
      "id": "DXq45N-kTJYP",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "\n",
        "Enable CUDA compuations in all the code above:\n",
        "\n",
        "1. Check if CUDA is available before the MLP definition\n",
        "2. Add the device field to the constructor and pass it to all constructed layers\n",
        "3. Add the device argument when constructing MLP\n",
        "4. Add the device arguments to all tensors that get passed to MLP"
      ],
      "metadata": {
        "id": "qL9aZbEYY2F3"
      },
      "id": "qL9aZbEYY2F3"
    },
    {
      "cell_type": "code",
      "source": [
        "########################################### check  if CUDA available #################################################\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "########################################### MLP definition on GPU #################################################\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   \n",
        "                                          \n",
        "            torch.nn.Flatten(),   \n",
        "            torch.nn.Linear(1*28*28, 1024, device = device),  \n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(1024, 2048, device = device),   \n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(2048, 256, device = device),\n",
        "            torch.nn.Sigmoid(),            \n",
        "            torch.nn.Linear(256, 10, device = device))    # device gets passed to the Sequential\n",
        " \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "########################################### TRAINING ON GPU #################################################  \n",
        "net = MLP(device = device)       #we build MLP on a computational_device\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   \n",
        "\n",
        "net.train()    \n",
        "for epoch in range(8):  \n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "               \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs.to(device))  # we move batch data to computational_device\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels.to(device), reduction = \"mean\") # we move batch labels to computational_device\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
        "        loss.backward()       \n",
        "        optimizer.step()     \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kBZ_fJ5Y0dQ",
        "outputId": "b86d77f4-3a43-4b9f-f2fa-87793ad210e2"
      },
      "id": "-kBZ_fJ5Y0dQ",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.3327698707580566\n",
            "epoch: 0 batch: 1 current batch loss: 2.395174980163574\n",
            "epoch: 0 batch: 2 current batch loss: 2.3359947204589844\n",
            "epoch: 0 batch: 3 current batch loss: 2.3047409057617188\n",
            "epoch: 0 batch: 4 current batch loss: 2.279634714126587\n",
            "epoch: 0 batch: 5 current batch loss: 2.2733078002929688\n",
            "epoch: 0 batch: 6 current batch loss: 2.2835099697113037\n",
            "epoch: 0 batch: 7 current batch loss: 2.2357735633850098\n",
            "epoch: 0 batch: 8 current batch loss: 2.200507640838623\n",
            "epoch: 0 batch: 9 current batch loss: 2.145057439804077\n",
            "epoch: 0 batch: 10 current batch loss: 2.089484214782715\n",
            "epoch: 0 batch: 11 current batch loss: 2.0303995609283447\n",
            "epoch: 0 batch: 12 current batch loss: 1.9527556896209717\n",
            "epoch: 0 batch: 13 current batch loss: 1.871125340461731\n",
            "epoch: 0 batch: 14 current batch loss: 1.783023476600647\n",
            "epoch: 0 batch: 15 current batch loss: 1.7121559381484985\n",
            "epoch: 0 batch: 16 current batch loss: 1.6140446662902832\n",
            "epoch: 0 batch: 17 current batch loss: 1.5367969274520874\n",
            "epoch: 0 batch: 18 current batch loss: 1.4473915100097656\n",
            "epoch: 0 batch: 19 current batch loss: 1.3982104063034058\n",
            "epoch: 0 batch: 20 current batch loss: 1.336653232574463\n",
            "epoch: 0 batch: 21 current batch loss: 1.2902215719223022\n",
            "epoch: 0 batch: 22 current batch loss: 1.2531800270080566\n",
            "epoch: 0 batch: 23 current batch loss: 1.176076889038086\n",
            "epoch: 0 batch: 24 current batch loss: 1.1073402166366577\n",
            "epoch: 0 batch: 25 current batch loss: 1.0577671527862549\n",
            "epoch: 0 batch: 26 current batch loss: 1.0330580472946167\n",
            "epoch: 0 batch: 27 current batch loss: 0.9709715843200684\n",
            "epoch: 0 batch: 28 current batch loss: 0.9077088236808777\n",
            "epoch: 0 batch: 29 current batch loss: 0.9020023345947266\n",
            "epoch: 1 batch: 0 current batch loss: 0.8506643772125244\n",
            "epoch: 1 batch: 1 current batch loss: 0.8323312997817993\n",
            "epoch: 1 batch: 2 current batch loss: 0.7693243622779846\n",
            "epoch: 1 batch: 3 current batch loss: 0.7569647431373596\n",
            "epoch: 1 batch: 4 current batch loss: 0.7628378868103027\n",
            "epoch: 1 batch: 5 current batch loss: 0.7295712828636169\n",
            "epoch: 1 batch: 6 current batch loss: 0.6653357744216919\n",
            "epoch: 1 batch: 7 current batch loss: 0.6573927402496338\n",
            "epoch: 1 batch: 8 current batch loss: 0.6375646591186523\n",
            "epoch: 1 batch: 9 current batch loss: 0.6239780783653259\n",
            "epoch: 1 batch: 10 current batch loss: 0.5863228440284729\n",
            "epoch: 1 batch: 11 current batch loss: 0.57966148853302\n",
            "epoch: 1 batch: 12 current batch loss: 0.5456210374832153\n",
            "epoch: 1 batch: 13 current batch loss: 0.5338807702064514\n",
            "epoch: 1 batch: 14 current batch loss: 0.5316056609153748\n",
            "epoch: 1 batch: 15 current batch loss: 0.4970589876174927\n",
            "epoch: 1 batch: 16 current batch loss: 0.5098038911819458\n",
            "epoch: 1 batch: 17 current batch loss: 0.4891400635242462\n",
            "epoch: 1 batch: 18 current batch loss: 0.47940874099731445\n",
            "epoch: 1 batch: 19 current batch loss: 0.46612370014190674\n",
            "epoch: 1 batch: 20 current batch loss: 0.4518476128578186\n",
            "epoch: 1 batch: 21 current batch loss: 0.4401703178882599\n",
            "epoch: 1 batch: 22 current batch loss: 0.4435717463493347\n",
            "epoch: 1 batch: 23 current batch loss: 0.4429624676704407\n",
            "epoch: 1 batch: 24 current batch loss: 0.40495434403419495\n",
            "epoch: 1 batch: 25 current batch loss: 0.3876514732837677\n",
            "epoch: 1 batch: 26 current batch loss: 0.4078938663005829\n",
            "epoch: 1 batch: 27 current batch loss: 0.4205041825771332\n",
            "epoch: 1 batch: 28 current batch loss: 0.39135369658470154\n",
            "epoch: 1 batch: 29 current batch loss: 0.35306286811828613\n",
            "epoch: 2 batch: 0 current batch loss: 0.3566224277019501\n",
            "epoch: 2 batch: 1 current batch loss: 0.4041503667831421\n",
            "epoch: 2 batch: 2 current batch loss: 0.3597829341888428\n",
            "epoch: 2 batch: 3 current batch loss: 0.358110249042511\n",
            "epoch: 2 batch: 4 current batch loss: 0.35376209020614624\n",
            "epoch: 2 batch: 5 current batch loss: 0.35345038771629333\n",
            "epoch: 2 batch: 6 current batch loss: 0.31614625453948975\n",
            "epoch: 2 batch: 7 current batch loss: 0.36602842807769775\n",
            "epoch: 2 batch: 8 current batch loss: 0.3049211800098419\n",
            "epoch: 2 batch: 9 current batch loss: 0.30210617184638977\n",
            "epoch: 2 batch: 10 current batch loss: 0.30594873428344727\n",
            "epoch: 2 batch: 11 current batch loss: 0.316159188747406\n",
            "epoch: 2 batch: 12 current batch loss: 0.31964603066444397\n",
            "epoch: 2 batch: 13 current batch loss: 0.28635120391845703\n",
            "epoch: 2 batch: 14 current batch loss: 0.3115483224391937\n",
            "epoch: 2 batch: 15 current batch loss: 0.2945650517940521\n",
            "epoch: 2 batch: 16 current batch loss: 0.3228747248649597\n",
            "epoch: 2 batch: 17 current batch loss: 0.2820327579975128\n",
            "epoch: 2 batch: 18 current batch loss: 0.25685855746269226\n",
            "epoch: 2 batch: 19 current batch loss: 0.2969604432582855\n",
            "epoch: 2 batch: 20 current batch loss: 0.2853085398674011\n",
            "epoch: 2 batch: 21 current batch loss: 0.27907341718673706\n",
            "epoch: 2 batch: 22 current batch loss: 0.2904866635799408\n",
            "epoch: 2 batch: 23 current batch loss: 0.25680360198020935\n",
            "epoch: 2 batch: 24 current batch loss: 0.26155126094818115\n",
            "epoch: 2 batch: 25 current batch loss: 0.28330788016319275\n",
            "epoch: 2 batch: 26 current batch loss: 0.27419450879096985\n",
            "epoch: 2 batch: 27 current batch loss: 0.28471773862838745\n",
            "epoch: 2 batch: 28 current batch loss: 0.22665022313594818\n",
            "epoch: 2 batch: 29 current batch loss: 0.2649464011192322\n",
            "epoch: 3 batch: 0 current batch loss: 0.24414697289466858\n",
            "epoch: 3 batch: 1 current batch loss: 0.25597530603408813\n",
            "epoch: 3 batch: 2 current batch loss: 0.25537723302841187\n",
            "epoch: 3 batch: 3 current batch loss: 0.23513659834861755\n",
            "epoch: 3 batch: 4 current batch loss: 0.24623717367649078\n",
            "epoch: 3 batch: 5 current batch loss: 0.2505921423435211\n",
            "epoch: 3 batch: 6 current batch loss: 0.23470105230808258\n",
            "epoch: 3 batch: 7 current batch loss: 0.23568780720233917\n",
            "epoch: 3 batch: 8 current batch loss: 0.23687545955181122\n",
            "epoch: 3 batch: 9 current batch loss: 0.24172750115394592\n",
            "epoch: 3 batch: 10 current batch loss: 0.22648555040359497\n",
            "epoch: 3 batch: 11 current batch loss: 0.21647228300571442\n",
            "epoch: 3 batch: 12 current batch loss: 0.24022473394870758\n",
            "epoch: 3 batch: 13 current batch loss: 0.2322932481765747\n",
            "epoch: 3 batch: 14 current batch loss: 0.23517227172851562\n",
            "epoch: 3 batch: 15 current batch loss: 0.19395801424980164\n",
            "epoch: 3 batch: 16 current batch loss: 0.2290339171886444\n",
            "epoch: 3 batch: 17 current batch loss: 0.19044719636440277\n",
            "epoch: 3 batch: 18 current batch loss: 0.20134899020195007\n",
            "epoch: 3 batch: 19 current batch loss: 0.22289173305034637\n",
            "epoch: 3 batch: 20 current batch loss: 0.20352032780647278\n",
            "epoch: 3 batch: 21 current batch loss: 0.21053363382816315\n",
            "epoch: 3 batch: 22 current batch loss: 0.2524023950099945\n",
            "epoch: 3 batch: 23 current batch loss: 0.2274317741394043\n",
            "epoch: 3 batch: 24 current batch loss: 0.19376826286315918\n",
            "epoch: 3 batch: 25 current batch loss: 0.1977103352546692\n",
            "epoch: 3 batch: 26 current batch loss: 0.19855304062366486\n",
            "epoch: 3 batch: 27 current batch loss: 0.22941836714744568\n",
            "epoch: 3 batch: 28 current batch loss: 0.1709575057029724\n",
            "epoch: 3 batch: 29 current batch loss: 0.20717839896678925\n",
            "epoch: 4 batch: 0 current batch loss: 0.18306508660316467\n",
            "epoch: 4 batch: 1 current batch loss: 0.17036888003349304\n",
            "epoch: 4 batch: 2 current batch loss: 0.19910407066345215\n",
            "epoch: 4 batch: 3 current batch loss: 0.1745251715183258\n",
            "epoch: 4 batch: 4 current batch loss: 0.158245250582695\n",
            "epoch: 4 batch: 5 current batch loss: 0.19371944665908813\n",
            "epoch: 4 batch: 6 current batch loss: 0.17492254078388214\n",
            "epoch: 4 batch: 7 current batch loss: 0.16504693031311035\n",
            "epoch: 4 batch: 8 current batch loss: 0.15761922299861908\n",
            "epoch: 4 batch: 9 current batch loss: 0.17384639382362366\n",
            "epoch: 4 batch: 10 current batch loss: 0.19174565374851227\n",
            "epoch: 4 batch: 11 current batch loss: 0.15842433273792267\n",
            "epoch: 4 batch: 12 current batch loss: 0.1677316427230835\n",
            "epoch: 4 batch: 13 current batch loss: 0.15073873102664948\n",
            "epoch: 4 batch: 14 current batch loss: 0.20222988724708557\n",
            "epoch: 4 batch: 15 current batch loss: 0.1808963119983673\n",
            "epoch: 4 batch: 16 current batch loss: 0.16614405810832977\n",
            "epoch: 4 batch: 17 current batch loss: 0.2077876478433609\n",
            "epoch: 4 batch: 18 current batch loss: 0.17704418301582336\n",
            "epoch: 4 batch: 19 current batch loss: 0.17256946861743927\n",
            "epoch: 4 batch: 20 current batch loss: 0.16167576611042023\n",
            "epoch: 4 batch: 21 current batch loss: 0.16992169618606567\n",
            "epoch: 4 batch: 22 current batch loss: 0.17836497724056244\n",
            "epoch: 4 batch: 23 current batch loss: 0.17248766124248505\n",
            "epoch: 4 batch: 24 current batch loss: 0.17164170742034912\n",
            "epoch: 4 batch: 25 current batch loss: 0.1588447242975235\n",
            "epoch: 4 batch: 26 current batch loss: 0.16938887536525726\n",
            "epoch: 4 batch: 27 current batch loss: 0.13408200442790985\n",
            "epoch: 4 batch: 28 current batch loss: 0.1889551877975464\n",
            "epoch: 4 batch: 29 current batch loss: 0.1388009786605835\n",
            "epoch: 5 batch: 0 current batch loss: 0.14036594331264496\n",
            "epoch: 5 batch: 1 current batch loss: 0.1350538581609726\n",
            "epoch: 5 batch: 2 current batch loss: 0.13360339403152466\n",
            "epoch: 5 batch: 3 current batch loss: 0.13454186916351318\n",
            "epoch: 5 batch: 4 current batch loss: 0.1389583945274353\n",
            "epoch: 5 batch: 5 current batch loss: 0.13640069961547852\n",
            "epoch: 5 batch: 6 current batch loss: 0.12711329758167267\n",
            "epoch: 5 batch: 7 current batch loss: 0.13277627527713776\n",
            "epoch: 5 batch: 8 current batch loss: 0.13892017304897308\n",
            "epoch: 5 batch: 9 current batch loss: 0.1338990479707718\n",
            "epoch: 5 batch: 10 current batch loss: 0.14569351077079773\n",
            "epoch: 5 batch: 11 current batch loss: 0.14649470150470734\n",
            "epoch: 5 batch: 12 current batch loss: 0.13260506093502045\n",
            "epoch: 5 batch: 13 current batch loss: 0.13483795523643494\n",
            "epoch: 5 batch: 14 current batch loss: 0.12917421758174896\n",
            "epoch: 5 batch: 15 current batch loss: 0.14196322858333588\n",
            "epoch: 5 batch: 16 current batch loss: 0.13983453810214996\n",
            "epoch: 5 batch: 17 current batch loss: 0.1219988763332367\n",
            "epoch: 5 batch: 18 current batch loss: 0.13118554651737213\n",
            "epoch: 5 batch: 19 current batch loss: 0.1284784972667694\n",
            "epoch: 5 batch: 20 current batch loss: 0.11711354553699493\n",
            "epoch: 5 batch: 21 current batch loss: 0.13259638845920563\n",
            "epoch: 5 batch: 22 current batch loss: 0.1400262862443924\n",
            "epoch: 5 batch: 23 current batch loss: 0.13360576331615448\n",
            "epoch: 5 batch: 24 current batch loss: 0.14566804468631744\n",
            "epoch: 5 batch: 25 current batch loss: 0.1510203629732132\n",
            "epoch: 5 batch: 26 current batch loss: 0.11606127768754959\n",
            "epoch: 5 batch: 27 current batch loss: 0.13053876161575317\n",
            "epoch: 5 batch: 28 current batch loss: 0.13151533901691437\n",
            "epoch: 5 batch: 29 current batch loss: 0.18305152654647827\n",
            "epoch: 6 batch: 0 current batch loss: 0.0967506393790245\n",
            "epoch: 6 batch: 1 current batch loss: 0.10793642699718475\n",
            "epoch: 6 batch: 2 current batch loss: 0.11808940768241882\n",
            "epoch: 6 batch: 3 current batch loss: 0.12377072870731354\n",
            "epoch: 6 batch: 4 current batch loss: 0.11837083846330643\n",
            "epoch: 6 batch: 5 current batch loss: 0.11266802996397018\n",
            "epoch: 6 batch: 6 current batch loss: 0.11191544681787491\n",
            "epoch: 6 batch: 7 current batch loss: 0.11654964834451675\n",
            "epoch: 6 batch: 8 current batch loss: 0.11614645272493362\n",
            "epoch: 6 batch: 9 current batch loss: 0.13235434889793396\n",
            "epoch: 6 batch: 10 current batch loss: 0.11622918397188187\n",
            "epoch: 6 batch: 11 current batch loss: 0.13710781931877136\n",
            "epoch: 6 batch: 12 current batch loss: 0.10714206844568253\n",
            "epoch: 6 batch: 13 current batch loss: 0.10966350138187408\n",
            "epoch: 6 batch: 14 current batch loss: 0.1073925569653511\n",
            "epoch: 6 batch: 15 current batch loss: 0.09268947690725327\n",
            "epoch: 6 batch: 16 current batch loss: 0.10833682864904404\n",
            "epoch: 6 batch: 17 current batch loss: 0.09734043478965759\n",
            "epoch: 6 batch: 18 current batch loss: 0.10169985890388489\n",
            "epoch: 6 batch: 19 current batch loss: 0.13023924827575684\n",
            "epoch: 6 batch: 20 current batch loss: 0.10814650356769562\n",
            "epoch: 6 batch: 21 current batch loss: 0.10705439746379852\n",
            "epoch: 6 batch: 22 current batch loss: 0.09743744879961014\n",
            "epoch: 6 batch: 23 current batch loss: 0.08723236620426178\n",
            "epoch: 6 batch: 24 current batch loss: 0.1161414086818695\n",
            "epoch: 6 batch: 25 current batch loss: 0.10646172612905502\n",
            "epoch: 6 batch: 26 current batch loss: 0.1002962738275528\n",
            "epoch: 6 batch: 27 current batch loss: 0.09977852553129196\n",
            "epoch: 6 batch: 28 current batch loss: 0.10020048916339874\n",
            "epoch: 6 batch: 29 current batch loss: 0.09595604985952377\n",
            "epoch: 7 batch: 0 current batch loss: 0.09472231566905975\n",
            "epoch: 7 batch: 1 current batch loss: 0.1031574159860611\n",
            "epoch: 7 batch: 2 current batch loss: 0.0975613072514534\n",
            "epoch: 7 batch: 3 current batch loss: 0.09434190392494202\n",
            "epoch: 7 batch: 4 current batch loss: 0.0973275899887085\n",
            "epoch: 7 batch: 5 current batch loss: 0.08919256180524826\n",
            "epoch: 7 batch: 6 current batch loss: 0.08377055823802948\n",
            "epoch: 7 batch: 7 current batch loss: 0.088844433426857\n",
            "epoch: 7 batch: 8 current batch loss: 0.12289103120565414\n",
            "epoch: 7 batch: 9 current batch loss: 0.09413802623748779\n",
            "epoch: 7 batch: 10 current batch loss: 0.10229343920946121\n",
            "epoch: 7 batch: 11 current batch loss: 0.09411704540252686\n",
            "epoch: 7 batch: 12 current batch loss: 0.08493863791227341\n",
            "epoch: 7 batch: 13 current batch loss: 0.08698128908872604\n",
            "epoch: 7 batch: 14 current batch loss: 0.10382590442895889\n",
            "epoch: 7 batch: 15 current batch loss: 0.09820783883333206\n",
            "epoch: 7 batch: 16 current batch loss: 0.09528549015522003\n",
            "epoch: 7 batch: 17 current batch loss: 0.09999052435159683\n",
            "epoch: 7 batch: 18 current batch loss: 0.07712272554636002\n",
            "epoch: 7 batch: 19 current batch loss: 0.09445028752088547\n",
            "epoch: 7 batch: 20 current batch loss: 0.09216299653053284\n",
            "epoch: 7 batch: 21 current batch loss: 0.09048188477754593\n",
            "epoch: 7 batch: 22 current batch loss: 0.09224580973386765\n",
            "epoch: 7 batch: 23 current batch loss: 0.08658978343009949\n",
            "epoch: 7 batch: 24 current batch loss: 0.09435557574033737\n",
            "epoch: 7 batch: 25 current batch loss: 0.07556460797786713\n",
            "epoch: 7 batch: 26 current batch loss: 0.09300701320171356\n",
            "epoch: 7 batch: 27 current batch loss: 0.07699324190616608\n",
            "epoch: 7 batch: 28 current batch loss: 0.08166532218456268\n",
            "epoch: 7 batch: 29 current batch loss: 0.09988310188055038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################ TESTING ON GPU #####################################################\n",
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              \n",
        "with torch.no_grad():   \n",
        "    for batch, data in enumerate(testloader): \n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint.to(device))                  # moving a batch to GPU\n",
        "        classification = torch.argmax(prediction)    \n",
        "\n",
        "        if classification.item() == label.item():     # the comparison is on item() which is not a tensor, i.e. there is no need to move labels to GPU\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxuAIBnoczmP",
        "outputId": "70c8557a-0cd6-4ebf-afcb-8c59ce718191"
      },
      "id": "dxuAIBnoczmP",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mf5Svh3FY0G2"
      },
      "id": "mf5Svh3FY0G2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}