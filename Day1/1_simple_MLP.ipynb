{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Workshops/blob/2023_1_solutions/Day1/1_simple_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
      "metadata": {
        "id": "fa309ca0-8167-40d3-8531-f52500c9e23d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers \n",
        "\n",
        "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
        "\n",
        "Linear layers must be separated by nonlinear components (also called *activation functions*). \n",
        "\n",
        "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
        "\n",
        "### Answers to task #1\n",
        "A nonlinear component in-between the linear layers is essential: \n",
        "1. without it, a compostion of linear components would be just a linear component, so a multilayer network would be equivalent to a single layered network. \n",
        "2. it is a nonlinear component that enables a neural network to express nonlinear functions, too.\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits 0-9. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf6b5b7-3bda-4b22-8e23-0fb9edfef2c3",
      "metadata": {
        "id": "ebf6b5b7-3bda-4b22-8e23-0fb9edfef2c3"
      },
      "source": [
        "# Before we begin\n",
        "\n",
        "There is a departure from the neuron-like biological terminology in ANN community. You will see even in this very simple example, that it is more convenient to think of layers not as of data points (neurons) but as of mathematical transformations (so a layer would be a matrix multiplication by weights or a layer would be aplication of nonlinear transform, or both, and in this latter case a layer would decompose further). \n",
        "\n",
        "In the case of more complex networks like Transformers it would be even hard to find a neuron analogy. Transformers explicitly work with matrices and generally with mathematical abstractions.\n",
        "\n",
        "The departure from the neural terminology is also justified by biology, itself. It turns out, that a single neuron in a brain behaves much more like a full artificial neural network than like an artificial neuron. \n",
        "\n",
        "The authors of the paper cited below show that, at the very least, a 5-layer 128-unit TCN — temporal convolutional network — is needed to simulate the I/O patterns of a pyramidal neuron at the millisecond resolution (single spike precision). To make a gross comparison: This means a single biological neuron needs between 640 and 2048 artificial neurons to be simulated adequately.\n",
        "\n",
        "[Beniaguev D, Segev I, London M. Single cortical neurons as deep artificial neural networks. Neuron. 2021 Sep 1;109(17):2727-2739.e3. doi: 10.1016/j.neuron.2021.07.002](https://pubmed.ncbi.nlm.nih.gov/34380016/)\n",
        "\n",
        "For the interested reader: [here you can read about Transformers](https://jalammar.github.io/illustrated-transformer/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68",
      "metadata": {
        "id": "00910f34-4eb2-4c25-b243-7914d1f1fe68"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
      "metadata": {
        "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72"
      },
      "source": [
        "### Reading MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
      "metadata": {
        "id": "a3f72963-c85d-49a6-9297-e14f823acc21"
      },
      "outputs": [],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
      "metadata": {
        "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
        "outputId": "25430e0e-63eb-45e2-b60e-752224256e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
      "metadata": {
        "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
        "outputId": "2e6a331c-713a-4463-cdc8-aa40036ff006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
      "metadata": {
        "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
        "outputId": "77eb2668-6daa-4a0a-ae79-38ad186053a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
      "metadata": {
        "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86"
      },
      "source": [
        "\n",
        "### Your task #2\n",
        "\n",
        "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28199903-bff6-431b-b855-32d37683ef2b",
      "metadata": {
        "id": "28199903-bff6-431b-b855-32d37683ef2b"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
        "\n",
        "*Tips:* \n",
        "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
        "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
      "metadata": {
        "id": "a756edcc-434b-4bbd-b171-b589a27b7f37",
        "outputId": "725c6a11-a9c4-4b06-b87c-207935afea7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1306604762738429, 0.30810780385646264)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "(trainset.data.numpy().mean()/255.0, trainset.data.numpy().std()/255.0)   #MNIST datapoints are RGB integers 0-255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
      "metadata": {
        "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69"
      },
      "source": [
        "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c16a443-c40d-430f-977b-e6749192950e",
      "metadata": {
        "id": "5c16a443-c40d-430f-977b-e6749192950e"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815882fa-4f93-403a-9221-36bb34649579",
      "metadata": {
        "id": "815882fa-4f93-403a-9221-36bb34649579"
      },
      "source": [
        "Let us visualise the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
      "metadata": {
        "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
        "outputId": "f50f56f5-6961-4806-d92c-5a3ebab82480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([9, 7, 9,  ..., 4, 6, 9])\n",
            "1 -th batch labels : tensor([0, 4, 3,  ..., 0, 9, 1])\n",
            "2 -th batch labels : tensor([6, 3, 6,  ..., 8, 8, 8])\n",
            "3 -th batch labels : tensor([0, 2, 6,  ..., 8, 4, 2])\n",
            "4 -th batch labels : tensor([9, 0, 1,  ..., 5, 9, 2])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
      "metadata": {
        "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2"
      },
      "source": [
        "\n",
        "### Your taks #4\n",
        "\n",
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch. \n",
        "\n",
        "Your task is to visualise and inspect the number of orders in data in batch_inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
      "metadata": {
        "id": "b1329ee0-827a-4a99-a0bd-b5b74087f274",
        "outputId": "dcc5de14-dc09-49fc-e54a-53e330b5e26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch inputs : tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
            "\n",
            "\n",
            "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          ...,\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
            "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i==0:\n",
        "            print(i, \"-th batch inputs :\", batch_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168",
      "metadata": {
        "id": "b75ea4d6-088e-40c8-84d5-354d4a37f168"
      },
      "source": [
        "OK, so each data image was initially a two dimensional image when we first saw it, but now the batches have order 4. The first index is a sample index within a batch, but a second index is always 0. This index represents a Channel number inserted here by ToTensor() transformation, always 0. As this order is one-dimensional, we can get rid of it, later, in training, in `Flatten()` layer or by using `squeeze()` on a tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
      "metadata": {
        "id": "19b73524-9d07-4882-a2b0-3e29233213a8"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Now, a definition of a simple MLP network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
      "metadata": {
        "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way, \n",
        "                                          #that output from a preceding layer serves as input to the next layer \n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.Sigmoid(),            #Sigmoid is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
      "metadata": {
        "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training consists of \n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
      "metadata": {
        "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
        "outputId": "34931d84-7d89-467b-8e5a-632743a7b33e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.348358392715454\n",
            "epoch: 0 batch: 1 current batch loss: 2.4188098907470703\n",
            "epoch: 0 batch: 2 current batch loss: 2.3433356285095215\n",
            "epoch: 0 batch: 3 current batch loss: 2.287778377532959\n",
            "epoch: 0 batch: 4 current batch loss: 2.290649175643921\n",
            "epoch: 0 batch: 5 current batch loss: 2.2987821102142334\n",
            "epoch: 0 batch: 6 current batch loss: 2.273108720779419\n",
            "epoch: 0 batch: 7 current batch loss: 2.2432844638824463\n",
            "epoch: 0 batch: 8 current batch loss: 2.2013280391693115\n",
            "epoch: 0 batch: 9 current batch loss: 2.1478593349456787\n",
            "epoch: 0 batch: 10 current batch loss: 2.093388080596924\n",
            "epoch: 0 batch: 11 current batch loss: 2.0318498611450195\n",
            "epoch: 0 batch: 12 current batch loss: 1.9707763195037842\n",
            "epoch: 0 batch: 13 current batch loss: 1.898699402809143\n",
            "epoch: 0 batch: 14 current batch loss: 1.7799267768859863\n",
            "epoch: 0 batch: 15 current batch loss: 1.7275538444519043\n",
            "epoch: 0 batch: 16 current batch loss: 1.6445443630218506\n",
            "epoch: 0 batch: 17 current batch loss: 1.6011385917663574\n",
            "epoch: 0 batch: 18 current batch loss: 1.5368987321853638\n",
            "epoch: 0 batch: 19 current batch loss: 1.4710110425949097\n",
            "epoch: 0 batch: 20 current batch loss: 1.407463550567627\n",
            "epoch: 0 batch: 21 current batch loss: 1.3627815246582031\n",
            "epoch: 0 batch: 22 current batch loss: 1.3348736763000488\n",
            "epoch: 0 batch: 23 current batch loss: 1.238632321357727\n",
            "epoch: 0 batch: 24 current batch loss: 1.2136598825454712\n",
            "epoch: 0 batch: 25 current batch loss: 1.1652753353118896\n",
            "epoch: 0 batch: 26 current batch loss: 1.1238659620285034\n",
            "epoch: 0 batch: 27 current batch loss: 1.1340495347976685\n",
            "epoch: 0 batch: 28 current batch loss: 1.0688652992248535\n",
            "epoch: 0 batch: 29 current batch loss: 1.0283191204071045\n",
            "epoch: 1 batch: 0 current batch loss: 0.9979045987129211\n",
            "epoch: 1 batch: 1 current batch loss: 0.952354907989502\n",
            "epoch: 1 batch: 2 current batch loss: 0.9776474237442017\n",
            "epoch: 1 batch: 3 current batch loss: 0.9364490509033203\n",
            "epoch: 1 batch: 4 current batch loss: 0.9077056646347046\n",
            "epoch: 1 batch: 5 current batch loss: 0.8904927968978882\n",
            "epoch: 1 batch: 6 current batch loss: 0.8178541660308838\n",
            "epoch: 1 batch: 7 current batch loss: 0.8341795206069946\n",
            "epoch: 1 batch: 8 current batch loss: 0.7704591155052185\n",
            "epoch: 1 batch: 9 current batch loss: 0.7672419548034668\n",
            "epoch: 1 batch: 10 current batch loss: 0.7587139010429382\n",
            "epoch: 1 batch: 11 current batch loss: 0.7598303556442261\n",
            "epoch: 1 batch: 12 current batch loss: 0.7253386974334717\n",
            "epoch: 1 batch: 13 current batch loss: 0.6611127257347107\n",
            "epoch: 1 batch: 14 current batch loss: 0.6583994626998901\n",
            "epoch: 1 batch: 15 current batch loss: 0.6855235695838928\n",
            "epoch: 1 batch: 16 current batch loss: 0.6528430581092834\n",
            "epoch: 1 batch: 17 current batch loss: 0.6331700682640076\n",
            "epoch: 1 batch: 18 current batch loss: 0.6328906416893005\n",
            "epoch: 1 batch: 19 current batch loss: 0.5919115543365479\n",
            "epoch: 1 batch: 20 current batch loss: 0.5839999914169312\n",
            "epoch: 1 batch: 21 current batch loss: 0.5831184983253479\n",
            "epoch: 1 batch: 22 current batch loss: 0.5778447389602661\n",
            "epoch: 1 batch: 23 current batch loss: 0.5459107160568237\n",
            "epoch: 1 batch: 24 current batch loss: 0.5576769113540649\n",
            "epoch: 1 batch: 25 current batch loss: 0.5177264213562012\n",
            "epoch: 1 batch: 26 current batch loss: 0.5322950482368469\n",
            "epoch: 1 batch: 27 current batch loss: 0.5246173739433289\n",
            "epoch: 1 batch: 28 current batch loss: 0.5306153893470764\n",
            "epoch: 1 batch: 29 current batch loss: 0.5226957201957703\n",
            "epoch: 2 batch: 0 current batch loss: 0.5006923675537109\n",
            "epoch: 2 batch: 1 current batch loss: 0.5007777810096741\n",
            "epoch: 2 batch: 2 current batch loss: 0.4695827066898346\n",
            "epoch: 2 batch: 3 current batch loss: 0.48090219497680664\n",
            "epoch: 2 batch: 4 current batch loss: 0.468080997467041\n",
            "epoch: 2 batch: 5 current batch loss: 0.44870302081108093\n",
            "epoch: 2 batch: 6 current batch loss: 0.4393797814846039\n",
            "epoch: 2 batch: 7 current batch loss: 0.4144822061061859\n",
            "epoch: 2 batch: 8 current batch loss: 0.43298038840293884\n",
            "epoch: 2 batch: 9 current batch loss: 0.4437716603279114\n",
            "epoch: 2 batch: 10 current batch loss: 0.4151555597782135\n",
            "epoch: 2 batch: 11 current batch loss: 0.38508743047714233\n",
            "epoch: 2 batch: 12 current batch loss: 0.4054654836654663\n",
            "epoch: 2 batch: 13 current batch loss: 0.42050087451934814\n",
            "epoch: 2 batch: 14 current batch loss: 0.38594552874565125\n",
            "epoch: 2 batch: 15 current batch loss: 0.3698962926864624\n",
            "epoch: 2 batch: 16 current batch loss: 0.36722850799560547\n",
            "epoch: 2 batch: 17 current batch loss: 0.39706310629844666\n",
            "epoch: 2 batch: 18 current batch loss: 0.388467013835907\n",
            "epoch: 2 batch: 19 current batch loss: 0.3764684498310089\n",
            "epoch: 2 batch: 20 current batch loss: 0.34918755292892456\n",
            "epoch: 2 batch: 21 current batch loss: 0.3428047001361847\n",
            "epoch: 2 batch: 22 current batch loss: 0.3687029480934143\n",
            "epoch: 2 batch: 23 current batch loss: 0.35195544362068176\n",
            "epoch: 2 batch: 24 current batch loss: 0.3624194264411926\n",
            "epoch: 2 batch: 25 current batch loss: 0.34877094626426697\n",
            "epoch: 2 batch: 26 current batch loss: 0.3261665105819702\n",
            "epoch: 2 batch: 27 current batch loss: 0.33614107966423035\n",
            "epoch: 2 batch: 28 current batch loss: 0.3029491901397705\n",
            "epoch: 2 batch: 29 current batch loss: 0.3381193280220032\n",
            "epoch: 3 batch: 0 current batch loss: 0.3305061459541321\n",
            "epoch: 3 batch: 1 current batch loss: 0.3386091887950897\n",
            "epoch: 3 batch: 2 current batch loss: 0.31271612644195557\n",
            "epoch: 3 batch: 3 current batch loss: 0.3127392530441284\n",
            "epoch: 3 batch: 4 current batch loss: 0.30408406257629395\n",
            "epoch: 3 batch: 5 current batch loss: 0.3247562348842621\n",
            "epoch: 3 batch: 6 current batch loss: 0.30864304304122925\n",
            "epoch: 3 batch: 7 current batch loss: 0.292728453874588\n",
            "epoch: 3 batch: 8 current batch loss: 0.2993946075439453\n",
            "epoch: 3 batch: 9 current batch loss: 0.27541032433509827\n",
            "epoch: 3 batch: 10 current batch loss: 0.2852969765663147\n",
            "epoch: 3 batch: 11 current batch loss: 0.2853624224662781\n",
            "epoch: 3 batch: 12 current batch loss: 0.2696288526058197\n",
            "epoch: 3 batch: 13 current batch loss: 0.2820112407207489\n",
            "epoch: 3 batch: 14 current batch loss: 0.2568541169166565\n",
            "epoch: 3 batch: 15 current batch loss: 0.2857494056224823\n",
            "epoch: 3 batch: 16 current batch loss: 0.2810932397842407\n",
            "epoch: 3 batch: 17 current batch loss: 0.2696952223777771\n",
            "epoch: 3 batch: 18 current batch loss: 0.29357612133026123\n",
            "epoch: 3 batch: 19 current batch loss: 0.2670270800590515\n",
            "epoch: 3 batch: 20 current batch loss: 0.2644803524017334\n",
            "epoch: 3 batch: 21 current batch loss: 0.27575603127479553\n",
            "epoch: 3 batch: 22 current batch loss: 0.2727747857570648\n",
            "epoch: 3 batch: 23 current batch loss: 0.26200994849205017\n",
            "epoch: 3 batch: 24 current batch loss: 0.26709696650505066\n",
            "epoch: 3 batch: 25 current batch loss: 0.23804454505443573\n",
            "epoch: 3 batch: 26 current batch loss: 0.24576610326766968\n",
            "epoch: 3 batch: 27 current batch loss: 0.2622864246368408\n",
            "epoch: 3 batch: 28 current batch loss: 0.265794038772583\n",
            "epoch: 3 batch: 29 current batch loss: 0.25532159209251404\n",
            "epoch: 4 batch: 0 current batch loss: 0.23905836045742035\n",
            "epoch: 4 batch: 1 current batch loss: 0.23852244019508362\n",
            "epoch: 4 batch: 2 current batch loss: 0.2593280076980591\n",
            "epoch: 4 batch: 3 current batch loss: 0.24885201454162598\n",
            "epoch: 4 batch: 4 current batch loss: 0.23148098587989807\n",
            "epoch: 4 batch: 5 current batch loss: 0.23483575880527496\n",
            "epoch: 4 batch: 6 current batch loss: 0.2051704227924347\n",
            "epoch: 4 batch: 7 current batch loss: 0.2328493446111679\n",
            "epoch: 4 batch: 8 current batch loss: 0.210591658949852\n",
            "epoch: 4 batch: 9 current batch loss: 0.22505232691764832\n",
            "epoch: 4 batch: 10 current batch loss: 0.22787129878997803\n",
            "epoch: 4 batch: 11 current batch loss: 0.22496801614761353\n",
            "epoch: 4 batch: 12 current batch loss: 0.20247982442378998\n",
            "epoch: 4 batch: 13 current batch loss: 0.21848681569099426\n",
            "epoch: 4 batch: 14 current batch loss: 0.21408866345882416\n",
            "epoch: 4 batch: 15 current batch loss: 0.2027977854013443\n",
            "epoch: 4 batch: 16 current batch loss: 0.21516923606395721\n",
            "epoch: 4 batch: 17 current batch loss: 0.22117292881011963\n",
            "epoch: 4 batch: 18 current batch loss: 0.23791836202144623\n",
            "epoch: 4 batch: 19 current batch loss: 0.2176177203655243\n",
            "epoch: 4 batch: 20 current batch loss: 0.21588028967380524\n",
            "epoch: 4 batch: 21 current batch loss: 0.21965302526950836\n",
            "epoch: 4 batch: 22 current batch loss: 0.22338758409023285\n",
            "epoch: 4 batch: 23 current batch loss: 0.2246299833059311\n",
            "epoch: 4 batch: 24 current batch loss: 0.203358992934227\n",
            "epoch: 4 batch: 25 current batch loss: 0.2060546576976776\n",
            "epoch: 4 batch: 26 current batch loss: 0.21014870703220367\n",
            "epoch: 4 batch: 27 current batch loss: 0.22009503841400146\n",
            "epoch: 4 batch: 28 current batch loss: 0.19789624214172363\n",
            "epoch: 4 batch: 29 current batch loss: 0.20891037583351135\n",
            "epoch: 5 batch: 0 current batch loss: 0.17600850760936737\n",
            "epoch: 5 batch: 1 current batch loss: 0.19747339189052582\n",
            "epoch: 5 batch: 2 current batch loss: 0.18979842960834503\n",
            "epoch: 5 batch: 3 current batch loss: 0.20593006908893585\n",
            "epoch: 5 batch: 4 current batch loss: 0.19403080642223358\n",
            "epoch: 5 batch: 5 current batch loss: 0.19442707300186157\n",
            "epoch: 5 batch: 6 current batch loss: 0.19819065928459167\n",
            "epoch: 5 batch: 7 current batch loss: 0.21572373807430267\n",
            "epoch: 5 batch: 8 current batch loss: 0.1671430766582489\n",
            "epoch: 5 batch: 9 current batch loss: 0.1665729284286499\n",
            "epoch: 5 batch: 10 current batch loss: 0.19185207784175873\n",
            "epoch: 5 batch: 11 current batch loss: 0.16873042285442352\n",
            "epoch: 5 batch: 12 current batch loss: 0.18741406500339508\n",
            "epoch: 5 batch: 13 current batch loss: 0.17970897257328033\n",
            "epoch: 5 batch: 14 current batch loss: 0.17882125079631805\n",
            "epoch: 5 batch: 15 current batch loss: 0.17714449763298035\n",
            "epoch: 5 batch: 16 current batch loss: 0.20378310978412628\n",
            "epoch: 5 batch: 17 current batch loss: 0.17263641953468323\n",
            "epoch: 5 batch: 18 current batch loss: 0.18571805953979492\n",
            "epoch: 5 batch: 19 current batch loss: 0.17011091113090515\n",
            "epoch: 5 batch: 20 current batch loss: 0.17928147315979004\n",
            "epoch: 5 batch: 21 current batch loss: 0.19197052717208862\n",
            "epoch: 5 batch: 22 current batch loss: 0.20691753923892975\n",
            "epoch: 5 batch: 23 current batch loss: 0.17374207079410553\n",
            "epoch: 5 batch: 24 current batch loss: 0.1640527844429016\n",
            "epoch: 5 batch: 25 current batch loss: 0.19067111611366272\n",
            "epoch: 5 batch: 26 current batch loss: 0.18074509501457214\n",
            "epoch: 5 batch: 27 current batch loss: 0.15682651102542877\n",
            "epoch: 5 batch: 28 current batch loss: 0.19112811982631683\n",
            "epoch: 5 batch: 29 current batch loss: 0.18523138761520386\n",
            "epoch: 6 batch: 0 current batch loss: 0.15954774618148804\n",
            "epoch: 6 batch: 1 current batch loss: 0.16238179802894592\n",
            "epoch: 6 batch: 2 current batch loss: 0.1524837762117386\n",
            "epoch: 6 batch: 3 current batch loss: 0.16677948832511902\n",
            "epoch: 6 batch: 4 current batch loss: 0.1663711965084076\n",
            "epoch: 6 batch: 5 current batch loss: 0.16698689758777618\n",
            "epoch: 6 batch: 6 current batch loss: 0.17437675595283508\n",
            "epoch: 6 batch: 7 current batch loss: 0.17488279938697815\n",
            "epoch: 6 batch: 8 current batch loss: 0.16189010441303253\n",
            "epoch: 6 batch: 9 current batch loss: 0.17926162481307983\n",
            "epoch: 6 batch: 10 current batch loss: 0.15994149446487427\n",
            "epoch: 6 batch: 11 current batch loss: 0.1668245494365692\n",
            "epoch: 6 batch: 12 current batch loss: 0.1521446704864502\n",
            "epoch: 6 batch: 13 current batch loss: 0.14980155229568481\n",
            "epoch: 6 batch: 14 current batch loss: 0.16223864257335663\n",
            "epoch: 6 batch: 15 current batch loss: 0.1481286585330963\n",
            "epoch: 6 batch: 16 current batch loss: 0.14650914072990417\n",
            "epoch: 6 batch: 17 current batch loss: 0.1628580242395401\n",
            "epoch: 6 batch: 18 current batch loss: 0.13652032613754272\n",
            "epoch: 6 batch: 19 current batch loss: 0.13856928050518036\n",
            "epoch: 6 batch: 20 current batch loss: 0.15160632133483887\n",
            "epoch: 6 batch: 21 current batch loss: 0.13545113801956177\n",
            "epoch: 6 batch: 22 current batch loss: 0.13406886160373688\n",
            "epoch: 6 batch: 23 current batch loss: 0.15038755536079407\n",
            "epoch: 6 batch: 24 current batch loss: 0.1580076664686203\n",
            "epoch: 6 batch: 25 current batch loss: 0.14262376725673676\n",
            "epoch: 6 batch: 26 current batch loss: 0.136900395154953\n",
            "epoch: 6 batch: 27 current batch loss: 0.12404821068048477\n",
            "epoch: 6 batch: 28 current batch loss: 0.16625462472438812\n",
            "epoch: 6 batch: 29 current batch loss: 0.16983012855052948\n",
            "epoch: 7 batch: 0 current batch loss: 0.14957062900066376\n",
            "epoch: 7 batch: 1 current batch loss: 0.13959185779094696\n",
            "epoch: 7 batch: 2 current batch loss: 0.12701740860939026\n",
            "epoch: 7 batch: 3 current batch loss: 0.16302478313446045\n",
            "epoch: 7 batch: 4 current batch loss: 0.13670805096626282\n",
            "epoch: 7 batch: 5 current batch loss: 0.14636126160621643\n",
            "epoch: 7 batch: 6 current batch loss: 0.15374755859375\n",
            "epoch: 7 batch: 7 current batch loss: 0.13619859516620636\n",
            "epoch: 7 batch: 8 current batch loss: 0.14287365972995758\n",
            "epoch: 7 batch: 9 current batch loss: 0.13179758191108704\n",
            "epoch: 7 batch: 10 current batch loss: 0.1371721774339676\n",
            "epoch: 7 batch: 11 current batch loss: 0.15035849809646606\n",
            "epoch: 7 batch: 12 current batch loss: 0.13824352622032166\n",
            "epoch: 7 batch: 13 current batch loss: 0.1445004791021347\n",
            "epoch: 7 batch: 14 current batch loss: 0.11564664542675018\n",
            "epoch: 7 batch: 15 current batch loss: 0.1377902776002884\n",
            "epoch: 7 batch: 16 current batch loss: 0.13776856660842896\n",
            "epoch: 7 batch: 17 current batch loss: 0.12423110008239746\n",
            "epoch: 7 batch: 18 current batch loss: 0.1197551041841507\n",
            "epoch: 7 batch: 19 current batch loss: 0.13532771170139313\n",
            "epoch: 7 batch: 20 current batch loss: 0.13937276601791382\n",
            "epoch: 7 batch: 21 current batch loss: 0.1430782973766327\n",
            "epoch: 7 batch: 22 current batch loss: 0.13611063361167908\n",
            "epoch: 7 batch: 23 current batch loss: 0.1404978185892105\n",
            "epoch: 7 batch: 24 current batch loss: 0.11665022373199463\n",
            "epoch: 7 batch: 25 current batch loss: 0.11252573132514954\n",
            "epoch: 7 batch: 26 current batch loss: 0.12609751522541046\n",
            "epoch: 7 batch: 27 current batch loss: 0.12956693768501282\n",
            "epoch: 7 batch: 28 current batch loss: 0.1385476440191269\n",
            "epoch: 7 batch: 29 current batch loss: 0.12682799994945526\n"
          ]
        }
      ],
      "source": [
        "net = MLP()\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
        "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
        "                                            #and MLP doesn't apply \n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item()) \n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network. \n",
        "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
      "metadata": {
        "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8"
      },
      "source": [
        "\n",
        "### Your task #5\n",
        "\n",
        "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either.\n",
        "\n",
        "An epoch is a one full passage through the whole training data. Why then, on the second epoch, the losses are different than in the first epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b257c80-3965-4bdf-9e47-0da1be6891de",
      "metadata": {
        "id": "9b257c80-3965-4bdf-9e47-0da1be6891de"
      },
      "source": [
        "### Answers to task #5\n",
        "\n",
        "The loss is not constant because in our code we are printing losses in batches, and each batch is a different data sample. A loss value calculated on different data sample may be different. Moreover, the second epoch and subsequent epochs, i.e. next runs through the whole data, they consist of different randomly shuffled batches, because we selected `shuffle = True` when initiating a training dataset. It means, that even in next epochs, batched samples will be different samples and the loss values may differ.\n",
        "\n",
        "But overall, loss desn't improve because the weights in the network do not change. The line responsible for changing the weights in the network is commented. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
      "metadata": {
        "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141"
      },
      "source": [
        "### Training - the second approach\n",
        "\n",
        "\n",
        "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
        "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of \n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- a definition of a scheduler to update the learning rate in an optimizer\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
      "metadata": {
        "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
        "outputId": "14cecc97-7c9e-42fe-b165-d267d14e2c96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.3388710021972656 current lr: 0.001\n",
            "epoch: 0 batch: 1 current batch loss: 2.395444393157959 current lr: 0.001\n",
            "epoch: 0 batch: 2 current batch loss: 2.363584041595459 current lr: 0.001\n",
            "epoch: 0 batch: 3 current batch loss: 2.322417736053467 current lr: 0.001\n",
            "epoch: 0 batch: 4 current batch loss: 2.286569833755493 current lr: 0.001\n",
            "epoch: 0 batch: 5 current batch loss: 2.293276071548462 current lr: 0.001\n",
            "epoch: 0 batch: 6 current batch loss: 2.2916300296783447 current lr: 0.001\n",
            "epoch: 0 batch: 7 current batch loss: 2.2794737815856934 current lr: 0.001\n",
            "epoch: 0 batch: 8 current batch loss: 2.241809844970703 current lr: 0.001\n",
            "epoch: 0 batch: 9 current batch loss: 2.21769380569458 current lr: 0.001\n",
            "epoch: 0 batch: 10 current batch loss: 2.1595876216888428 current lr: 0.001\n",
            "epoch: 0 batch: 11 current batch loss: 2.118454933166504 current lr: 0.001\n",
            "epoch: 0 batch: 12 current batch loss: 2.067172050476074 current lr: 0.001\n",
            "epoch: 0 batch: 13 current batch loss: 1.997836709022522 current lr: 0.001\n",
            "epoch: 0 batch: 14 current batch loss: 1.931765079498291 current lr: 0.001\n",
            "epoch: 0 batch: 15 current batch loss: 1.8428690433502197 current lr: 0.001\n",
            "epoch: 0 batch: 16 current batch loss: 1.7511131763458252 current lr: 0.001\n",
            "epoch: 0 batch: 17 current batch loss: 1.6672253608703613 current lr: 0.001\n",
            "epoch: 0 batch: 18 current batch loss: 1.608845829963684 current lr: 0.001\n",
            "epoch: 0 batch: 19 current batch loss: 1.5176399946212769 current lr: 0.001\n",
            "epoch: 0 batch: 20 current batch loss: 1.455374836921692 current lr: 0.001\n",
            "epoch: 0 batch: 21 current batch loss: 1.3729838132858276 current lr: 0.001\n",
            "epoch: 0 batch: 22 current batch loss: 1.311513066291809 current lr: 0.001\n",
            "epoch: 0 batch: 23 current batch loss: 1.2841790914535522 current lr: 0.001\n",
            "epoch: 0 batch: 24 current batch loss: 1.2157386541366577 current lr: 0.001\n",
            "epoch: 0 batch: 25 current batch loss: 1.1442627906799316 current lr: 0.001\n",
            "epoch: 0 batch: 26 current batch loss: 1.1183688640594482 current lr: 0.001\n",
            "epoch: 0 batch: 27 current batch loss: 1.0706796646118164 current lr: 0.001\n",
            "epoch: 0 batch: 28 current batch loss: 1.0289406776428223 current lr: 0.001\n",
            "epoch: 0 batch: 29 current batch loss: 1.0142349004745483 current lr: 0.001\n",
            "epoch: 1 batch: 0 current batch loss: 0.9557109475135803 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 1 current batch loss: 0.9633325338363647 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 2 current batch loss: 0.896721363067627 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 3 current batch loss: 0.9115339517593384 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 4 current batch loss: 0.8867632746696472 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 5 current batch loss: 0.817905604839325 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 6 current batch loss: 0.8361926078796387 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 7 current batch loss: 0.7951437830924988 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 8 current batch loss: 0.7323667407035828 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 9 current batch loss: 0.7357438206672668 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 10 current batch loss: 0.7260574698448181 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 11 current batch loss: 0.6915013194084167 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 12 current batch loss: 0.6564058065414429 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 13 current batch loss: 0.6390313506126404 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 14 current batch loss: 0.6320770382881165 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 15 current batch loss: 0.6263450980186462 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 16 current batch loss: 0.6042875647544861 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 17 current batch loss: 0.6180950403213501 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 18 current batch loss: 0.5848217010498047 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 19 current batch loss: 0.5244784355163574 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 20 current batch loss: 0.5473949909210205 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 21 current batch loss: 0.5692415237426758 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 22 current batch loss: 0.5518784523010254 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 23 current batch loss: 0.5306441783905029 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 24 current batch loss: 0.5165342092514038 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 25 current batch loss: 0.5056816339492798 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 26 current batch loss: 0.5199037790298462 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 27 current batch loss: 0.48194122314453125 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 28 current batch loss: 0.47496065497398376 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 29 current batch loss: 0.3871421217918396 current lr: 0.0009000000000000001\n",
            "epoch: 2 batch: 0 current batch loss: 0.45919281244277954 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 1 current batch loss: 0.437001496553421 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 2 current batch loss: 0.4062468111515045 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 3 current batch loss: 0.4573896527290344 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 4 current batch loss: 0.4409202039241791 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 5 current batch loss: 0.44032353162765503 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 6 current batch loss: 0.4095841944217682 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 7 current batch loss: 0.4105908274650574 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 8 current batch loss: 0.41496333479881287 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 9 current batch loss: 0.3985016345977783 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 10 current batch loss: 0.40621045231819153 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 11 current batch loss: 0.38896647095680237 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 12 current batch loss: 0.39959830045700073 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 13 current batch loss: 0.3894311189651489 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 14 current batch loss: 0.40692591667175293 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 15 current batch loss: 0.3827191889286041 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 16 current batch loss: 0.3743114173412323 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 17 current batch loss: 0.39340850710868835 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 18 current batch loss: 0.3581201136112213 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 19 current batch loss: 0.3351944088935852 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 20 current batch loss: 0.3638850152492523 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 21 current batch loss: 0.3906278610229492 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 22 current batch loss: 0.3581972122192383 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 23 current batch loss: 0.3592362701892853 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 24 current batch loss: 0.33570435643196106 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 25 current batch loss: 0.351318895816803 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 26 current batch loss: 0.31499090790748596 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 27 current batch loss: 0.3413734436035156 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 28 current batch loss: 0.3471272587776184 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 29 current batch loss: 0.3995361626148224 current lr: 0.0008100000000000001\n",
            "epoch: 3 batch: 0 current batch loss: 0.31392887234687805 current lr: 0.000729\n",
            "epoch: 3 batch: 1 current batch loss: 0.3134806752204895 current lr: 0.000729\n",
            "epoch: 3 batch: 2 current batch loss: 0.32006174325942993 current lr: 0.000729\n",
            "epoch: 3 batch: 3 current batch loss: 0.292303204536438 current lr: 0.000729\n",
            "epoch: 3 batch: 4 current batch loss: 0.326275497674942 current lr: 0.000729\n",
            "epoch: 3 batch: 5 current batch loss: 0.3280622065067291 current lr: 0.000729\n",
            "epoch: 3 batch: 6 current batch loss: 0.3347927927970886 current lr: 0.000729\n",
            "epoch: 3 batch: 7 current batch loss: 0.32067617774009705 current lr: 0.000729\n",
            "epoch: 3 batch: 8 current batch loss: 0.29654383659362793 current lr: 0.000729\n",
            "epoch: 3 batch: 9 current batch loss: 0.31748664379119873 current lr: 0.000729\n",
            "epoch: 3 batch: 10 current batch loss: 0.2972305715084076 current lr: 0.000729\n",
            "epoch: 3 batch: 11 current batch loss: 0.2973141074180603 current lr: 0.000729\n",
            "epoch: 3 batch: 12 current batch loss: 0.2829935550689697 current lr: 0.000729\n",
            "epoch: 3 batch: 13 current batch loss: 0.3065055310726166 current lr: 0.000729\n",
            "epoch: 3 batch: 14 current batch loss: 0.29439204931259155 current lr: 0.000729\n",
            "epoch: 3 batch: 15 current batch loss: 0.3162543475627899 current lr: 0.000729\n",
            "epoch: 3 batch: 16 current batch loss: 0.3136795163154602 current lr: 0.000729\n",
            "epoch: 3 batch: 17 current batch loss: 0.3002573847770691 current lr: 0.000729\n",
            "epoch: 3 batch: 18 current batch loss: 0.3076353371143341 current lr: 0.000729\n",
            "epoch: 3 batch: 19 current batch loss: 0.2682074010372162 current lr: 0.000729\n",
            "epoch: 3 batch: 20 current batch loss: 0.2618500590324402 current lr: 0.000729\n",
            "epoch: 3 batch: 21 current batch loss: 0.2735247015953064 current lr: 0.000729\n",
            "epoch: 3 batch: 22 current batch loss: 0.3102390170097351 current lr: 0.000729\n",
            "epoch: 3 batch: 23 current batch loss: 0.29554805159568787 current lr: 0.000729\n",
            "epoch: 3 batch: 24 current batch loss: 0.27958428859710693 current lr: 0.000729\n",
            "epoch: 3 batch: 25 current batch loss: 0.2685721516609192 current lr: 0.000729\n",
            "epoch: 3 batch: 26 current batch loss: 0.2939895689487457 current lr: 0.000729\n",
            "epoch: 3 batch: 27 current batch loss: 0.3025568425655365 current lr: 0.000729\n",
            "epoch: 3 batch: 28 current batch loss: 0.30912405252456665 current lr: 0.000729\n",
            "epoch: 3 batch: 29 current batch loss: 0.24333253502845764 current lr: 0.000729\n",
            "epoch: 4 batch: 0 current batch loss: 0.2666647732257843 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 1 current batch loss: 0.2676694989204407 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 2 current batch loss: 0.2614735960960388 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 3 current batch loss: 0.2759011685848236 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 4 current batch loss: 0.27363523840904236 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 5 current batch loss: 0.24465839564800262 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 6 current batch loss: 0.2595905363559723 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 7 current batch loss: 0.2851499915122986 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 8 current batch loss: 0.2512972354888916 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 9 current batch loss: 0.23885971307754517 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 10 current batch loss: 0.24722042679786682 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 11 current batch loss: 0.2446151077747345 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 12 current batch loss: 0.2246253937482834 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 13 current batch loss: 0.2558569610118866 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 14 current batch loss: 0.25604939460754395 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 15 current batch loss: 0.2485043704509735 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 16 current batch loss: 0.2356511801481247 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 17 current batch loss: 0.27186119556427 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 18 current batch loss: 0.21985960006713867 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 19 current batch loss: 0.271761417388916 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 20 current batch loss: 0.23813396692276 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 21 current batch loss: 0.25069743394851685 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 22 current batch loss: 0.220570370554924 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 23 current batch loss: 0.22450833022594452 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 24 current batch loss: 0.222610741853714 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 25 current batch loss: 0.24513418972492218 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 26 current batch loss: 0.226498544216156 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 27 current batch loss: 0.21810871362686157 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 28 current batch loss: 0.2565048933029175 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 29 current batch loss: 0.2128000557422638 current lr: 0.0006561000000000001\n",
            "epoch: 5 batch: 0 current batch loss: 0.20817111432552338 current lr: 0.00059049\n",
            "epoch: 5 batch: 1 current batch loss: 0.2205040156841278 current lr: 0.00059049\n",
            "epoch: 5 batch: 2 current batch loss: 0.23020611703395844 current lr: 0.00059049\n",
            "epoch: 5 batch: 3 current batch loss: 0.21416184306144714 current lr: 0.00059049\n",
            "epoch: 5 batch: 4 current batch loss: 0.22501535713672638 current lr: 0.00059049\n",
            "epoch: 5 batch: 5 current batch loss: 0.2201978862285614 current lr: 0.00059049\n",
            "epoch: 5 batch: 6 current batch loss: 0.20583896338939667 current lr: 0.00059049\n",
            "epoch: 5 batch: 7 current batch loss: 0.23555904626846313 current lr: 0.00059049\n",
            "epoch: 5 batch: 8 current batch loss: 0.20930956304073334 current lr: 0.00059049\n",
            "epoch: 5 batch: 9 current batch loss: 0.23345938324928284 current lr: 0.00059049\n",
            "epoch: 5 batch: 10 current batch loss: 0.22898489236831665 current lr: 0.00059049\n",
            "epoch: 5 batch: 11 current batch loss: 0.21132276952266693 current lr: 0.00059049\n",
            "epoch: 5 batch: 12 current batch loss: 0.2022600919008255 current lr: 0.00059049\n",
            "epoch: 5 batch: 13 current batch loss: 0.22927242517471313 current lr: 0.00059049\n",
            "epoch: 5 batch: 14 current batch loss: 0.22661671042442322 current lr: 0.00059049\n",
            "epoch: 5 batch: 15 current batch loss: 0.22234006226062775 current lr: 0.00059049\n",
            "epoch: 5 batch: 16 current batch loss: 0.18405179679393768 current lr: 0.00059049\n",
            "epoch: 5 batch: 17 current batch loss: 0.1858680546283722 current lr: 0.00059049\n",
            "epoch: 5 batch: 18 current batch loss: 0.20654191076755524 current lr: 0.00059049\n",
            "epoch: 5 batch: 19 current batch loss: 0.2079250067472458 current lr: 0.00059049\n",
            "epoch: 5 batch: 20 current batch loss: 0.2057153880596161 current lr: 0.00059049\n",
            "epoch: 5 batch: 21 current batch loss: 0.2046608328819275 current lr: 0.00059049\n",
            "epoch: 5 batch: 22 current batch loss: 0.20724646747112274 current lr: 0.00059049\n",
            "epoch: 5 batch: 23 current batch loss: 0.18712112307548523 current lr: 0.00059049\n",
            "epoch: 5 batch: 24 current batch loss: 0.20185835659503937 current lr: 0.00059049\n",
            "epoch: 5 batch: 25 current batch loss: 0.20558308064937592 current lr: 0.00059049\n",
            "epoch: 5 batch: 26 current batch loss: 0.21002118289470673 current lr: 0.00059049\n",
            "epoch: 5 batch: 27 current batch loss: 0.2081940621137619 current lr: 0.00059049\n",
            "epoch: 5 batch: 28 current batch loss: 0.20080453157424927 current lr: 0.00059049\n",
            "epoch: 5 batch: 29 current batch loss: 0.18494507670402527 current lr: 0.00059049\n",
            "epoch: 6 batch: 0 current batch loss: 0.18368080258369446 current lr: 0.000531441\n",
            "epoch: 6 batch: 1 current batch loss: 0.19366459548473358 current lr: 0.000531441\n",
            "epoch: 6 batch: 2 current batch loss: 0.19676153361797333 current lr: 0.000531441\n",
            "epoch: 6 batch: 3 current batch loss: 0.18920768797397614 current lr: 0.000531441\n",
            "epoch: 6 batch: 4 current batch loss: 0.17407259345054626 current lr: 0.000531441\n",
            "epoch: 6 batch: 5 current batch loss: 0.20531292259693146 current lr: 0.000531441\n",
            "epoch: 6 batch: 6 current batch loss: 0.19670182466506958 current lr: 0.000531441\n",
            "epoch: 6 batch: 7 current batch loss: 0.17978137731552124 current lr: 0.000531441\n",
            "epoch: 6 batch: 8 current batch loss: 0.20785003900527954 current lr: 0.000531441\n",
            "epoch: 6 batch: 9 current batch loss: 0.20538614690303802 current lr: 0.000531441\n",
            "epoch: 6 batch: 10 current batch loss: 0.18641719222068787 current lr: 0.000531441\n",
            "epoch: 6 batch: 11 current batch loss: 0.1873254030942917 current lr: 0.000531441\n",
            "epoch: 6 batch: 12 current batch loss: 0.18608658015727997 current lr: 0.000531441\n",
            "epoch: 6 batch: 13 current batch loss: 0.18530073761940002 current lr: 0.000531441\n",
            "epoch: 6 batch: 14 current batch loss: 0.20255301892757416 current lr: 0.000531441\n",
            "epoch: 6 batch: 15 current batch loss: 0.17600110173225403 current lr: 0.000531441\n",
            "epoch: 6 batch: 16 current batch loss: 0.17382757365703583 current lr: 0.000531441\n",
            "epoch: 6 batch: 17 current batch loss: 0.18861372768878937 current lr: 0.000531441\n",
            "epoch: 6 batch: 18 current batch loss: 0.18260106444358826 current lr: 0.000531441\n",
            "epoch: 6 batch: 19 current batch loss: 0.18588005006313324 current lr: 0.000531441\n",
            "epoch: 6 batch: 20 current batch loss: 0.18326064944267273 current lr: 0.000531441\n",
            "epoch: 6 batch: 21 current batch loss: 0.18586626648902893 current lr: 0.000531441\n",
            "epoch: 6 batch: 22 current batch loss: 0.17649546265602112 current lr: 0.000531441\n",
            "epoch: 6 batch: 23 current batch loss: 0.18857546150684357 current lr: 0.000531441\n",
            "epoch: 6 batch: 24 current batch loss: 0.18417203426361084 current lr: 0.000531441\n",
            "epoch: 6 batch: 25 current batch loss: 0.17238681018352509 current lr: 0.000531441\n",
            "epoch: 6 batch: 26 current batch loss: 0.18825663626194 current lr: 0.000531441\n",
            "epoch: 6 batch: 27 current batch loss: 0.19127480685710907 current lr: 0.000531441\n",
            "epoch: 6 batch: 28 current batch loss: 0.16572023928165436 current lr: 0.000531441\n",
            "epoch: 6 batch: 29 current batch loss: 0.16388331353664398 current lr: 0.000531441\n",
            "epoch: 7 batch: 0 current batch loss: 0.1709187924861908 current lr: 0.0004782969\n",
            "epoch: 7 batch: 1 current batch loss: 0.15379472076892853 current lr: 0.0004782969\n",
            "epoch: 7 batch: 2 current batch loss: 0.18241499364376068 current lr: 0.0004782969\n",
            "epoch: 7 batch: 3 current batch loss: 0.17635764181613922 current lr: 0.0004782969\n",
            "epoch: 7 batch: 4 current batch loss: 0.15924958884716034 current lr: 0.0004782969\n",
            "epoch: 7 batch: 5 current batch loss: 0.15226592123508453 current lr: 0.0004782969\n",
            "epoch: 7 batch: 6 current batch loss: 0.17120985686779022 current lr: 0.0004782969\n",
            "epoch: 7 batch: 7 current batch loss: 0.16780634224414825 current lr: 0.0004782969\n",
            "epoch: 7 batch: 8 current batch loss: 0.16375699639320374 current lr: 0.0004782969\n",
            "epoch: 7 batch: 9 current batch loss: 0.20075668394565582 current lr: 0.0004782969\n",
            "epoch: 7 batch: 10 current batch loss: 0.17529195547103882 current lr: 0.0004782969\n",
            "epoch: 7 batch: 11 current batch loss: 0.16881035268306732 current lr: 0.0004782969\n",
            "epoch: 7 batch: 12 current batch loss: 0.166341632604599 current lr: 0.0004782969\n",
            "epoch: 7 batch: 13 current batch loss: 0.17836761474609375 current lr: 0.0004782969\n",
            "epoch: 7 batch: 14 current batch loss: 0.16299937665462494 current lr: 0.0004782969\n",
            "epoch: 7 batch: 15 current batch loss: 0.1784466654062271 current lr: 0.0004782969\n",
            "epoch: 7 batch: 16 current batch loss: 0.1466078758239746 current lr: 0.0004782969\n",
            "epoch: 7 batch: 17 current batch loss: 0.16961336135864258 current lr: 0.0004782969\n",
            "epoch: 7 batch: 18 current batch loss: 0.15937726199626923 current lr: 0.0004782969\n",
            "epoch: 7 batch: 19 current batch loss: 0.1655525267124176 current lr: 0.0004782969\n",
            "epoch: 7 batch: 20 current batch loss: 0.18831178545951843 current lr: 0.0004782969\n",
            "epoch: 7 batch: 21 current batch loss: 0.1470353752374649 current lr: 0.0004782969\n",
            "epoch: 7 batch: 22 current batch loss: 0.1633366197347641 current lr: 0.0004782969\n",
            "epoch: 7 batch: 23 current batch loss: 0.17268183827400208 current lr: 0.0004782969\n",
            "epoch: 7 batch: 24 current batch loss: 0.14271660149097443 current lr: 0.0004782969\n",
            "epoch: 7 batch: 25 current batch loss: 0.1516677439212799 current lr: 0.0004782969\n",
            "epoch: 7 batch: 26 current batch loss: 0.16500714421272278 current lr: 0.0004782969\n",
            "epoch: 7 batch: 27 current batch loss: 0.1745157241821289 current lr: 0.0004782969\n",
            "epoch: 7 batch: 28 current batch loss: 0.19417957961559296 current lr: 0.0004782969\n",
            "epoch: 7 batch: 29 current batch loss: 0.16220898926258087 current lr: 0.0004782969\n"
          ]
        }
      ],
      "source": [
        "net_with_scheduler = MLP()\n",
        "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001. \n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
        "\n",
        "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear \n",
        "                                            #and MLP doesn't apply \n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        \n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0]) \n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network. \n",
        "                                \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
      "metadata": {
        "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592"
      },
      "source": [
        "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
        "### Your task #6\n",
        "\n",
        "Well, it seems that we were able to get the learning to levels of 0.1 even without a scheduler. Can you bring it under 0.05? Can you keep it under 0.05? Maybe the proposed gamma was to low (0.9 only)?. Please experiment with different settings for the optimizer learning rate and different scheduler settings. There are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
        "\n",
        "![ledger](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/ledger.png) Some other schedulers you might want to experiment with: \n",
        "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) \n",
        "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
        "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
        "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
        "\n",
        "![pencil](https://www.webfx.com/wp-content/themes/fx/assets/img/tools/emoji-cheat-sheet/graphics/emojis/pencil2.png) \n",
        "### Your task #7\n",
        "\n",
        "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
      "metadata": {
        "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Now we will test those two nets - the one without and the one with the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
      "metadata": {
        "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
        "outputId": "210a8ac1-84c4-4e72-8cca-45ae6881f350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9678\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
      "metadata": {
        "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
        "outputId": "0d9d8989-9b51-42be-9c84-4c36d09bf6eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9621\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
      "metadata": {
        "id": "e4831d2c-ee3f-42be-969b-627d888692c8"
      },
      "source": [
        "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}