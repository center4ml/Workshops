{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Workshops/blob/2023_2/Day_1/0_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "FqN61PZKgElr"
      },
      "id": "FqN61PZKgElr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e6d06270-eb68-428e-9661-202cafbe6fc0",
      "metadata": {
        "id": "e6d06270-eb68-428e-9661-202cafbe6fc0"
      },
      "source": [
        "\n",
        "# Before we begin\n",
        "\n",
        "## Departure from neuron-like terminology. Arrival of mathematically oriented terminology\n",
        "\n",
        "There is a departure from the neuron-like biological terminology in ANN community. You will see even in this very simple example, that it is more convenient to think of layers not as of data points (neurons) but as of mathematical transformations (so a layer would be a matrix multiplication by weights or a layer would be aplication of nonlinear transform, or both, and in this latter case a layer would decompose further).\n",
        "\n",
        "In the case of more complex networks like Transformers it would be even hard to find a neuron analogy. Transformers explicitly work with matrices and generally with mathematical abstractions.\n",
        "\n",
        "The departure from the neural terminology is also justified by biology, itself. It turns out, that a single neuron in a brain behaves much more like a full artificial neural network than like an artificial neuron.\n",
        "\n",
        "The authors of the paper cited below show that, at the very least, a 5-layer 128-unit TCN — temporal convolutional network — is needed to simulate the I/O patterns of a pyramidal neuron at the millisecond resolution (single spike precision). To make a gross comparison: This means a single biological neuron needs between 640 and 2048 artificial neurons to be simulated adequately.\n",
        "\n",
        "[Beniaguev D, Segev I, London M. Single cortical neurons as deep artificial neural networks. Neuron. 2021 Sep 1;109(17):2727-2739.e3. doi: 10.1016/j.neuron.2021.07.002](https://pubmed.ncbi.nlm.nih.gov/34380016/)\n",
        "\n",
        "**For the interested reader: [here you can read about Transformers](https://jalammar.github.io/illustrated-transformer/).**\n",
        "\n",
        "# Let's talk about Tensors\n",
        "\n",
        "A PyTorch or TensorFlow Tensor is just a **multidimensional array**. But it is much more transformation-centered, and when I say *transformation* I mean mathematical transformation.\n",
        "\n",
        "Many times in the past it proved for me hard to speak clearly of Tensor transformations and dimensions. Below is a screenshot from a random tutorial, which shows a clear confusion when the author talks about Tensor dimensions.\n",
        "\n",
        "![Tensor dimensions confusion - screenshot from https://beerensahu.wordpress.com/2018/03/21/pytorch-tutorial-lesson-1-tensor/](https://i.imgur.com/jikCq6K.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e5cef8-40d6-4d5b-bb97-e4df26a43afa",
      "metadata": {
        "id": "d8e5cef8-40d6-4d5b-bb97-e4df26a43afa",
        "outputId": "c514a7a7-b3f4-4d86-ab4b-3eed0e51d25f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.9204e-12,  4.4884e-41, -9.1724e+01],\n",
              "        [ 3.2007e-41,  2.4085e+09,  4.4882e-41]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.Tensor(2,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6c9412-13c2-485b-a1ec-bbc65b4036bc",
      "metadata": {
        "id": "eb6c9412-13c2-485b-a1ec-bbc65b4036bc"
      },
      "source": [
        "Considering that a Tensor is a much more mathematically inclined object than a multidimensional array, let me introduce a naming convention which will be compatible with mathematical objects a Tensor represents. For instance, a vector from $\\mathbb{R}^n$ is considered $n$-dimensional in mathematics but it can be stored in a one-dimensional array, a matrix in $\\mathbb{R}^{r \\times c}$ is considered to be $r \\times c$-dimensional  object, but can be stored in a two-dimensional array. This duality creates a confusion. In my experience the following naming convention is coherent considering that Tensors **are** mathematical objects.\n",
        "\n",
        "Tensor Terms  | Meaning | Multidimensional Array Terms\n",
        "---|---|---\n",
        "**order** | number of indices (levels) in a Tensor | dimension\n",
        "**dimension** | number of components a Tensor can store in a given order | size\n",
        "Tensor of order zero | a constant | - |\n",
        "Tensor of order one | a vector | one-dimensional array |\n",
        "Tensor of order two | a matrix | two-dimensional array |\n",
        "`torch.tensor([1.1, 4.12, 8.9, 14.85])` | an order-one 4-dimensional tensor representing a vector in $\\mathbb{R}^4$ | `[1.1, 4.12, 8.9, 14.85]`\n",
        "\n",
        "\n",
        "**Now, together, let's go through [this presentation on tensors and basic layers](https://drive.google.com/file/d/1JyTDpcDhe3Ep3bnzEtK5eP0KhaDru8Nc/view?usp=sharing).**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}