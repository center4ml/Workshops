{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/center4ml/Workshops/blob/2023_2_solutions/Day_1/2_loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1389d586-2bf4-4817-924d-b5082a7d4542",
      "metadata": {
        "id": "1389d586-2bf4-4817-924d-b5082a7d4542"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1bee46-53a6-455b-85e5-258f6810a41a",
      "metadata": {
        "id": "1a1bee46-53a6-455b-85e5-258f6810a41a"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "In neural network supervised training (and to lesser extent in evaluation) one of the key concepts is the loss function. The loss function measures what is the distance between the known target values and neural network predictions. A good loss function is differentiable and has non-zero gradients.\n",
        "\n",
        "As an example, **Accuracy**, a measure often used in neural network evaluation in classification tasks, has gradients almost everywhere equal to zero. This is not good to train the neural network, which needs to update the weights in a process called backpropagation, and for that it needs non-zero gradients. Thus, Accuracy isn't suitable as a loss function.\n",
        "\n",
        "**[Some extra reading material on that topic](https://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a).**\n",
        "\n",
        "Examples of loss functions:\n",
        "\n",
        "- Mean Square Error - MSE - used for regression,\n",
        "- Binary Cross Entropy - a measure used for classification with two classes, it approximates Accuracy but has non-zero gradients,\n",
        "- Cross Entropy - a measure used for general classification.\n",
        "\n",
        "Let us provide an exact definition of MSE.\n",
        "\n",
        "If a target vector is\n",
        "\n",
        "$T=(T_i), i=1, \\ldots, N$\n",
        "\n",
        "and a prediction vector of a regressor is\n",
        "\n",
        "$P=(P_i), i=1, \\ldots, N$\n",
        "\n",
        "Then $MSE(P, T) = \\frac{\\sum_{i=1}^N (T_i-P_i)^2}{N}$\n",
        "\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "Calculate, using Python, MSE loss of the prediction $P=(1.1, 4.12, 8.9, 14.85)$ versus the target values $T=(1,4,9,16)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d557ccbd-da3d-4184-a3dd-d2f45acedb9e",
      "metadata": {
        "id": "d557ccbd-da3d-4184-a3dd-d2f45acedb9e",
        "outputId": "383b1350-b374-4f5e-e52a-4ed5eeb0cd37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3392250000000002\n"
          ]
        }
      ],
      "source": [
        "target = [1, 4, 9, 16]\n",
        "prediction = [1.1, 4.12, 8.9, 14.85]\n",
        "sum = 0.0\n",
        "for i in range(4):\n",
        "    sum += (target[i]-prediction[i])**2\n",
        "mse = sum/len(target)\n",
        "print(mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de13e007-c5e4-43c3-9138-f5ef678d3653",
      "metadata": {
        "id": "de13e007-c5e4-43c3-9138-f5ef678d3653"
      },
      "source": [
        "# Predefined Loss Functions in PyTorch\n",
        "\n",
        "In PyTorch there are predefined methods for some of the loss functions. Of course, one is free to define his own loss functions, too, but a predefined loss functions have some advantages\n",
        "- the implementation is numerically stable. As an example, Cross Entropy has a logarithm following the exponent. If you do that correctly, the result is identity. But the exponent of even moderately large values is infinite in numerical calculations.\n",
        "- they usually have more efficient implementations\n",
        "- they have built-in reduction methods\n",
        "\n",
        "The predefined loss functions in PyTorch are all part of `torch.nn.functional` and are [documented here](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
        "\n",
        "Let us have a look at a predefined version of MSE loss that we've just calculated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e85b939-3b1a-4029-acf9-3da47d473f4d",
      "metadata": {
        "id": "1e85b939-3b1a-4029-acf9-3da47d473f4d",
        "outputId": "415ad4de-5881-4477-d1b5-c2b54cdf5153"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.3392)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explain why the naive calculation of log-exp pair is numerically unstable, let's have a look at this example:"
      ],
      "metadata": {
        "id": "umF0YssQV7ww"
      },
      "id": "umF0YssQV7ww"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(700, 800):\n",
        "  print(i, math.log(math.exp(i)))"
      ],
      "metadata": {
        "id": "3ggZ7xIGV_Pf"
      },
      "id": "3ggZ7xIGV_Pf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "23f1564a-61cd-4429-b9dd-c36ca674db1c",
      "metadata": {
        "id": "23f1564a-61cd-4429-b9dd-c36ca674db1c"
      },
      "source": [
        "### Reductions\n",
        "\n",
        "Reductions are ways to consolidate the result into one value. Depending on the exact use-case,\n",
        "a user may want to calculate mean loos (as in the definition of MSE) or the sum or even the individual loss values (before any consolidation). This can be controlled with the\n",
        "additional parameter `reduction`\n",
        "\n",
        "- `reduction=\"mean\"`  (this is the default)\n",
        "- `reduction=\"sum\"`\n",
        "- `reduction=\"none\"` to get an array of individual results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7844f0-1cbd-45ac-87b0-c757d9ff0959",
      "metadata": {
        "id": "0f7844f0-1cbd-45ac-87b0-c757d9ff0959",
        "outputId": "18de2d65-8d20-480c-c0ef-7c2449115e51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.3569)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]), reduction=\"sum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c5c84d-1ec7-4964-b41b-4d29b7aadeca",
      "metadata": {
        "id": "57c5c84d-1ec7-4964-b41b-4d29b7aadeca",
        "outputId": "d82dae6d-020b-414e-efa8-1da551c96f2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0100, 0.0144, 0.0100, 1.3225])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.mse_loss(torch.tensor([1.1, 4.12, 8.9, 14.85]), torch.tensor([1, 4, 9, 16]), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42802f49-30ae-4532-ab0b-edf1b9cbd73d",
      "metadata": {
        "id": "42802f49-30ae-4532-ab0b-edf1b9cbd73d"
      },
      "source": [
        "We can see that the fourth element of a prediction error had a largest contribution in a MSE by using `reduction = \"none\"`. If you want to sum the values, rather than average them, use `reduction = \"sum\"`.\n",
        "\n",
        "## Classification loss\n",
        "\n",
        "### Two classes - Binary Cross Entropy\n",
        "\n",
        "OK, now let's examine other loss functions, the ones that are used for classification. If the target is only classes 0 or 1, and predictions are **class probabilities**, i.e. the floats between 0 and 1, then it is binary classification and the appropriate loss is Binary Cross Entropy loss with the following usage example in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4926174c-31c5-4cfc-93e0-3af3b31e2d34",
      "metadata": {
        "id": "4926174c-31c5-4cfc-93e0-3af3b31e2d34",
        "outputId": "f7eab080-0373-4a2f-9b3f-5979c466212a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.2614, 0.1165, 0.0101])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.binary_cross_entropy(torch.tensor([0.0, 0.77, 0.11, 0.99]), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "155c9724-b096-45ee-b3f8-19cbaa69226f",
      "metadata": {
        "id": "155c9724-b096-45ee-b3f8-19cbaa69226f"
      },
      "source": [
        "If the target is only classes 0 or 1, and predictions are arbitrary floats, then it is still binary classification but before using Binary Cross Entropy loss the values should be transformed first into $<0,1>$ interval with the use of a Sigmoid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14579ae-7ea3-493c-b694-36fcb483d47c",
      "metadata": {
        "id": "d14579ae-7ea3-493c-b694-36fcb483d47c",
        "outputId": "b77de0d3-0310-4cdb-9b32-96e399548235"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.2693e-01, 4.2789e-02, 6.9315e-01, 1.0000e+02])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.binary_cross_entropy(torch.sigmoid(torch.tensor([-2.0, 3.13, 0.0, -120.0])), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf37b51-fc2e-4b6b-9325-95a4b827cea0",
      "metadata": {
        "id": "dbf37b51-fc2e-4b6b-9325-95a4b827cea0"
      },
      "source": [
        "Or, you can apply a sigmoid automatically (which is recommended because of numerical stability) with using `torch.nn.functional.binary_cross_entropy_with_logits()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c366a7-793e-4168-8fa2-2854b2a53184",
      "metadata": {
        "id": "93c366a7-793e-4168-8fa2-2854b2a53184",
        "outputId": "0936ac69-395f-4d87-8009-c86c6e1b0f1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.2693e-01, 4.2789e-02, 6.9315e-01, 1.2000e+02])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-2.0, 3.13, 0.0, -120.0]), torch.tensor([0,1,0,1]).type(torch.float), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43243162-d179-4917-ae44-39dac88763dd",
      "metadata": {
        "id": "43243162-d179-4917-ae44-39dac88763dd"
      },
      "source": [
        "### Arbitrary number of classes - Cross Entropy\n",
        "\n",
        "It used for multiclass classification, but - in principle - there is nothing stopping us from using it for two classes, too. Please observe, that instead of a vector of logits, you must provide the loss function with raw predictions for all classes (thus, as predictions, we provide a tensor with one more level, i.e. with an increased order).\n",
        "\n",
        "**This is very important: softmax is executed internally while calculating loss, so there should be no softmax application as part of neural network forward pass**.\n",
        "\n",
        "Observe also, that classes are provided as `torch.long` type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003f5c2f-68b6-42e0-8f66-1ab226473f0e",
      "metadata": {
        "id": "003f5c2f-68b6-42e0-8f66-1ab226473f0e",
        "outputId": "2ef09e63-6b99-48f7-cde0-7f0532dfb6c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  0.6931,   0.6931,   0.6931, 120.0000])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.cross_entropy(torch.tensor([[-2.0, -2.0], [3.14, 3.14], [0.0, 0.0], [120.0, 0.0]]), torch.tensor([0,1,0,1]).type(torch.long), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f266b91-b360-4d9b-8b1a-6638d0498687",
      "metadata": {
        "id": "7f266b91-b360-4d9b-8b1a-6638d0498687"
      },
      "source": [
        "### Summary of classification loss options\n",
        "\n",
        "![summary](https://imgur.com/COfTYRh.png)\n",
        "\n",
        "### Your task #2\n",
        "\n",
        "You do a two class classification task. Your classifier has a few layers, but it does NOT have a sigmoid as the final nonlinarity. Rather, it may output arbitrary values between minus and plus infinity.\n",
        "\n",
        "For the input data examples with known classes $(0,1,0,1,1,1,0)$ the classifier outputs $(-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9)$.\n",
        "\n",
        "What is the mean Binary Cross Entropy Loss?\n",
        "What is the Binary Cross Entropy Loss for the individual data points?\n",
        "Which data point has maximal loss, and which has the minimal loss?\n",
        "\n",
        "**Remember!**\n",
        "- target tensor should be provided as `torch.float` type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091b76c5-e1de-487d-bae2-e941ea66febf",
      "metadata": {
        "id": "091b76c5-e1de-487d-bae2-e941ea66febf",
        "outputId": "0132b8b7-d9ed-4bf6-b6a7-10486cb38511"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.1366)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"mean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8510b7fe-7f6f-4385-bb96-726b05948990",
      "metadata": {
        "id": "8510b7fe-7f6f-4385-bb96-726b05948990",
        "outputId": "06870700-6fab-459e-d2fc-fb14e473da00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2.7418e-06, 4.8587e-02, 8.5436e-01, 5.3563e-02, 0.0000e+00, 7.1526e-07,\n",
              "        6.7949e-06])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfc0f5e-31a9-4aa9-baf3-9436b7e33c18",
      "metadata": {
        "id": "adfc0f5e-31a9-4aa9-baf3-9436b7e33c18",
        "outputId": "777f3bdd-84e4-4521-ddaa-307bf67fb6c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"none\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df20d37-d191-4bf4-b28a-84d95dc0d741",
      "metadata": {
        "id": "7df20d37-d191-4bf4-b28a-84d95dc0d741",
        "outputId": "eec045d0-376a-4750-a986-59fb09c5cc5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmin(torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([-12.8, 3.0, 0.3, 2.9, 17.3, 14.2, -11.9]), torch.tensor([0, 1, 0, 1, 1, 1, 0]).type(torch.float), reduction=\"none\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1df8167-e708-4a02-b5c8-d22e70732644",
      "metadata": {
        "id": "f1df8167-e708-4a02-b5c8-d22e70732644"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "You do a three class classification task. Your classifier has a few layers and it outputs arbitrary values between minus and plus infinity.\n",
        "\n",
        "For the four input data examples with known classes $(0, 2, 1, 0)$ the classifier outputs\n",
        "- $(2.8, -2.0, 0.1)$ for the first data point,\n",
        "- $(2.1, 0.3, 1.8)$ for the second data point,\n",
        "- $(0.2, 0.2, 0.3)$ for the third data point,\n",
        "- $(0.1, -0.2, 0.0)$ for the last data point.\n",
        "\n",
        "What is the mean Cross Entropy Loss?\n",
        "What is the Cross Entropy Loss for the individual data points?\n",
        "Which data point has maximal loss, and which has the minimal loss?\n",
        "\n",
        "**Remember!**\n",
        "- increase the order of prediction tensor by one level.\n",
        "- target classes should be provided as `torch.long` type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdc8c00-a576-4ebf-8d8f-d7f09af8128f",
      "metadata": {
        "id": "4fdc8c00-a576-4ebf-8d8f-d7f09af8128f",
        "outputId": "b5c42005-6e91-452f-92d9-bc201540a66f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7809)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"mean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0444e08c-c486-4103-83ce-ef382dadf6e9",
      "metadata": {
        "id": "0444e08c-c486-4103-83ce-ef382dadf6e9",
        "outputId": "cac39caf-9234-48dc-da7d-9d44ea246430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0727, 0.9451, 1.1331, 0.9729])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4644a11-7f5f-462a-893e-e1290e62b239",
      "metadata": {
        "id": "e4644a11-7f5f-462a-893e-e1290e62b239",
        "outputId": "84a0e5d3-a3de-4e59-8342-b06a4aec7f4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmin(torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"none\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb0b2e7-8ba9-4635-acf9-8205ce2c6eae",
      "metadata": {
        "id": "cdb0b2e7-8ba9-4635-acf9-8205ce2c6eae",
        "outputId": "f26c004d-2784-4df1-a89f-7ac08f251a1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(torch.nn.functional.cross_entropy(torch.tensor([[2.8, -2.0, 0.1], [2.1, 0.3, 1.8], [0.2, 0.2, 0.3], [0.1, -0.2, 0.0]]), torch.tensor([0, 2, 1, 0]).type(torch.long), reduction=\"none\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
