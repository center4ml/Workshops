{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Workshops/blob/2023_2/Day_1/3_simple_MLP_Classification_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "_rQws4T1lq7q"
      },
      "id": "_rQws4T1lq7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fa309ca0-8167-40d3-8531-f52500c9e23d",
      "metadata": {
        "id": "fa309ca0-8167-40d3-8531-f52500c9e23d"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will dive into a construction of a simple MultiLayer Perceptron neural network. A MultiLayer Perceptron, also termed MLP, is a simple network consisting of a few fully connected linear layers\n",
        "\n",
        "![MLP - image from https://www.researchgate.net/publication/341626283_Prioritizing_and_Analyzing_the_Role_of_Climate_and_Urban_Parameters_in_the_Confirmed_Cases_of_COVID-19_Based_on_Artificial_Intelligence_Applications](https://i.imgur.com/8cw4GD3.png)\n",
        "\n",
        "Linear layers must be separated by nonlinear components (also called *activation functions*).\n",
        "\n",
        "![non-linear components - image from https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron](https://imgur.com/L3YxcSs.png)\n",
        "\n",
        "### Your task #1\n",
        "\n",
        "What is the purpose of the non-linearities in-between the layers of the neural network?\n",
        "\n",
        "## Scope of work today - classification\n",
        "\n",
        "In this workshop, we will construct an MLP network designed to a specific task of classification of MNIST dataset: a set of handwritten digits from *zero* to *nine*. MNIST stands for Modified National Institute of Standards and Technology database.\n",
        "\n",
        "**You can read more about this dataset [here](https://colah.github.io/posts/2014-10-Visualizing-MNIST/#MNIST).**\n",
        "\n",
        "# Mathematically oriented notation for the MLP\n",
        "\n",
        "In this workshop we will be classifying 28 by 28 images into 10 classes. Thus, a three layer perceptron we will work further with can be defined in a mathematematically apealing way as\n",
        "\n",
        "$f:\\mathbb{R}^{28\\cdot 28} \\rightarrow \\mathbb{R}^{10}$ defined as\n",
        "\n",
        "$f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right) =  W_4 \\left[ W_3 \\left[ W_2 \\left[ W_1 x  + b_1 \\right]_+  + b_2 \\right]_+  + b_3 \\right]_+ + b_4$,\n",
        "\n",
        "where matrices $W_1, \\ldots, W_4$ are tensors of order two (matrices) with matching dimensions and bias terms $b_1, \\ldots, b_4$ are tensors of order one (vectors) of matching dimensions, and $\\left[ \\cdot \\right]_+$ is taking a positive part, which is another notation for ReLU.\n",
        "\n",
        "Note, that there is no nonlinear activation after the last layer in our neural network. **There is an implicit softmax applied while cross entropy loss is calculated by `torch.nn.functional`.**\n",
        "\n",
        "# Automatic gradient\n",
        "\n",
        "The [automatic gradient functionality covered in another workshop](https://github.com/center4ml/Workshops/blob/2023_2/Day_1/1_computational_graph.ipynb) will be used to automatically calculate\n",
        "gradient of\n",
        "$loss \\left(f \\left(x; W_1, W_2, W_3, W_4, b_1, b_2, b_3, b_4 \\right), y_i\\right)$ for the training set $\\left(x_i, y_i \\right)_{i=1, \\ldots, N}$\n",
        "with respect to each component of $W_i$ and $b_i$ tensors.\n",
        "\n",
        "The training set will be batched, but it is just a technical detail.\n",
        "\n",
        "# Loss\n",
        "\n",
        "For the loss function we will use a crossentropy loss directly from PyTorch functional library `torch.nn.functional`. You can read more about this loss [in PyTorch documentation about CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) or generally in [`torch.nn.functional` loss functions section](https://pytorch.org/docs/2.1/nn.functional.html#loss-functions).\n",
        "\n",
        "### Workshop dedicated to loss functions\n",
        "\n",
        "Also, [there is a workshop dedicated to loss functions](https://github.com/center4ml/Workshops/blob/2023_2/Day_1/2_loss_functions.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72",
      "metadata": {
        "id": "e68b3fa0-99e0-490c-ae8c-f2c650c8bf72"
      },
      "source": [
        "### Reading MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
      "metadata": {
        "id": "a3f72963-c85d-49a6-9297-e14f823acc21",
        "outputId": "e955ed09-af5c-44c9-e951-3446600e801e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 31968701.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 7245824.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 37581668.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22815004.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
      "metadata": {
        "id": "4ff41560-0ea3-4d94-a7ad-c96eaf8206b0",
        "outputId": "d08e35cc-0ea3-471f-960f-8d34bfde05f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_image, train_target = trainset[0]    #let us examine the 0-th sample\n",
        "pyplot.imshow(train_image)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
      "metadata": {
        "id": "06195ef1-534d-45fa-99dd-40ae24b55011",
        "outputId": "9b3b111a-f66a-4ccb-b509-c626016a0678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
              "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
              "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
              "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
              "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
              "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
              "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
              "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
              "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
              "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
              "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
              "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "       dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "trainset.data[0]     #it will be shown in two rows, so a human has hard time classificating it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
      "metadata": {
        "id": "9eb5008e-a4b7-4b26-a08a-674988dc8854",
        "outputId": "79d83e42-0ad8-470e-f791-689e06ebb201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "trainset[0][1]    #check if you classified it correctly in your mind"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86",
      "metadata": {
        "id": "5a36d86f-9f32-477b-bd2d-b05eea185f86"
      },
      "source": [
        "\n",
        "### Your task #2\n",
        "\n",
        "Examine a few more samples from mnist dataset. Try to guess the correct class correctly, classifing images with your human classification skills. Try to estimate the accuracy, i.e. what is your percentage of correct classifications - treating a training set that we are examining now as a test set for you. It is sound, because you have not trained on that set before attempting the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28199903-bff6-431b-b855-32d37683ef2b",
      "metadata": {
        "id": "28199903-bff6-431b-b855-32d37683ef2b"
      },
      "source": [
        "\n",
        "### Your task #3\n",
        "\n",
        "Try to convert the dataset into numpy `ndarray`. Then estimate mean and standard deviation of MNIST dataset. Please remember that it is customary to first divide each value in MNIST dataset by 255, to normalize the initial pixel RGB values 0-255 into (0,1) range.\n",
        "\n",
        "*Tips:*\n",
        "- to convert MNIST dataset to numpy, use `trainset.data.numpy()`\n",
        "- in numpy, there are methods `mean()` and `std()` to calculate statistics of a vector."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69",
      "metadata": {
        "id": "982887f9-f016-4e62-9ee0-0e8415f8dc69"
      },
      "source": [
        "Now, we will reread the dataset (**train** and **test** parts) and transform it (standardize it) so it will be zero-mean and unit-std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c16a443-c40d-430f-977b-e6749192950e",
      "metadata": {
        "id": "5c16a443-c40d-430f-977b-e6749192950e"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose(\n",
        "    [ torchvision.transforms.ToTensor(), #Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
        "      torchvision.transforms.Normalize((0.1307), (0.3081))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)   #we do shuffle it to give more randomizations to training epochs\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815882fa-4f93-403a-9221-36bb34649579",
      "metadata": {
        "id": "815882fa-4f93-403a-9221-36bb34649579"
      },
      "source": [
        "Let us visualise the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
      "metadata": {
        "id": "96d91c90-680e-4b1a-9045-3f8709f1bad9",
        "outputId": "53717bea-2268-4c97-a757-8e2ae0e230cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -th batch labels : tensor([2, 2, 1,  ..., 1, 7, 7])\n",
            "1 -th batch labels : tensor([5, 2, 9,  ..., 3, 4, 3])\n",
            "2 -th batch labels : tensor([0, 1, 0,  ..., 4, 0, 3])\n",
            "3 -th batch labels : tensor([7, 0, 8,  ..., 5, 4, 3])\n",
            "4 -th batch labels : tensor([2, 9, 8,  ..., 9, 1, 6])\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "\n",
        "        if i<5:\n",
        "            print(i, \"-th batch labels :\", batch_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2",
      "metadata": {
        "id": "ad9a5923-85fb-47e7-9136-e1fecd4d38d2"
      },
      "source": [
        "\n",
        "### Your taks #4\n",
        "\n",
        "A single label is an entity of order zero (a constant), but batched labels are of order one. The first (and only) index is a sample index within a batch.\n",
        "\n",
        "Your task is to visualise and inspect the number of orders in data in batch_inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b73524-9d07-4882-a2b0-3e29233213a8",
      "metadata": {
        "id": "19b73524-9d07-4882-a2b0-3e29233213a8"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Now, a definition of a simple MLP network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64",
      "metadata": {
        "id": "74fb8859-01d0-4b5e-a3e0-f06e77c72a64"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(   #Sequential is a structure which allows stacking layers one on another in such a way,\n",
        "                                          #that output from a preceding layer serves as input to the next layer\n",
        "            torch.nn.Flatten(),   #change the last three orders in data (with dimensions 1, 28 and 28 respectively) into one order of dimensions (1*28*28)\n",
        "            torch.nn.Linear(1*28*28, 1024),  #which is used as INPUT to the first Linear layer\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(1024, 2048),   #IMPORTANT! Please observe, that the OUTPUT dimension of a preceding layer is always equal to the INPUT dimension of the next layer.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.ReLU(),            #ReLU (or a Sigmoid if you want) is a nonlinear function which is used in-between layers\n",
        "            torch.nn.Linear(256, 10),\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a",
      "metadata": {
        "id": "3fdc9a16-9cd3-40e6-988e-bc783e67833a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training consists of\n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
      "metadata": {
        "id": "58af5c66-1b38-4dec-82c7-44bcd0548d25",
        "outputId": "b4c0416e-e51c-4ed1-d28d-3c590cd8c0e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.2978594303131104\n",
            "epoch: 0 batch: 1 current batch loss: 2.0667903423309326\n",
            "epoch: 0 batch: 2 current batch loss: 1.5915852785110474\n",
            "epoch: 0 batch: 3 current batch loss: 1.1085354089736938\n",
            "epoch: 0 batch: 4 current batch loss: 0.9652660489082336\n",
            "epoch: 0 batch: 5 current batch loss: 1.3754258155822754\n",
            "epoch: 0 batch: 6 current batch loss: 1.1524540185928345\n",
            "epoch: 0 batch: 7 current batch loss: 1.153294563293457\n",
            "epoch: 0 batch: 8 current batch loss: 0.9412615895271301\n",
            "epoch: 0 batch: 9 current batch loss: 0.6012739539146423\n",
            "epoch: 0 batch: 10 current batch loss: 0.6364166140556335\n",
            "epoch: 0 batch: 11 current batch loss: 0.7409091591835022\n",
            "epoch: 0 batch: 12 current batch loss: 0.6830206513404846\n",
            "epoch: 0 batch: 13 current batch loss: 0.6094441413879395\n",
            "epoch: 0 batch: 14 current batch loss: 0.49436619877815247\n",
            "epoch: 0 batch: 15 current batch loss: 0.49619442224502563\n",
            "epoch: 0 batch: 16 current batch loss: 0.48837360739707947\n",
            "epoch: 0 batch: 17 current batch loss: 0.49525371193885803\n",
            "epoch: 0 batch: 18 current batch loss: 0.5098381042480469\n",
            "epoch: 0 batch: 19 current batch loss: 0.40926697850227356\n",
            "epoch: 0 batch: 20 current batch loss: 0.4206010401248932\n",
            "epoch: 0 batch: 21 current batch loss: 0.35406166315078735\n",
            "epoch: 0 batch: 22 current batch loss: 0.33238738775253296\n",
            "epoch: 0 batch: 23 current batch loss: 0.3554477393627167\n",
            "epoch: 0 batch: 24 current batch loss: 0.3305460810661316\n",
            "epoch: 0 batch: 25 current batch loss: 0.3201032876968384\n",
            "epoch: 0 batch: 26 current batch loss: 0.3141852915287018\n",
            "epoch: 0 batch: 27 current batch loss: 0.3462721109390259\n",
            "epoch: 0 batch: 28 current batch loss: 0.2801200747489929\n",
            "epoch: 0 batch: 29 current batch loss: 0.2558145225048065\n",
            "epoch: 1 batch: 0 current batch loss: 0.27320989966392517\n",
            "epoch: 1 batch: 1 current batch loss: 0.25256186723709106\n",
            "epoch: 1 batch: 2 current batch loss: 0.2719610333442688\n",
            "epoch: 1 batch: 3 current batch loss: 0.23332397639751434\n",
            "epoch: 1 batch: 4 current batch loss: 0.2635640799999237\n",
            "epoch: 1 batch: 5 current batch loss: 0.2543705999851227\n",
            "epoch: 1 batch: 6 current batch loss: 0.240604430437088\n",
            "epoch: 1 batch: 7 current batch loss: 0.23735767602920532\n",
            "epoch: 1 batch: 8 current batch loss: 0.23015297949314117\n",
            "epoch: 1 batch: 9 current batch loss: 0.2608100175857544\n",
            "epoch: 1 batch: 10 current batch loss: 0.2438548356294632\n",
            "epoch: 1 batch: 11 current batch loss: 0.23939859867095947\n",
            "epoch: 1 batch: 12 current batch loss: 0.22980986535549164\n",
            "epoch: 1 batch: 13 current batch loss: 0.21988584101200104\n",
            "epoch: 1 batch: 14 current batch loss: 0.2170516550540924\n",
            "epoch: 1 batch: 15 current batch loss: 0.2406655102968216\n",
            "epoch: 1 batch: 16 current batch loss: 0.20260615646839142\n",
            "epoch: 1 batch: 17 current batch loss: 0.2054140567779541\n",
            "epoch: 1 batch: 18 current batch loss: 0.19349505007266998\n",
            "epoch: 1 batch: 19 current batch loss: 0.20949001610279083\n",
            "epoch: 1 batch: 20 current batch loss: 0.17691218852996826\n",
            "epoch: 1 batch: 21 current batch loss: 0.2171507328748703\n",
            "epoch: 1 batch: 22 current batch loss: 0.1861962229013443\n",
            "epoch: 1 batch: 23 current batch loss: 0.16848772764205933\n",
            "epoch: 1 batch: 24 current batch loss: 0.17675644159317017\n",
            "epoch: 1 batch: 25 current batch loss: 0.17697367072105408\n",
            "epoch: 1 batch: 26 current batch loss: 0.17365263402462006\n",
            "epoch: 1 batch: 27 current batch loss: 0.15535320341587067\n",
            "epoch: 1 batch: 28 current batch loss: 0.16390034556388855\n",
            "epoch: 1 batch: 29 current batch loss: 0.16448462009429932\n",
            "epoch: 2 batch: 0 current batch loss: 0.14325223863124847\n",
            "epoch: 2 batch: 1 current batch loss: 0.18514303863048553\n",
            "epoch: 2 batch: 2 current batch loss: 0.15224075317382812\n",
            "epoch: 2 batch: 3 current batch loss: 0.15780873596668243\n",
            "epoch: 2 batch: 4 current batch loss: 0.13638903200626373\n",
            "epoch: 2 batch: 5 current batch loss: 0.15205897390842438\n",
            "epoch: 2 batch: 6 current batch loss: 0.1460932344198227\n",
            "epoch: 2 batch: 7 current batch loss: 0.1308126002550125\n",
            "epoch: 2 batch: 8 current batch loss: 0.1313483864068985\n",
            "epoch: 2 batch: 9 current batch loss: 0.15946891903877258\n",
            "epoch: 2 batch: 10 current batch loss: 0.1210743635892868\n",
            "epoch: 2 batch: 11 current batch loss: 0.15074366331100464\n",
            "epoch: 2 batch: 12 current batch loss: 0.13948865234851837\n",
            "epoch: 2 batch: 13 current batch loss: 0.12220257520675659\n",
            "epoch: 2 batch: 14 current batch loss: 0.1164320856332779\n",
            "epoch: 2 batch: 15 current batch loss: 0.14112411439418793\n",
            "epoch: 2 batch: 16 current batch loss: 0.13344232738018036\n",
            "epoch: 2 batch: 17 current batch loss: 0.14789170026779175\n",
            "epoch: 2 batch: 18 current batch loss: 0.12302953004837036\n",
            "epoch: 2 batch: 19 current batch loss: 0.14217810332775116\n",
            "epoch: 2 batch: 20 current batch loss: 0.13385406136512756\n",
            "epoch: 2 batch: 21 current batch loss: 0.1243913397192955\n",
            "epoch: 2 batch: 22 current batch loss: 0.13671639561653137\n",
            "epoch: 2 batch: 23 current batch loss: 0.13957300782203674\n",
            "epoch: 2 batch: 24 current batch loss: 0.12373287230730057\n",
            "epoch: 2 batch: 25 current batch loss: 0.11420948803424835\n",
            "epoch: 2 batch: 26 current batch loss: 0.1248364970088005\n",
            "epoch: 2 batch: 27 current batch loss: 0.1188231036067009\n",
            "epoch: 2 batch: 28 current batch loss: 0.11870401352643967\n",
            "epoch: 2 batch: 29 current batch loss: 0.14136335253715515\n",
            "epoch: 3 batch: 0 current batch loss: 0.10421105474233627\n",
            "epoch: 3 batch: 1 current batch loss: 0.10548489540815353\n",
            "epoch: 3 batch: 2 current batch loss: 0.1201123595237732\n",
            "epoch: 3 batch: 3 current batch loss: 0.09502778202295303\n",
            "epoch: 3 batch: 4 current batch loss: 0.11036615073680878\n",
            "epoch: 3 batch: 5 current batch loss: 0.09102119505405426\n",
            "epoch: 3 batch: 6 current batch loss: 0.10104580968618393\n",
            "epoch: 3 batch: 7 current batch loss: 0.10392919182777405\n",
            "epoch: 3 batch: 8 current batch loss: 0.0884813666343689\n",
            "epoch: 3 batch: 9 current batch loss: 0.09409232437610626\n",
            "epoch: 3 batch: 10 current batch loss: 0.1066734790802002\n",
            "epoch: 3 batch: 11 current batch loss: 0.08533112704753876\n",
            "epoch: 3 batch: 12 current batch loss: 0.08954215794801712\n",
            "epoch: 3 batch: 13 current batch loss: 0.09244031459093094\n",
            "epoch: 3 batch: 14 current batch loss: 0.09641770273447037\n",
            "epoch: 3 batch: 15 current batch loss: 0.08940780907869339\n",
            "epoch: 3 batch: 16 current batch loss: 0.090217225253582\n",
            "epoch: 3 batch: 17 current batch loss: 0.09128499031066895\n",
            "epoch: 3 batch: 18 current batch loss: 0.10544245690107346\n",
            "epoch: 3 batch: 19 current batch loss: 0.10143040865659714\n",
            "epoch: 3 batch: 20 current batch loss: 0.10280489921569824\n",
            "epoch: 3 batch: 21 current batch loss: 0.11238112300634384\n",
            "epoch: 3 batch: 22 current batch loss: 0.08559810370206833\n",
            "epoch: 3 batch: 23 current batch loss: 0.09309856593608856\n",
            "epoch: 3 batch: 24 current batch loss: 0.0941813588142395\n",
            "epoch: 3 batch: 25 current batch loss: 0.09875496476888657\n",
            "epoch: 3 batch: 26 current batch loss: 0.09597521275281906\n",
            "epoch: 3 batch: 27 current batch loss: 0.08874116837978363\n",
            "epoch: 3 batch: 28 current batch loss: 0.1064412072300911\n",
            "epoch: 3 batch: 29 current batch loss: 0.08254534751176834\n",
            "epoch: 4 batch: 0 current batch loss: 0.0876530185341835\n",
            "epoch: 4 batch: 1 current batch loss: 0.08320921659469604\n",
            "epoch: 4 batch: 2 current batch loss: 0.0797995924949646\n",
            "epoch: 4 batch: 3 current batch loss: 0.08126785606145859\n",
            "epoch: 4 batch: 4 current batch loss: 0.07336566597223282\n",
            "epoch: 4 batch: 5 current batch loss: 0.07081257551908493\n",
            "epoch: 4 batch: 6 current batch loss: 0.0863330140709877\n",
            "epoch: 4 batch: 7 current batch loss: 0.07748424261808395\n",
            "epoch: 4 batch: 8 current batch loss: 0.07349024713039398\n",
            "epoch: 4 batch: 9 current batch loss: 0.07033643871545792\n",
            "epoch: 4 batch: 10 current batch loss: 0.06315978616476059\n",
            "epoch: 4 batch: 11 current batch loss: 0.0700036883354187\n",
            "epoch: 4 batch: 12 current batch loss: 0.07111752778291702\n",
            "epoch: 4 batch: 13 current batch loss: 0.07923389971256256\n",
            "epoch: 4 batch: 14 current batch loss: 0.06476640701293945\n",
            "epoch: 4 batch: 15 current batch loss: 0.06617145240306854\n",
            "epoch: 4 batch: 16 current batch loss: 0.06485997140407562\n",
            "epoch: 4 batch: 17 current batch loss: 0.08444218337535858\n",
            "epoch: 4 batch: 18 current batch loss: 0.0801563635468483\n",
            "epoch: 4 batch: 19 current batch loss: 0.07912987470626831\n",
            "epoch: 4 batch: 20 current batch loss: 0.06604844331741333\n",
            "epoch: 4 batch: 21 current batch loss: 0.06762346625328064\n",
            "epoch: 4 batch: 22 current batch loss: 0.06382840871810913\n",
            "epoch: 4 batch: 23 current batch loss: 0.07321195304393768\n",
            "epoch: 4 batch: 24 current batch loss: 0.0764990821480751\n",
            "epoch: 4 batch: 25 current batch loss: 0.07808718085289001\n",
            "epoch: 4 batch: 26 current batch loss: 0.056176312267780304\n",
            "epoch: 4 batch: 27 current batch loss: 0.08172528445720673\n",
            "epoch: 4 batch: 28 current batch loss: 0.06923516094684601\n",
            "epoch: 4 batch: 29 current batch loss: 0.09937693923711777\n",
            "epoch: 5 batch: 0 current batch loss: 0.07861696928739548\n",
            "epoch: 5 batch: 1 current batch loss: 0.06855959445238113\n",
            "epoch: 5 batch: 2 current batch loss: 0.07725778222084045\n",
            "epoch: 5 batch: 3 current batch loss: 0.0650353729724884\n",
            "epoch: 5 batch: 4 current batch loss: 0.06546379625797272\n",
            "epoch: 5 batch: 5 current batch loss: 0.057947367429733276\n",
            "epoch: 5 batch: 6 current batch loss: 0.06167837604880333\n",
            "epoch: 5 batch: 7 current batch loss: 0.06347673386335373\n",
            "epoch: 5 batch: 8 current batch loss: 0.06931133568286896\n",
            "epoch: 5 batch: 9 current batch loss: 0.06666994094848633\n",
            "epoch: 5 batch: 10 current batch loss: 0.05498524010181427\n",
            "epoch: 5 batch: 11 current batch loss: 0.06726536899805069\n",
            "epoch: 5 batch: 12 current batch loss: 0.0598696731030941\n",
            "epoch: 5 batch: 13 current batch loss: 0.0788225531578064\n",
            "epoch: 5 batch: 14 current batch loss: 0.06431148201227188\n",
            "epoch: 5 batch: 15 current batch loss: 0.0588812492787838\n",
            "epoch: 5 batch: 16 current batch loss: 0.05746619030833244\n",
            "epoch: 5 batch: 17 current batch loss: 0.05948428809642792\n",
            "epoch: 5 batch: 18 current batch loss: 0.05613473430275917\n",
            "epoch: 5 batch: 19 current batch loss: 0.06650377064943314\n",
            "epoch: 5 batch: 20 current batch loss: 0.05597885325551033\n",
            "epoch: 5 batch: 21 current batch loss: 0.06037627160549164\n",
            "epoch: 5 batch: 22 current batch loss: 0.05418168380856514\n",
            "epoch: 5 batch: 23 current batch loss: 0.05633379891514778\n",
            "epoch: 5 batch: 24 current batch loss: 0.06022176519036293\n",
            "epoch: 5 batch: 25 current batch loss: 0.05589946359395981\n",
            "epoch: 5 batch: 26 current batch loss: 0.05917099490761757\n",
            "epoch: 5 batch: 27 current batch loss: 0.0404287725687027\n",
            "epoch: 5 batch: 28 current batch loss: 0.05643758550286293\n",
            "epoch: 5 batch: 29 current batch loss: 0.05118109658360481\n",
            "epoch: 6 batch: 0 current batch loss: 0.05370686575770378\n",
            "epoch: 6 batch: 1 current batch loss: 0.04017259180545807\n",
            "epoch: 6 batch: 2 current batch loss: 0.0467858724296093\n",
            "epoch: 6 batch: 3 current batch loss: 0.04260031133890152\n",
            "epoch: 6 batch: 4 current batch loss: 0.05435992777347565\n",
            "epoch: 6 batch: 5 current batch loss: 0.049116235226392746\n",
            "epoch: 6 batch: 6 current batch loss: 0.04700661823153496\n",
            "epoch: 6 batch: 7 current batch loss: 0.034559737890958786\n",
            "epoch: 6 batch: 8 current batch loss: 0.04889800027012825\n",
            "epoch: 6 batch: 9 current batch loss: 0.04118579998612404\n",
            "epoch: 6 batch: 10 current batch loss: 0.03323415666818619\n",
            "epoch: 6 batch: 11 current batch loss: 0.0472334548830986\n",
            "epoch: 6 batch: 12 current batch loss: 0.04227297008037567\n",
            "epoch: 6 batch: 13 current batch loss: 0.04246161878108978\n",
            "epoch: 6 batch: 14 current batch loss: 0.05572297051548958\n",
            "epoch: 6 batch: 15 current batch loss: 0.05623836815357208\n",
            "epoch: 6 batch: 16 current batch loss: 0.04903097823262215\n",
            "epoch: 6 batch: 17 current batch loss: 0.05667645111680031\n",
            "epoch: 6 batch: 18 current batch loss: 0.0516294464468956\n",
            "epoch: 6 batch: 19 current batch loss: 0.038159213960170746\n",
            "epoch: 6 batch: 20 current batch loss: 0.06209491193294525\n",
            "epoch: 6 batch: 21 current batch loss: 0.040063295513391495\n",
            "epoch: 6 batch: 22 current batch loss: 0.06350036710500717\n",
            "epoch: 6 batch: 23 current batch loss: 0.047119710594415665\n",
            "epoch: 6 batch: 24 current batch loss: 0.057591382414102554\n",
            "epoch: 6 batch: 25 current batch loss: 0.039961569011211395\n",
            "epoch: 6 batch: 26 current batch loss: 0.05477381497621536\n",
            "epoch: 6 batch: 27 current batch loss: 0.05936795845627785\n",
            "epoch: 6 batch: 28 current batch loss: 0.04463043808937073\n",
            "epoch: 6 batch: 29 current batch loss: 0.04753419756889343\n",
            "epoch: 7 batch: 0 current batch loss: 0.0336339958012104\n",
            "epoch: 7 batch: 1 current batch loss: 0.043532803654670715\n",
            "epoch: 7 batch: 2 current batch loss: 0.04114947468042374\n",
            "epoch: 7 batch: 3 current batch loss: 0.04343610629439354\n",
            "epoch: 7 batch: 4 current batch loss: 0.049835529178380966\n",
            "epoch: 7 batch: 5 current batch loss: 0.041298527270555496\n",
            "epoch: 7 batch: 6 current batch loss: 0.04296800494194031\n",
            "epoch: 7 batch: 7 current batch loss: 0.03605891764163971\n",
            "epoch: 7 batch: 8 current batch loss: 0.03969920054078102\n",
            "epoch: 7 batch: 9 current batch loss: 0.03944040462374687\n",
            "epoch: 7 batch: 10 current batch loss: 0.035818275064229965\n",
            "epoch: 7 batch: 11 current batch loss: 0.04392462968826294\n",
            "epoch: 7 batch: 12 current batch loss: 0.054584305733442307\n",
            "epoch: 7 batch: 13 current batch loss: 0.031279489398002625\n",
            "epoch: 7 batch: 14 current batch loss: 0.04264305904507637\n",
            "epoch: 7 batch: 15 current batch loss: 0.039745032787323\n",
            "epoch: 7 batch: 16 current batch loss: 0.04678306728601456\n",
            "epoch: 7 batch: 17 current batch loss: 0.05147683992981911\n",
            "epoch: 7 batch: 18 current batch loss: 0.03832599148154259\n",
            "epoch: 7 batch: 19 current batch loss: 0.03765003755688667\n",
            "epoch: 7 batch: 20 current batch loss: 0.038415826857089996\n",
            "epoch: 7 batch: 21 current batch loss: 0.03334592282772064\n",
            "epoch: 7 batch: 22 current batch loss: 0.03345916420221329\n",
            "epoch: 7 batch: 23 current batch loss: 0.04880905896425247\n",
            "epoch: 7 batch: 24 current batch loss: 0.03352534398436546\n",
            "epoch: 7 batch: 25 current batch loss: 0.03875911980867386\n",
            "epoch: 7 batch: 26 current batch loss: 0.04934539273381233\n",
            "epoch: 7 batch: 27 current batch loss: 0.04329027980566025\n",
            "epoch: 7 batch: 28 current batch loss: 0.0408046580851078\n",
            "epoch: 7 batch: 29 current batch loss: 0.05107223242521286\n"
          ]
        }
      ],
      "source": [
        "net = MLP()\n",
        "optimizer = torch.optim.Adam(net.parameters(), 0.001)   #initial and fixed learning rate of 0.001. We will be using ADAM optimizer throughout the workshop\n",
        "                                                        #different choices are possible, but this is outside the scope of this workshop\n",
        "\n",
        "net.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item())\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "                                ####You can experiment - comment this line and check, that the loss DOES NOT improve, meaning that the network doesn't update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8",
      "metadata": {
        "id": "99335717-7fdf-4335-a8e4-5bd0c30f60e8"
      },
      "source": [
        "\n",
        "### Your task #5\n",
        "\n",
        "Comment the line `optimizer.step()` above. Rerun the above code. Note that the loss is NOT constant as the comment in the code seems to promise, but anyway, the loss doesn't improve, either. Please explain, why the loss is not constant. Please explain, why the loss doesn't improve, either.\n",
        "\n",
        "An epoch is a one full passage through the whole training data. Why then, on the second epoch, the losses are different than in the first epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141",
      "metadata": {
        "id": "6ef0ebc3-16fd-4dfd-b4d2-eac9601a4141"
      },
      "source": [
        "### Training - the second approach\n",
        "\n",
        "\n",
        "Sometimes during training loss stabilizes and doesn't improve anymore. It is not the case here (yet, but we have only run 8 epochs), but a real problem in practice.\n",
        "We can include a new tool called a **scheduler** that would update the *learning rate* in an otpimizer after each epoch. This usually helps the training. Let us reformulate the traning so it consists of\n",
        "- an initiation of a network\n",
        "- a definition of an optimizer. Optimizer does a gradient descent on gradients computed in a `backward()` step on a loss.\n",
        "- a definition of a scheduler to update the learning rate in an optimizer\n",
        "- running through multiple epochs and updating the network weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
      "metadata": {
        "id": "8a9e98b1-a2aa-4af6-b037-0f5e1352dddc",
        "outputId": "eed9661e-0676-4af7-91da-b307c26e614b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 batch: 0 current batch loss: 2.305359363555908 current lr: 0.001\n",
            "epoch: 0 batch: 1 current batch loss: 2.0593483448028564 current lr: 0.001\n",
            "epoch: 0 batch: 2 current batch loss: 1.5960462093353271 current lr: 0.001\n",
            "epoch: 0 batch: 3 current batch loss: 1.0438258647918701 current lr: 0.001\n",
            "epoch: 0 batch: 4 current batch loss: 0.9311957359313965 current lr: 0.001\n",
            "epoch: 0 batch: 5 current batch loss: 1.0869390964508057 current lr: 0.001\n",
            "epoch: 0 batch: 6 current batch loss: 0.9743321537971497 current lr: 0.001\n",
            "epoch: 0 batch: 7 current batch loss: 1.1022498607635498 current lr: 0.001\n",
            "epoch: 0 batch: 8 current batch loss: 0.8468344807624817 current lr: 0.001\n",
            "epoch: 0 batch: 9 current batch loss: 0.736850917339325 current lr: 0.001\n",
            "epoch: 0 batch: 10 current batch loss: 0.6613917350769043 current lr: 0.001\n",
            "epoch: 0 batch: 11 current batch loss: 0.6775449514389038 current lr: 0.001\n",
            "epoch: 0 batch: 12 current batch loss: 0.6167800426483154 current lr: 0.001\n",
            "epoch: 0 batch: 13 current batch loss: 0.5298731327056885 current lr: 0.001\n",
            "epoch: 0 batch: 14 current batch loss: 0.5080510973930359 current lr: 0.001\n",
            "epoch: 0 batch: 15 current batch loss: 0.4747823178768158 current lr: 0.001\n",
            "epoch: 0 batch: 16 current batch loss: 0.49003058671951294 current lr: 0.001\n",
            "epoch: 0 batch: 17 current batch loss: 0.4974406957626343 current lr: 0.001\n",
            "epoch: 0 batch: 18 current batch loss: 0.47067469358444214 current lr: 0.001\n",
            "epoch: 0 batch: 19 current batch loss: 0.40881016850471497 current lr: 0.001\n",
            "epoch: 0 batch: 20 current batch loss: 0.38832736015319824 current lr: 0.001\n",
            "epoch: 0 batch: 21 current batch loss: 0.3446577191352844 current lr: 0.001\n",
            "epoch: 0 batch: 22 current batch loss: 0.36125215888023376 current lr: 0.001\n",
            "epoch: 0 batch: 23 current batch loss: 0.3384031057357788 current lr: 0.001\n",
            "epoch: 0 batch: 24 current batch loss: 0.3475731909275055 current lr: 0.001\n",
            "epoch: 0 batch: 25 current batch loss: 0.31344079971313477 current lr: 0.001\n",
            "epoch: 0 batch: 26 current batch loss: 0.2957768440246582 current lr: 0.001\n",
            "epoch: 0 batch: 27 current batch loss: 0.3009013831615448 current lr: 0.001\n",
            "epoch: 0 batch: 28 current batch loss: 0.3303297460079193 current lr: 0.001\n",
            "epoch: 0 batch: 29 current batch loss: 0.25949326157569885 current lr: 0.001\n",
            "epoch: 1 batch: 0 current batch loss: 0.28399670124053955 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 1 current batch loss: 0.2658379375934601 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 2 current batch loss: 0.2834469676017761 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 3 current batch loss: 0.2565004229545593 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 4 current batch loss: 0.24073059856891632 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 5 current batch loss: 0.2381700873374939 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 6 current batch loss: 0.2525772750377655 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 7 current batch loss: 0.22685635089874268 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 8 current batch loss: 0.20932340621948242 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 9 current batch loss: 0.19288887083530426 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 10 current batch loss: 0.22580327093601227 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 11 current batch loss: 0.24473147094249725 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 12 current batch loss: 0.23200152814388275 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 13 current batch loss: 0.19901853799819946 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 14 current batch loss: 0.2216997593641281 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 15 current batch loss: 0.23764589428901672 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 16 current batch loss: 0.22957031428813934 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 17 current batch loss: 0.21151353418827057 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 18 current batch loss: 0.1773664355278015 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 19 current batch loss: 0.1718462109565735 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 20 current batch loss: 0.22665169835090637 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 21 current batch loss: 0.20217517018318176 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 22 current batch loss: 0.1779704988002777 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 23 current batch loss: 0.1891307234764099 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 24 current batch loss: 0.1787737011909485 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 25 current batch loss: 0.18909989297389984 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 26 current batch loss: 0.17145654559135437 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 27 current batch loss: 0.1796284019947052 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 28 current batch loss: 0.18409226834774017 current lr: 0.0009000000000000001\n",
            "epoch: 1 batch: 29 current batch loss: 0.1728426218032837 current lr: 0.0009000000000000001\n",
            "epoch: 2 batch: 0 current batch loss: 0.15101996064186096 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 1 current batch loss: 0.1544056236743927 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 2 current batch loss: 0.15388549864292145 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 3 current batch loss: 0.15109170973300934 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 4 current batch loss: 0.14400503039360046 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 5 current batch loss: 0.16886427998542786 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 6 current batch loss: 0.15980516374111176 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 7 current batch loss: 0.1431422233581543 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 8 current batch loss: 0.1719263792037964 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 9 current batch loss: 0.1442791372537613 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 10 current batch loss: 0.16278068721294403 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 11 current batch loss: 0.14947667717933655 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 12 current batch loss: 0.1160900890827179 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 13 current batch loss: 0.13152526319026947 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 14 current batch loss: 0.13970395922660828 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 15 current batch loss: 0.158631831407547 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 16 current batch loss: 0.1465776264667511 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 17 current batch loss: 0.13808797299861908 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 18 current batch loss: 0.13938066363334656 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 19 current batch loss: 0.13471557199954987 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 20 current batch loss: 0.11925341188907623 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 21 current batch loss: 0.1474616825580597 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 22 current batch loss: 0.13315193355083466 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 23 current batch loss: 0.1275963932275772 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 24 current batch loss: 0.13071566820144653 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 25 current batch loss: 0.13299831748008728 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 26 current batch loss: 0.13983793556690216 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 27 current batch loss: 0.11487843096256256 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 28 current batch loss: 0.12959761917591095 current lr: 0.0008100000000000001\n",
            "epoch: 2 batch: 29 current batch loss: 0.09578470140695572 current lr: 0.0008100000000000001\n",
            "epoch: 3 batch: 0 current batch loss: 0.12279054522514343 current lr: 0.000729\n",
            "epoch: 3 batch: 1 current batch loss: 0.10554951429367065 current lr: 0.000729\n",
            "epoch: 3 batch: 2 current batch loss: 0.09516670554876328 current lr: 0.000729\n",
            "epoch: 3 batch: 3 current batch loss: 0.11947166174650192 current lr: 0.000729\n",
            "epoch: 3 batch: 4 current batch loss: 0.11772897094488144 current lr: 0.000729\n",
            "epoch: 3 batch: 5 current batch loss: 0.10490065068006516 current lr: 0.000729\n",
            "epoch: 3 batch: 6 current batch loss: 0.11513537168502808 current lr: 0.000729\n",
            "epoch: 3 batch: 7 current batch loss: 0.11066855490207672 current lr: 0.000729\n",
            "epoch: 3 batch: 8 current batch loss: 0.10566949844360352 current lr: 0.000729\n",
            "epoch: 3 batch: 9 current batch loss: 0.1237131655216217 current lr: 0.000729\n",
            "epoch: 3 batch: 10 current batch loss: 0.08965179324150085 current lr: 0.000729\n",
            "epoch: 3 batch: 11 current batch loss: 0.1174093708395958 current lr: 0.000729\n",
            "epoch: 3 batch: 12 current batch loss: 0.09809015691280365 current lr: 0.000729\n",
            "epoch: 3 batch: 13 current batch loss: 0.11477652937173843 current lr: 0.000729\n",
            "epoch: 3 batch: 14 current batch loss: 0.10177876055240631 current lr: 0.000729\n",
            "epoch: 3 batch: 15 current batch loss: 0.10515312105417252 current lr: 0.000729\n",
            "epoch: 3 batch: 16 current batch loss: 0.09582830220460892 current lr: 0.000729\n",
            "epoch: 3 batch: 17 current batch loss: 0.10299341380596161 current lr: 0.000729\n",
            "epoch: 3 batch: 18 current batch loss: 0.12085797637701035 current lr: 0.000729\n",
            "epoch: 3 batch: 19 current batch loss: 0.0966070219874382 current lr: 0.000729\n",
            "epoch: 3 batch: 20 current batch loss: 0.0910029485821724 current lr: 0.000729\n",
            "epoch: 3 batch: 21 current batch loss: 0.09381110221147537 current lr: 0.000729\n",
            "epoch: 3 batch: 22 current batch loss: 0.09623493999242783 current lr: 0.000729\n",
            "epoch: 3 batch: 23 current batch loss: 0.09954775124788284 current lr: 0.000729\n",
            "epoch: 3 batch: 24 current batch loss: 0.09637586772441864 current lr: 0.000729\n",
            "epoch: 3 batch: 25 current batch loss: 0.09414558112621307 current lr: 0.000729\n",
            "epoch: 3 batch: 26 current batch loss: 0.09179617464542389 current lr: 0.000729\n",
            "epoch: 3 batch: 27 current batch loss: 0.09070395678281784 current lr: 0.000729\n",
            "epoch: 3 batch: 28 current batch loss: 0.08621883392333984 current lr: 0.000729\n",
            "epoch: 3 batch: 29 current batch loss: 0.10315808653831482 current lr: 0.000729\n",
            "epoch: 4 batch: 0 current batch loss: 0.07448148727416992 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 1 current batch loss: 0.0900803729891777 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 2 current batch loss: 0.09009688347578049 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 3 current batch loss: 0.08540516346693039 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 4 current batch loss: 0.09211590886116028 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 5 current batch loss: 0.07506229728460312 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 6 current batch loss: 0.09100691229104996 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 7 current batch loss: 0.0846153050661087 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 8 current batch loss: 0.08337313681840897 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 9 current batch loss: 0.0778794139623642 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 10 current batch loss: 0.0829148218035698 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 11 current batch loss: 0.0572323203086853 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 12 current batch loss: 0.08518736064434052 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 13 current batch loss: 0.08104381710290909 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 14 current batch loss: 0.08880447596311569 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 15 current batch loss: 0.07609405368566513 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 16 current batch loss: 0.0669691264629364 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 17 current batch loss: 0.08346179127693176 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 18 current batch loss: 0.09202703833580017 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 19 current batch loss: 0.09638073295354843 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 20 current batch loss: 0.08225559443235397 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 21 current batch loss: 0.06579529494047165 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 22 current batch loss: 0.08316438645124435 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 23 current batch loss: 0.07291576266288757 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 24 current batch loss: 0.0855303704738617 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 25 current batch loss: 0.07714461535215378 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 26 current batch loss: 0.09191731363534927 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 27 current batch loss: 0.07480233162641525 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 28 current batch loss: 0.08767607063055038 current lr: 0.0006561000000000001\n",
            "epoch: 4 batch: 29 current batch loss: 0.05560033395886421 current lr: 0.0006561000000000001\n",
            "epoch: 5 batch: 0 current batch loss: 0.07756052166223526 current lr: 0.00059049\n",
            "epoch: 5 batch: 1 current batch loss: 0.0709572583436966 current lr: 0.00059049\n",
            "epoch: 5 batch: 2 current batch loss: 0.066599041223526 current lr: 0.00059049\n",
            "epoch: 5 batch: 3 current batch loss: 0.06831305474042892 current lr: 0.00059049\n",
            "epoch: 5 batch: 4 current batch loss: 0.08293069154024124 current lr: 0.00059049\n",
            "epoch: 5 batch: 5 current batch loss: 0.06671391427516937 current lr: 0.00059049\n",
            "epoch: 5 batch: 6 current batch loss: 0.06369134783744812 current lr: 0.00059049\n",
            "epoch: 5 batch: 7 current batch loss: 0.07061775028705597 current lr: 0.00059049\n",
            "epoch: 5 batch: 8 current batch loss: 0.056849800050258636 current lr: 0.00059049\n",
            "epoch: 5 batch: 9 current batch loss: 0.06892368942499161 current lr: 0.00059049\n",
            "epoch: 5 batch: 10 current batch loss: 0.07008589059114456 current lr: 0.00059049\n",
            "epoch: 5 batch: 11 current batch loss: 0.06133134290575981 current lr: 0.00059049\n",
            "epoch: 5 batch: 12 current batch loss: 0.05828218162059784 current lr: 0.00059049\n",
            "epoch: 5 batch: 13 current batch loss: 0.06254294514656067 current lr: 0.00059049\n",
            "epoch: 5 batch: 14 current batch loss: 0.06707771122455597 current lr: 0.00059049\n",
            "epoch: 5 batch: 15 current batch loss: 0.05912390351295471 current lr: 0.00059049\n",
            "epoch: 5 batch: 16 current batch loss: 0.0776083841919899 current lr: 0.00059049\n",
            "epoch: 5 batch: 17 current batch loss: 0.06091884523630142 current lr: 0.00059049\n",
            "epoch: 5 batch: 18 current batch loss: 0.07319233566522598 current lr: 0.00059049\n",
            "epoch: 5 batch: 19 current batch loss: 0.0571792796254158 current lr: 0.00059049\n",
            "epoch: 5 batch: 20 current batch loss: 0.06608136743307114 current lr: 0.00059049\n",
            "epoch: 5 batch: 21 current batch loss: 0.06650511175394058 current lr: 0.00059049\n",
            "epoch: 5 batch: 22 current batch loss: 0.08053522557020187 current lr: 0.00059049\n",
            "epoch: 5 batch: 23 current batch loss: 0.044578902423381805 current lr: 0.00059049\n",
            "epoch: 5 batch: 24 current batch loss: 0.0662791058421135 current lr: 0.00059049\n",
            "epoch: 5 batch: 25 current batch loss: 0.06790313869714737 current lr: 0.00059049\n",
            "epoch: 5 batch: 26 current batch loss: 0.07533705979585648 current lr: 0.00059049\n",
            "epoch: 5 batch: 27 current batch loss: 0.0521235316991806 current lr: 0.00059049\n",
            "epoch: 5 batch: 28 current batch loss: 0.05457206070423126 current lr: 0.00059049\n",
            "epoch: 5 batch: 29 current batch loss: 0.056704677641391754 current lr: 0.00059049\n",
            "epoch: 6 batch: 0 current batch loss: 0.06675317138433456 current lr: 0.000531441\n",
            "epoch: 6 batch: 1 current batch loss: 0.049577660858631134 current lr: 0.000531441\n",
            "epoch: 6 batch: 2 current batch loss: 0.05231352150440216 current lr: 0.000531441\n",
            "epoch: 6 batch: 3 current batch loss: 0.054753147065639496 current lr: 0.000531441\n",
            "epoch: 6 batch: 4 current batch loss: 0.053107231855392456 current lr: 0.000531441\n",
            "epoch: 6 batch: 5 current batch loss: 0.06429329514503479 current lr: 0.000531441\n",
            "epoch: 6 batch: 6 current batch loss: 0.062061160802841187 current lr: 0.000531441\n",
            "epoch: 6 batch: 7 current batch loss: 0.06310412287712097 current lr: 0.000531441\n",
            "epoch: 6 batch: 8 current batch loss: 0.06451386958360672 current lr: 0.000531441\n",
            "epoch: 6 batch: 9 current batch loss: 0.05142609030008316 current lr: 0.000531441\n",
            "epoch: 6 batch: 10 current batch loss: 0.04680223390460014 current lr: 0.000531441\n",
            "epoch: 6 batch: 11 current batch loss: 0.06536996364593506 current lr: 0.000531441\n",
            "epoch: 6 batch: 12 current batch loss: 0.04798263683915138 current lr: 0.000531441\n",
            "epoch: 6 batch: 13 current batch loss: 0.05045553669333458 current lr: 0.000531441\n",
            "epoch: 6 batch: 14 current batch loss: 0.04764553904533386 current lr: 0.000531441\n",
            "epoch: 6 batch: 15 current batch loss: 0.050501175224781036 current lr: 0.000531441\n",
            "epoch: 6 batch: 16 current batch loss: 0.06554920971393585 current lr: 0.000531441\n",
            "epoch: 6 batch: 17 current batch loss: 0.05094166100025177 current lr: 0.000531441\n",
            "epoch: 6 batch: 18 current batch loss: 0.048954032361507416 current lr: 0.000531441\n",
            "epoch: 6 batch: 19 current batch loss: 0.05422486737370491 current lr: 0.000531441\n",
            "epoch: 6 batch: 20 current batch loss: 0.057938847690820694 current lr: 0.000531441\n",
            "epoch: 6 batch: 21 current batch loss: 0.0640096515417099 current lr: 0.000531441\n",
            "epoch: 6 batch: 22 current batch loss: 0.06030842289328575 current lr: 0.000531441\n",
            "epoch: 6 batch: 23 current batch loss: 0.05425107106566429 current lr: 0.000531441\n",
            "epoch: 6 batch: 24 current batch loss: 0.05068466067314148 current lr: 0.000531441\n",
            "epoch: 6 batch: 25 current batch loss: 0.05491257458925247 current lr: 0.000531441\n",
            "epoch: 6 batch: 26 current batch loss: 0.0508275106549263 current lr: 0.000531441\n",
            "epoch: 6 batch: 27 current batch loss: 0.048370230942964554 current lr: 0.000531441\n",
            "epoch: 6 batch: 28 current batch loss: 0.05882437154650688 current lr: 0.000531441\n",
            "epoch: 6 batch: 29 current batch loss: 0.037077538669109344 current lr: 0.000531441\n",
            "epoch: 7 batch: 0 current batch loss: 0.05214683338999748 current lr: 0.0004782969\n",
            "epoch: 7 batch: 1 current batch loss: 0.047076404094696045 current lr: 0.0004782969\n",
            "epoch: 7 batch: 2 current batch loss: 0.043357621878385544 current lr: 0.0004782969\n",
            "epoch: 7 batch: 3 current batch loss: 0.04505261033773422 current lr: 0.0004782969\n",
            "epoch: 7 batch: 4 current batch loss: 0.042064376175403595 current lr: 0.0004782969\n",
            "epoch: 7 batch: 5 current batch loss: 0.03950951620936394 current lr: 0.0004782969\n",
            "epoch: 7 batch: 6 current batch loss: 0.05640779808163643 current lr: 0.0004782969\n",
            "epoch: 7 batch: 7 current batch loss: 0.04198002442717552 current lr: 0.0004782969\n",
            "epoch: 7 batch: 8 current batch loss: 0.057652294635772705 current lr: 0.0004782969\n",
            "epoch: 7 batch: 9 current batch loss: 0.0446031354367733 current lr: 0.0004782969\n",
            "epoch: 7 batch: 10 current batch loss: 0.037363506853580475 current lr: 0.0004782969\n",
            "epoch: 7 batch: 11 current batch loss: 0.051859062165021896 current lr: 0.0004782969\n",
            "epoch: 7 batch: 12 current batch loss: 0.039133988320827484 current lr: 0.0004782969\n",
            "epoch: 7 batch: 13 current batch loss: 0.0481988862156868 current lr: 0.0004782969\n",
            "epoch: 7 batch: 14 current batch loss: 0.03781586512923241 current lr: 0.0004782969\n",
            "epoch: 7 batch: 15 current batch loss: 0.04153156653046608 current lr: 0.0004782969\n",
            "epoch: 7 batch: 16 current batch loss: 0.04073261469602585 current lr: 0.0004782969\n",
            "epoch: 7 batch: 17 current batch loss: 0.03950086236000061 current lr: 0.0004782969\n",
            "epoch: 7 batch: 18 current batch loss: 0.04002292826771736 current lr: 0.0004782969\n",
            "epoch: 7 batch: 19 current batch loss: 0.05545656755566597 current lr: 0.0004782969\n",
            "epoch: 7 batch: 20 current batch loss: 0.03665895760059357 current lr: 0.0004782969\n",
            "epoch: 7 batch: 21 current batch loss: 0.0443432480096817 current lr: 0.0004782969\n",
            "epoch: 7 batch: 22 current batch loss: 0.05045507848262787 current lr: 0.0004782969\n",
            "epoch: 7 batch: 23 current batch loss: 0.04375423118472099 current lr: 0.0004782969\n",
            "epoch: 7 batch: 24 current batch loss: 0.03548282012343407 current lr: 0.0004782969\n",
            "epoch: 7 batch: 25 current batch loss: 0.054957251995801926 current lr: 0.0004782969\n",
            "epoch: 7 batch: 26 current batch loss: 0.0448148138821125 current lr: 0.0004782969\n",
            "epoch: 7 batch: 27 current batch loss: 0.052313368767499924 current lr: 0.0004782969\n",
            "epoch: 7 batch: 28 current batch loss: 0.04786394536495209 current lr: 0.0004782969\n",
            "epoch: 7 batch: 29 current batch loss: 0.039097584784030914 current lr: 0.0004782969\n"
          ]
        }
      ],
      "source": [
        "net_with_scheduler = MLP()\n",
        "optimizer = torch.optim.Adam(net_with_scheduler.parameters(), 0.001)   #initial learning rate of 0.001.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)    #updates the learning rate after each epoch. There are many ways to do that: StepLR multiplies learning rate by gamma\n",
        "\n",
        "net_with_scheduler.train()    #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing traning\n",
        "for epoch in range(8):  #  an epoch is a training run through the whole data set\n",
        "\n",
        "    loss = 0.0\n",
        "    for batch, data in enumerate(trainloader):\n",
        "        batch_inputs, batch_labels = data\n",
        "        #batch_inputs.squeeze(1)     #alternatively if not for a Flatten layer, squeeze() could be used to remove the second order of the tensor, the Channel, which is one-dimensional (this index can be equal to 0 only)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_outputs = net_with_scheduler(batch_inputs)   #this line calls the forward(self, x) method of the MLP object. Please note, that the last layer of the MLP is linear\n",
        "                                            #and MLP doesn't apply\n",
        "                                            #the nonlinear activation after the last layer\n",
        "        loss = torch.nn.functional.cross_entropy(batch_outputs, batch_labels, reduction = \"mean\") #instead, nonlinear softmax is applied internally in THIS loss function\n",
        "\n",
        "        print(\"epoch:\", epoch, \"batch:\", batch, \"current batch loss:\", loss.item(), \"current lr:\", scheduler.get_last_lr()[0])\n",
        "        loss.backward()       #this computes gradients as we have seen in previous workshops\n",
        "        optimizer.step()     #but this line in fact updates our neural network.\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592",
      "metadata": {
        "id": "6aa1db9e-263a-421e-a23f-1b6918fd4592"
      },
      "source": [
        "\n",
        "### Your task #6\n",
        "\n",
        "Well, it seems that we were able to get the learning to levels of 0.03 even without a scheduler. Can you bring it under 0.02? Can you keep it under 0.02? The scheduler didn't help much. Maybe the proposed gamma was to low (0.9 only)? Please experiment with different settings for the optimizer learning rate and different scheduler settings. There are other schedulers you can experiment with, too. Please verify what would happen if the nets were allowed to train for more training epochs.\n",
        "\n",
        "**Some other schedulers you might want to experiment with:**\n",
        "- Exponential LR - decays the learning rate of each parameter group by gamma every epoch - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html)\n",
        "- Step LR - more general then the Exponential LR: decays the learning rate of each parameter group by gamma every step_size epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)\n",
        "- Cosine Annealing LR - learning rate follows the first quarter of the cosine curve - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
        "- Cosine with Warm Restarts - learning rate follows the first quarter of the cosine curve and restarts after a predefined number of epochs - [link to documentation](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html)\n",
        "\n",
        "\n",
        "### Your task #7\n",
        "\n",
        "Please explain, what are the dangers of bringing the loss too low? What is an *overtrained* neural network? How can one prevent it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f",
      "metadata": {
        "id": "ad5ae403-3ea0-4c28-896b-2cb5ec67492f"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Now we will test those two nets - the one without and the one with the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
      "metadata": {
        "id": "8ba700ae-97ca-41fa-8105-89980c5c9406",
        "outputId": "88478116-640f-4e73-cb28-435477b11568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9813\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net.eval()              #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():   #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)    #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
      "metadata": {
        "id": "2d35d8ad-e858-40d5-8702-20db94f56879",
        "outputId": "e93bc701-ccae-4752-a604-9da11c21b9ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  0.9797\n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "wrong = 0\n",
        "\n",
        "net_with_scheduler.eval()   #it notifies the network layers (especially batchnorm or dropout layers, which we don't use in this example) that we are doing evaluation\n",
        "with torch.no_grad():       #it prevents that the net learns during evalution. The gradients are not computed, so this makes it faster, too\n",
        "    for batch, data in enumerate(testloader): #batches in test are of size 1\n",
        "\n",
        "        datapoint, label = data\n",
        "\n",
        "        prediction = net_with_scheduler(datapoint)                  #prediction has values representing the \"prevalence\" of the corresponding class\n",
        "        classification = torch.argmax(prediction)                   #the class is the index of maximal \"prevalence\"\n",
        "\n",
        "        if classification.item() == label.item():\n",
        "            good += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"accuracy = \", good/(good+wrong))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4831d2c-ee3f-42be-969b-627d888692c8",
      "metadata": {
        "id": "e4831d2c-ee3f-42be-969b-627d888692c8"
      },
      "source": [
        "Well, not bad. Now it is your turn to experiment - change the number and sizes of layers in out neural networks, change the activation function, play with the learning rate and the optimizer and the scheduler.\n",
        "\n",
        "\n",
        "# Moving computations to GPU\n",
        "\n",
        "In the code above we didn't move the computations to GPU. The first task is moving the computations to GPU. How do you do that?\n",
        "\n",
        "Ability to compute on GPU is called CUDA:  Compute Unified Device Architecture.\n",
        "\n",
        "First, we check if CUDA is available:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "ehuPd4dXGRkO",
        "outputId": "b10a29a9-def9-48c0-e4de-2f21c7d28040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ehuPd4dXGRkO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to make CUDA available if it is not?\n",
        "\n",
        "* On a local machine you must connect and properly configure a GPU card, it is out of scope of this workshop.\n",
        "\n",
        "* In Google Colab online, you click `Runtime` > `Change runtime` and change `Hardware acceleration` setting to `GPU`.\n",
        "\n",
        "* In Google Colab **Polish version**, you click `Środowisko wykonawcze` > `Zmień typ środowiska wykonawczego` and change `Akcelerator sprzętowy` setting to `GPU`.\n",
        "\n",
        "Once we have CUDA (or not: this case should be handled in the code, too), we are ready to determine the computation device. Note that it is just a character string:\n"
      ],
      "metadata": {
        "id": "bKCTsxL2GXva"
      },
      "id": "bKCTsxL2GXva"
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "HSvyKWniGc4V",
        "outputId": "80e52f82-1b63-4c29-c9a0-75d74d0ab67b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HSvyKWniGc4V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we move our neural network model and each tensor with batch data to GPU. We can do it by passing the `device = device` argument during the model or tensor construction, or by calling a `to(device)` method on a model or a tensor:"
      ],
      "metadata": {
        "id": "EZaZF0NJHIRL"
      },
      "id": "EZaZF0NJHIRL"
    },
    {
      "cell_type": "code",
      "source": [
        "layer = torch.nn.Linear(2048, 256, device = device),\n",
        "layer = torch.nn.Linear(2048, 256).to(device)"
      ],
      "metadata": {
        "id": "TPa7YYZWHMI0"
      },
      "id": "TPa7YYZWHMI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your task #8\n",
        "\n",
        "Enable CUDA computations in all the code above:\n",
        "\n",
        "1. Check if CUDA is available before the MLP definition\n",
        "2. Add the device field to the constructor and pass it to all constructed layers\n",
        "3. Add the device argument when constructing MLP\n",
        "4. Add the device arguments to all tensors that get passed to MLP"
      ],
      "metadata": {
        "id": "xMovK3L5HMx6"
      },
      "id": "xMovK3L5HMx6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desequencing forward computations\n",
        "\n",
        "In the MLP definition above we used a sequential layer. In order to demonstrate how in general one can perform arbitrary forward computations, we will remove the sequential layer now.\n",
        "\n",
        "### Your task #9\n",
        "\n",
        "Remove the sequential layer\n",
        "\n",
        "1. Remove the sequential layer by using separate variables for the layers within.\n",
        "2. Change the forward flow to pass `x` through all layers."
      ],
      "metadata": {
        "id": "S6a6gAQU6h_z"
      },
      "id": "S6a6gAQU6h_z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 What next\n",
        "\n",
        "First and foremost - **save your updated Colab notebook into GDrive**, so you can work with it later.\n",
        "\n",
        "Then - experiment yourself. Try to add a scheduler into LeNet training. Observe that in the final epochs the loss didn't improve much and that the final accuracy was very mediocre. It should supercede than of the MLP network!\n",
        "\n",
        "There are also aspects of ML we didn't even touch:\n",
        "\n",
        "1. Validation on an additional validation set, or cross-validation\n",
        "2. Initializing the network\n",
        "3. Choice of a scheduler (we used exclusively Adam)\n",
        "4. Choice of a batch size and accompanying learning rate\n",
        "5. Choice of a specific loss function for very specific tasks\n",
        "\n",
        "Those concepts constitute a broad and separate subject, each of them. Explore them and have fun!"
      ],
      "metadata": {
        "id": "W6Haobvk4N1J"
      },
      "id": "W6Haobvk4N1J"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}