{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258f4ec1-8dfe-4031-998f-15b8bf342b50",
   "metadata": {},
   "source": [
    "# Machine learning introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666a0e3-8463-48e0-b185-97572de7d9d8",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28b97c-a0a5-4311-a5c5-f8de44c4b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e919e5-710c-4d3c-9b5c-94c6ab7b0d43",
   "metadata": {},
   "source": [
    "## Machine Learning in a nutshell\n",
    "\n",
    "Any Machine Learning (ML) model makes an atempt to contruct a function from\n",
    "the `n` dimensioanal input space (`features`) to the ``m`` dimensional output space (`targets` or `labels`):\n",
    "\n",
    "$$\\Huge{\n",
    "R^{n} \\rightarrow R^{m}\n",
    "}\n",
    "$$\n",
    "\n",
    "There are two main distinctions of ML models basing on the type ot the output space:\n",
    "\n",
    "* according to the type of the output space:\n",
    "    * **regression** - the output space is the ussual $R^{m}$\n",
    "    * **categorisation** - the output space is a discrete set of possibilities - ``categories``. The model is epected to provide probability that example ``x`` belong to class ``y``.  \n",
    "* according to the avalaibility of the data in the output space - ``labels``:\n",
    "    * **supervised learning** - a full set of input: ``X``, and output: ``Y`` values is available: ``data=(X,Y)``\n",
    "    * **unsupervised learing** - only the input data: ``X`` is avalable. ``Y`` values are usually unknown: ``data=(X)``\n",
    "\n",
    "Here capital X, Y denote a set of feature, or label values.A single ``example`` from the full dataset will be denoted with small letterrs: \n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "(x,y) \\in (X,Y)\n",
    "}\n",
    "$$\n",
    "\n",
    "The $X \\rightarrow Y$  correspondence is ussually not deterministic (mostly due to lack of our knowlwdge, e,g, ``X`` does not cover all the variables controlling the ``Y`` value) the ML models provide some probabilistic estimates, like expectation value:\n",
    "\n",
    "$$\n",
    "\\Huge{\n",
    "   f(x) = \\int_{Y} y \\cdot p(x,y) dy = <y(x)>\n",
    "}\n",
    "$$\n",
    "where $p(x,y)$ is a joint probability ditribution for (x,y) pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bbb43-a747-4d5b-a783-03464baf5406",
   "metadata": {},
   "source": [
    "## Linear model - an archetypic ML model.\n",
    "\n",
    "We will play with a simple, thereore easy to understand linar model:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "x = (x_{1}, ...,x_{m}), y = (y) \\\\\n",
    "f(x) = \\theta_{0} + \\theta_{i} \\cdot x_{i}\n",
    "}\n",
    "$$\n",
    "\n",
    "This example will allow us to introduce basic ML concepts of `loss function` and `gradient decent`.\n",
    "We will also learn basic programing techniques using numpy arrays parrarel manipulations.\n",
    "\n",
    "We will use a rank 1 ```x``` and ```y``` arrays: a single dimensional case. We will:\n",
    "\n",
    "1) draw a random set X\n",
    "2) draw a random set Y (y values for every x value). The Y will have a random noise ($\\mathbf{\\epsilon}$), around the ideal case:\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    "y = \\theta_{0} + \\theta_{1} \\cdot x + \\epsilon \\\\\n",
    "}\n",
    "$$\n",
    "3) define a linear model:\n",
    "$$\n",
    "\\huge{\n",
    "f(x) = \\theta_{0} + \\theta_{1} \\cdot x\n",
    "}\n",
    "$$\n",
    "4) perform a ``training``\n",
    "5) estimate the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f1e49-c20d-424d-b0b1-cb912aedc99b",
   "metadata": {},
   "source": [
    "### Generate the X set\n",
    "\n",
    "Here the ``X`` is just a set of random values in one dimention. This meas we have one feature, `n=1`, and `nPoints` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857cf43-d041-4e7d-929a-9354c2282f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "minX = 0\n",
    "maxX = 1\n",
    "nPoints = 5\n",
    "x = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "\n",
    "print(colored(\"Shape of the X array:\",\"blue\"),x.shape)\n",
    "\n",
    "#it will be convenient to represent X as a an array: nPoints rows, one column\n",
    "x = np.reshape(x, (-1,1))\n",
    "print(colored(\"Shape of the X array:\",\"blue\"),x.shape)\n",
    "print(colored(\"First ten examples:\\n\",\"blue\"),x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822ebbf-a27c-419d-8d7d-8884e9cb4ccf",
   "metadata": {},
   "source": [
    "### Generate Y according to given ```true``` formula:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y = \\theta_{0} + \\theta_{1} \\cdot x \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00deeb9-9c85-408f-94e0-7321ba137dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define coefficients as a array\n",
    "theta = (1,3)\n",
    "\n",
    "# calculate the y values before smearing\n",
    "y = theta[0] + theta[1]*x\n",
    "print(colored(\"First few Y values:\\n\",\"blue\"), y[:10])\n",
    "\n",
    "# calculate the y values using a more compact formula\n",
    "# substitute each X value by a pair (1,X):\n",
    "ones = np.ones(x.shape[0])\n",
    "x = np.column_stack((ones, x))\n",
    "\n",
    "print(colored(\"Extended X shape:\\n\",\"blue\"),x.shape)\n",
    "print(colored(\"First few X values:\\n\",\"blue\"),x[:10])\n",
    "\n",
    "#write linear formula as a element-wise multiplication,\n",
    "#followed by sum of columns: along axis=1 direction\n",
    "y = np.sum(theta*x, axis=1)\n",
    "print(colored(\"First few Y values:\\n\",\"blue\"), y[:10])\n",
    "\n",
    "#change y to a rank-2 array:\n",
    "#here a row is particular example\n",
    "#column - is a target at this example\n",
    "y = np.reshape(y,(-1,1))\n",
    "print(colored(\"First few Y values:\\n\",\"blue\"), y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f43147-d569-4d21-b722-6b0ddfacf18c",
   "metadata": {},
   "source": [
    "### Training: a loss function\n",
    "\n",
    "We find the estimates of out model parameters: $a = (a_{0}, a_{1})$ at those which provide a model ``f(X)`` that minimizes a **loss function:**\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y)\n",
    "}\n",
    "$$\n",
    "\n",
    "Most polular loss functions:\n",
    "\n",
    "* **regression taks:** mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y) = \\frac{1}{N} \\sum_{X} (f(x) - y)^{2}\n",
    "}\n",
    "$$\n",
    "\n",
    "where `N` in the size of the dataset (X,Y). We **never** use all the data points at hand here! ``X`` is a subsable - a **training** set of the whole data we have. Unless we can have infinie amount of data -as in our case, where we always can generate more data.\n",
    "\n",
    "Most popular loss functions:\n",
    "\n",
    "* **categorisation taks:** cross entropy:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y) = -\\frac{1}{N} \\sum_{X} \\log f_{correct~class}(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "If one observes that:\n",
    "\n",
    "* model provides probabilty that example belongs to class ``i``: $f(x) = (p_{0}(x), p_{1}(x),...p_{m}(x))$ (note here f(x) is a vector, with rank equal to number of classes). We select only the componenct corresponding to correct class.\n",
    "\n",
    "* sum over all examples from given class, divided by ``N`` gives the ``true`` probability that random example belong to class ``i``:\n",
    "$$\n",
    "\\large{\n",
    "-\\frac{1}{N} \\sum_{X_{particular~class}} \\log f_{this~class}(x) = \\\\\n",
    "-\\log f_{this~class}(x) \\sum_{X_{particular~class}} 1 = \\\\\n",
    "-\\log f_{this~class}(x) \\frac{N_{particular~class}}{N} = \\\\\n",
    "-\\log f_{this~class}(x) p_{this~class}\n",
    "}\n",
    "$$\n",
    "\n",
    "the cross entropy can be written in most popular form (i - denotes correct class index):\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "L(f(X), Y) = -\\sum_{X} p_{i,true}\\log p_{i,model}(x)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901821a-7798-4a1d-8c4f-b2f26839732a",
   "metadata": {},
   "source": [
    "Let's calculate the loss function on our data (without the noise) for some random values of model parameters.\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* add a block of code that will check the loss function value with model parameters the same as used for sample generation.\n",
    "What is the expected result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86e7d5-80a8-40fb-8aed-217e64d2ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_theta = 0\n",
    "max_theta = 3\n",
    "nParams = 2\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "print(colored(\"Some random parameters are:\",\"blue\"),theta_model)\n",
    "\n",
    "#calculate the model result on the data\n",
    "#use keepdims=1 to get correct output shape\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "\n",
    "#calculate the loss function\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Loss function with random parameters:\",\"blue\"), loss)\n",
    "\n",
    "#BEGIN_SOLUTION\n",
    "y_model = np.sum(theta*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Loss function with nominal parameters:\",\"blue\"), loss)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10104465-a66d-4560-8a6b-5015ec6225e7",
   "metadata": {},
   "source": [
    "### Training: the gradient descent\n",
    "\n",
    "How the parameters of the model are found? \n",
    "\n",
    "`We calculate gradient vector of the loss function wrt. the parameters, and go along the gradient to the minimim of the loss function.`\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1) define starting values for $\\theta$ parameters: `init_theta`\n",
    "2) select a subsample of examples to be used for the loss function calculation: a `batch`\n",
    "3) calculate the loss function gradient. In our case this is:\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    "\\nabla_{\\theta} L = \\nabla_{\\theta} \\frac{1}{N} \\sum_{X} (\\theta^{T} \\cdot x - y)^{2} = \\\\\n",
    "\\frac{2}{N} \\sum_{X} (\\theta^{T} \\cdot x - y)x\n",
    "}\n",
    "$$\n",
    "\n",
    "4) update the $\\theta$ parameter proportionally to the gradient (proportionality parameter $\\alpha)$:\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    " \\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla_{\\theta} L\n",
    "}\n",
    "$$\n",
    "5) loop with batches thorugh the whole dataset\n",
    "5) repeat reading the whole datasert many times. One pass through the whole dataset is called an `epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5cb500-5c52-4c54-94a3-4623d660ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descend(x_batch, y_batch, init_theta, alpha, verbose=False):\n",
    "    \n",
    "    theta = init_theta \n",
    "    #print(colored(\"theta value:\\n\",\"blue\"),theta)\n",
    "    #model response\n",
    "    y_model = np.sum(theta*x_batch, axis=1, keepdims=True)\n",
    "    if verbose:\n",
    "        print(colored(\"model response:\\n\",\"blue\"),y_model[:10])\n",
    "    #calculate the update value\n",
    "    delta = -alpha*(y_model - y_batch)*x_batch\n",
    "    if verbose:\n",
    "        print(colored(\"delta for each example:\\n\",\"blue\"),delta[:10])\n",
    "    #sum the update value over the examples provides\n",
    "    delta = np.mean(delta, axis=0)\n",
    "    if verbose:\n",
    "        print(colored(\"delta averaged over examples:\\n\",\"blue\"),delta)\n",
    "    #apply the update to theta\n",
    "    theta = theta + delta\n",
    "    return theta \n",
    "\n",
    "\n",
    "#test the function on small batch\n",
    "x_batch = x[:3]\n",
    "y_batch = y[:3,]\n",
    "print(colored(\"X test:\\n\",\"blue\"),x_batch)\n",
    "print(colored(\"Y test:\\n\",\"blue\"),y_batch)\n",
    "\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "alpha = 1\n",
    "minDelta = 1E-3\n",
    "batch_gradient_descend(x_batch, y_batch, theta_model, alpha, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c0f6c-fa20-4042-ae9e-01f8c0bbf247",
   "metadata": {},
   "source": [
    "**Please:**\n",
    "* provide input parameters to the `batch_gradient_descend()` for which you can easily (=without any calculations!) evalue the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e1f53-6093-4718-9780-d06e31f4b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEGIN_SOLUTION\n",
    "batch_gradient_descend(x_batch, y_batch, theta, alpha, True)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713f3cc-68a9-4736-8ffc-75aeb4f1771f",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "* define loop with `batches` over the dataset: `epoch`\n",
    "* calculate the loss function for each epoch\n",
    "\n",
    "For simplicity we will take the whole dataset as a single batch. Ussualy batch size is a comprimise between memory capacity, and number of parameters updates we want to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f116d9-e261-405b-a4b7-7336dc76596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Theta value before training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with initial parameters:\",\"blue\"), loss)\n",
    "\n",
    "alpha = 0.1\n",
    "nEpochs = 10\n",
    "for iEpoch in range(nEpochs):\n",
    "    x_batch = x\n",
    "    y_batch = y\n",
    "    theta_model = batch_gradient_descend(x_batch, y_batch, theta_model, alpha)\n",
    "    \n",
    "    y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "    loss = np.mean((y_model - y)**2)\n",
    "    #print(colored(\"Epoch:\",\"blue\"), iEpoch, colored(\"loss:\",\"blue\"),loss)\n",
    "    \n",
    "print(colored(\"Theta value after training:\",\"blue\"),theta_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58e98a-2476-4b93-bdfd-380c5f8fd48e",
   "metadata": {},
   "source": [
    "## Model performance on training data\n",
    "\n",
    "* we plot the model results and investigate by eye how well it works on the `training` data.\n",
    "  Unortunately in real application eye inspection is ussually not possible as data is multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773573e-6d3c-43f1-8a4f-088720c1bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#plot the data points\n",
    "ax.plot(x[:,1],y, \"bo\", label=\"training data\")\n",
    "\n",
    "#plot the fitted line\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "ax.plot(x[:,1],y_model, \"ro\", label=\"model result\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7776d1-1242-4b88-9378-ce3d7a6b5a78",
   "metadata": {},
   "source": [
    "## Model performance on test data\n",
    "\n",
    "* we investigate how the model works for data not used for training - the `test` dataset. The performance estimate has to be always made on a separate sample - the `test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e68ce-9c8a-44ec-84f9-827c56da8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#create the test dataset\n",
    "nPoints = 15\n",
    "x_test = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "x_test = np.reshape(x_test, (-1,1))\n",
    "ones = np.ones(x_test.shape[0])\n",
    "x_test = np.column_stack((ones, x_test))\n",
    "y_test = np.sum(theta*x_test, axis=1)\n",
    "\n",
    "#plot the data points\n",
    "ax.plot(x_test[:,1],y_test, \"bo\", label=\"test data\")\n",
    "\n",
    "#plot the fitted line\n",
    "y_model = np.sum(theta_model*x_test, axis=1, keepdims=True)\n",
    "ax.plot(x_test[:,1],y_model, \"ro\", label=\"model result\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a983bbb-6c2c-4d18-96e8-f1e485c3b5d8",
   "metadata": {},
   "source": [
    "## Improve the model performance - run a lorger training.\n",
    "\n",
    "Our task is a very simple, but the model does not work very well - this is because we have made not enough model parameter update iterations.\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* run the training for more iterations, until you will be satified with the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c56607-2331-4397-abb9-a8533349a26f",
   "metadata": {},
   "source": [
    "## Run the training on noisy data\n",
    "\n",
    "So far the model performance was the same on training and test data. This was due to the fact there was no statistical fluctuations between the two datasets.\n",
    "Now we will use a noisy dataset for training to see the effect of fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2d6b5-bd5d-4d12-a61b-ddb960cb4e0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate data  with Gausian noise added to the ```pure``` formula:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y =  \\epsilon \\\\\n",
    "\\epsilon = Normal(\\mu=y, \\sigma=1)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1059ec-b354-4278-bcec-dfe7e680e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "nPoints = 10000\n",
    "x = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "\n",
    "ones = np.ones(x.shape[0])\n",
    "x = np.column_stack((ones, x))\n",
    "\n",
    "y = np.sum(theta*x, axis=1).reshape((-1,1))\n",
    "\n",
    "#define the gaussian smearing parameters\n",
    "mu = y\n",
    "sigma = 2.0\n",
    "\n",
    "#generate the data \n",
    "y_with_noise = np.random.default_rng().normal(mu, sigma)\n",
    "\n",
    "print(colored(\"Shape of the Y array:\",\"blue\"),y_with_noise.shape)\n",
    "print(colored(\"First few Y values with noise:\\n\",\"blue\"), y_with_noise[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c467d0-8908-4256-8ad7-3de9fd827ced",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make a control plot of noise data\n",
    "\n",
    "Plot a histogram of `Y_noise - Y`. Check if it looks at expected.\n",
    "\n",
    "**Please**\n",
    "\n",
    "* generate more points if the histogram does not look good\n",
    "* go back to 5 points for futher exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cff4cd-4b36-4a9a-8db2-fb0e25d15911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#control the size of the plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# plot a histogram of fidderence between noisy and clean values\n",
    "ax.hist(y_with_noise-y, bins=20, density=True, label=r\"$y_{noise} - y$\");\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827278bc-da53-4fa7-8efb-411c10c39282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "theta_model = np.random.default_rng().uniform(min_theta, max_theta, nParams)\n",
    "\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y_with_noise)**2)\n",
    "print(colored(\"Theta value before training:\",\"blue\"),theta_model)\n",
    "print(colored(\"Loss function with initial parameters:\",\"blue\"), loss)\n",
    "\n",
    "alpha = 0.01\n",
    "nEpochs = 10000\n",
    "for iEpoch in range(nEpochs):\n",
    "    x_batch = x\n",
    "    y_batch = y_with_noise\n",
    "    theta_model = batch_gradient_descend(x_batch, y_batch, theta_model, alpha)\n",
    "    \n",
    "    y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "    loss = np.mean((y_model - y_with_noise)**2)\n",
    "    #print(colored(\"Epoch:\",\"blue\"), iEpoch, colored(\"loss:\",\"blue\"),loss)\n",
    "    \n",
    "print(colored(\"Theta value after training:\",\"blue\"),theta_model)\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "loss = np.mean((y_model - y)**2)\n",
    "print(colored(\"Loss function with final parameters:\",\"blue\"), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9d317-0c3c-4fc6-ab09-afd9390b3c2b",
   "metadata": {},
   "source": [
    "* plot the model result on the training, noisy data\n",
    "* plot the model result on the test, clean data (corrspongin to average value of the noisy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000fe87-4f12-4830-8293-996c92636127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "#plot the data points\n",
    "ax[0].plot(x[:,1],y_with_noise, \"bo\", label=\"training data\")\n",
    "\n",
    "#plot the fitted line\n",
    "y_model = np.sum(theta_model*x, axis=1, keepdims=True)\n",
    "ax[0].plot(x[:,1],y_model, \"ro\", label=\"model result\")\n",
    "\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].legend();\n",
    "\n",
    "#create the test dataset\n",
    "nPoints = 15\n",
    "x_test = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "x_test = np.reshape(x_test, (-1,1))\n",
    "ones = np.ones(x_test.shape[0])\n",
    "x_test = np.column_stack((ones, x_test))\n",
    "y_test = np.sum(theta*x_test, axis=1)\n",
    "\n",
    "#plot the data points\n",
    "ax[1].plot(x_test[:,1],y_test, \"bo\", label=\"mean target value\")\n",
    "\n",
    "#plot the fitted line\n",
    "y_model = np.sum(theta_model*x_test, axis=1, keepdims=True)\n",
    "ax[1].plot(x_test[:,1],y_model, \"ro\", label=\"model result\")\n",
    "\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e11dd7-a84c-4096-bb8d-9e0c1dfc5a9e",
   "metadata": {},
   "source": [
    "## Large fluctuation require more data.\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* generate larger training dataset - 10000 points\n",
    "* run the training again\n",
    "* remake the plots on traiing and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375b5b7-f787-4075-b3c3-79a33a9c06b4",
   "metadata": {},
   "source": [
    "## Non linear models: neural networks\n",
    "\n",
    "# An universal approximation theorem [Cybenko, 1989](https://link.springer.com/article/10.1007/BF02551274):\n",
    "\n",
    "Let's define a ``neuron`` function on $R^{n} \\rightarrow R$:\n",
    "$$\n",
    "\\huge{\n",
    " f(\\theta, x) = A(\\sum_{i=1}^{N} \\theta_{i} x_{i} + \\beta)\n",
    "}\n",
    "$$\n",
    "\n",
    "where `A` - activation function: any function of a single argument that fullfils requirents:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    " \\lim_{x\\rightarrow -\\infty} f(x) \\rightarrow 0 \\\\\n",
    " \\lim_{x\\rightarrow +\\infty} f(x) \\rightarrow 1 \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "Every continous function on $R^{n} \\rightarrow R$ can be approximated in basis of neural functions (=one layer of neurons).\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y(x) \\simeq \\sum_{k} w_{k} f_{k}(\\theta_{k}, x) + y_{0}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8cc3cc-2118-415d-a842-6a5ddd68b6ef",
   "metadata": {},
   "source": [
    "We will not implement the gradients, and the training loop for the neural networks by hand. We will use pyTorch framework, which has all the components ready. This is the topis of the next session: a simple multi layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30961f92-b49c-44fe-9a3f-421ac3ad3174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
